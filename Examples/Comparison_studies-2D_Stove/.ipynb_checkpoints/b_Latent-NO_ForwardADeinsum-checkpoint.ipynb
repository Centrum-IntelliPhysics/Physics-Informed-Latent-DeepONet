{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccc73df",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "This code can execute the following variants of losses:  \n",
    "\n",
    "1. **Variant 1:**  Purely Physics  \n",
    "   $L_{\\theta} = L_{\\text{PDE}}$  \n",
    "   Use $n_{\\text{used}} = 0$  \n",
    "\n",
    "2. **Variant 2:**  Physics + Data  \n",
    "   $L_{\\theta} = L_{\\text{PDE}} + \\Sigma_{i=1}^{n_{\\text{used}}}\\| u_i - \\hat{u}_i \\|_2^2$  \n",
    "   Use $n_{\\text{used}} \\in (0, 500]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a75ad",
   "metadata": {
    "papermill": {
     "duration": 0.00391,
     "end_time": "2024-12-29T08:18:49.707454",
     "exception": false,
     "start_time": "2024-12-29T08:18:49.703544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1395ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:18:49.714635Z",
     "iopub.status.busy": "2024-12-29T08:18:49.714378Z",
     "iopub.status.idle": "2024-12-29T08:18:55.102229Z",
     "shell.execute_reply": "2024-12-29T08:18:55.101600Z"
    },
    "papermill": {
     "duration": 5.392677,
     "end_time": "2024-12-29T08:18:55.103495",
     "exception": false,
     "start_time": "2024-12-29T08:18:49.710818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from termcolor import colored\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from utils.networks import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "from utils.deeponet_networks_2d import *\n",
    "from utils.misc_stove import *\n",
    "from utils.visualizer_stove import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921aabe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:18:55.111883Z",
     "iopub.status.busy": "2024-12-29T08:18:55.111572Z",
     "iopub.status.idle": "2024-12-29T08:18:55.114342Z",
     "shell.execute_reply": "2024-12-29T08:18:55.113907Z"
    },
    "papermill": {
     "duration": 0.007458,
     "end_time": "2024-12-29T08:18:55.115142",
     "exception": false,
     "start_time": "2024-12-29T08:18:55.107684",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Tag this cell with 'parameters'\n",
    "# parameters\n",
    "seed = 0 # Seed number.\n",
    "n_used = 0 # Ensure n_used is a multiple of 10 (as group size is 10) # Number of full training fields used for estimating the data-driven loss term\n",
    "n_iterations = 1000 # Number of iterations.\n",
    "save = False # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:18:55.131449Z",
     "iopub.status.busy": "2024-12-29T08:18:55.131137Z",
     "iopub.status.idle": "2024-12-29T08:18:55.133830Z",
     "shell.execute_reply": "2024-12-29T08:18:55.133433Z"
    },
    "papermill": {
     "duration": 0.006966,
     "end_time": "2024-12-29T08:18:55.134628",
     "exception": false,
     "start_time": "2024-12-29T08:18:55.127662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','ForwardAD_neval_t_constant','b_Latent-NO_ForwardADeinsum') \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7126e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:18:55.141311Z",
     "iopub.status.busy": "2024-12-29T08:18:55.141084Z",
     "iopub.status.idle": "2024-12-29T08:18:55.144242Z",
     "shell.execute_reply": "2024-12-29T08:18:55.143812Z"
    },
    "papermill": {
     "duration": 0.007396,
     "end_time": "2024-12-29T08:18:55.145062",
     "exception": false,
     "start_time": "2024-12-29T08:18:55.137666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d11f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:18:55.151993Z",
     "iopub.status.busy": "2024-12-29T08:18:55.151688Z",
     "iopub.status.idle": "2024-12-29T08:18:55.838291Z",
     "shell.execute_reply": "2024-12-29T08:18:55.837884Z"
    },
    "papermill": {
     "duration": 0.691003,
     "end_time": "2024-12-29T08:18:55.839162",
     "exception": false,
     "start_time": "2024-12-29T08:18:55.148159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346e7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:18:55.847045Z",
     "iopub.status.busy": "2024-12-29T08:18:55.846683Z",
     "iopub.status.idle": "2024-12-29T08:18:56.484562Z",
     "shell.execute_reply": "2024-12-29T08:18:56.484123Z"
    },
    "papermill": {
     "duration": 0.642743,
     "end_time": "2024-12-29T08:18:56.485508",
     "exception": false,
     "start_time": "2024-12-29T08:18:55.842765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata_np = np.load(os.path.join('..','..','data/2D_Stove/metadata.npz'), allow_pickle=True)\n",
    "metadata = convert_metadata_to_torch(metadata_np, device)\n",
    "for key, value in metadata.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"Shape of {key}: {value.shape}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"{key} is a dictionary with {len(value)} keys.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02558fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:18:56.493498Z",
     "iopub.status.busy": "2024-12-29T08:18:56.493214Z",
     "iopub.status.idle": "2024-12-29T08:18:56.571236Z",
     "shell.execute_reply": "2024-12-29T08:18:56.570853Z"
    },
    "papermill": {
     "duration": 0.08286,
     "end_time": "2024-12-29T08:18:56.572099",
     "exception": false,
     "start_time": "2024-12-29T08:18:56.489239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_span, x_span, y_span = metadata['t_span'], metadata['x_span'], metadata['y_span']\n",
    "\n",
    "nt, nx, ny = len(t_span), len(x_span), len(y_span) # number of discretizations in time, location_x and location_y.\n",
    "print(\"nt =\",nt, \", nx =\",nx, \"ny =\",ny)\n",
    "print(\"Shape of t-span, x-span, and y-span:\",t_span.shape, x_span.shape, y_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "print(\"y-span:\", y_span)\n",
    "\n",
    "L = 2.         # Simulation domain [-L, L]^2\n",
    "T = 1.         # Simulation time\n",
    "D_value = 1.0  # Diffusion coefficient  \n",
    "\n",
    "grid = torch.vstack((t_span.repeat_interleave(ny*nx), \n",
    "              x_span.flatten().repeat(nt),\n",
    "              y_span.flatten().repeat(nt))).T\n",
    "print(\"Shape of grid:\", grid.shape) # (nt*nx*ny, 3)\n",
    "print(\"grid:\", grid) # time, location_x, location_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129b4ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:18:56.579969Z",
     "iopub.status.busy": "2024-12-29T08:18:56.579628Z",
     "iopub.status.idle": "2024-12-29T08:18:56.586584Z",
     "shell.execute_reply": "2024-12-29T08:18:56.586246Z"
    },
    "papermill": {
     "duration": 0.011727,
     "end_time": "2024-12-29T08:18:56.587368",
     "exception": false,
     "start_time": "2024-12-29T08:18:56.575641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stove_full_solution_fields_groups_np = np.load(os.path.join('..','..','data/2D_Stove/stove_full_solution_fields.npz'), allow_pickle=True)\n",
    "print(stove_full_solution_fields_groups_np.keys())\n",
    "\n",
    "stove_source_fields_only_groups_np = np.load(os.path.join('..','..','data/2D_Stove/stove_source_fields_only.npz'), allow_pickle=True)\n",
    "print(stove_source_fields_only_groups_np.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b358345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:18:56.594992Z",
     "iopub.status.busy": "2024-12-29T08:18:56.594688Z",
     "iopub.status.idle": "2024-12-29T08:18:59.991059Z",
     "shell.execute_reply": "2024-12-29T08:18:59.990278Z"
    },
    "papermill": {
     "duration": 3.401942,
     "end_time": "2024-12-29T08:18:59.992724",
     "exception": false,
     "start_time": "2024-12-29T08:18:56.590782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the NumPy groups to Pytorch\n",
    "stove_full_solution_fields_groups = convert_groupdict_to_torch(stove_full_solution_fields_groups_np, device)\n",
    "stove_source_fields_only_groups = convert_groupdict_to_torch(stove_source_fields_only_groups_np, device)\n",
    "\n",
    "# Check the shapes\n",
    "print('stove_full_solution_fields_groups:')\n",
    "check_shapes(stove_full_solution_fields_groups)\n",
    "print(colored('#' * 230, 'green'))\n",
    "\n",
    "print('stove_source_fields_only_groups:')\n",
    "check_shapes(stove_source_fields_only_groups)\n",
    "print(colored('#' * 230, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2018762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:19:00.001964Z",
     "iopub.status.busy": "2024-12-29T08:19:00.001740Z",
     "iopub.status.idle": "2024-12-29T08:19:00.010842Z",
     "shell.execute_reply": "2024-12-29T08:19:00.010506Z"
    },
    "papermill": {
     "duration": 0.015229,
     "end_time": "2024-12-29T08:19:00.011806",
     "exception": false,
     "start_time": "2024-12-29T08:18:59.996577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing groups\n",
    "train_group, test_group = split_groups(stove_full_solution_fields_groups, seed, train_size=50, test_size=10)\n",
    "\n",
    "# Print the shapes for verification\n",
    "print(\"Training Group:\")\n",
    "print(colored('#' * 20, 'red'))\n",
    "print_group_shapes(train_group)\n",
    "print(colored('#' * 230, 'green'))\n",
    "print(\"Testing Group:\")\n",
    "print(colored('#' * 20, 'red'))\n",
    "print_group_shapes(test_group)\n",
    "print(colored('#' * 230, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7d29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:19:00.019522Z",
     "iopub.status.busy": "2024-12-29T08:19:00.019375Z",
     "iopub.status.idle": "2024-12-29T08:19:00.031384Z",
     "shell.execute_reply": "2024-12-29T08:19:00.031061Z"
    },
    "papermill": {
     "duration": 0.017143,
     "end_time": "2024-12-29T08:19:00.032431",
     "exception": false,
     "start_time": "2024-12-29T08:19:00.015288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = load_and_combine_groups(train_group, 'Train', combine=True, device=device)\n",
    "input_parameters_train = train_data['input_parameters']\n",
    "inputs_train = train_data['input_samples']\n",
    "outputs_train = train_data['output_samples']\n",
    "\n",
    "test_data = load_and_combine_groups(test_group, 'Test', combine=True, device=device)\n",
    "input_parameters_test = test_data['input_parameters']\n",
    "inputs_test = test_data['input_samples']\n",
    "outputs_test = test_data['output_samples']\n",
    "\n",
    "print(colored('#' * 20, 'red'))\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of input_parameters_train:\", input_parameters_train.shape)\n",
    "print(\"Shape of input_parameters_test:\", input_parameters_test.shape)\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print(colored('#' * 20, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1bc32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:19:00.040020Z",
     "iopub.status.busy": "2024-12-29T08:19:00.039880Z",
     "iopub.status.idle": "2024-12-29T08:19:00.060756Z",
     "shell.execute_reply": "2024-12-29T08:19:00.060402Z"
    },
    "papermill": {
     "duration": 0.025949,
     "end_time": "2024-12-29T08:19:00.061795",
     "exception": false,
     "start_time": "2024-12-29T08:19:00.035846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stove_source_fields_only = load_and_combine_groups(stove_source_fields_only_groups, 'Source only', combine=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745cb5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:19:00.069399Z",
     "iopub.status.busy": "2024-12-29T08:19:00.069259Z",
     "iopub.status.idle": "2024-12-29T08:19:00.073365Z",
     "shell.execute_reply": "2024-12-29T08:19:00.073008Z"
    },
    "papermill": {
     "duration": 0.009021,
     "end_time": "2024-12-29T08:19:00.074295",
     "exception": false,
     "start_time": "2024-12-29T08:19:00.065274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term\n",
    "input_parameters_train_used = input_parameters_train[:n_used, :]\n",
    "print(\"Shape of input_parameters_train_used:\", input_parameters_train_used.shape)\n",
    "inputs_train_used = inputs_train[:n_used, :, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16 # d_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5719c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:19:06.739600Z",
     "iopub.status.busy": "2024-12-29T08:19:06.739419Z",
     "iopub.status.idle": "2024-12-29T08:19:07.005739Z",
     "shell.execute_reply": "2024-12-29T08:19:07.005227Z"
    },
    "papermill": {
     "duration": 0.272553,
     "end_time": "2024-12-29T08:19:07.006985",
     "exception": false,
     "start_time": "2024-12-29T08:19:06.734432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_neurons_latent_branch: Number of input neurons in the latent_branch net.\n",
    "input_neurons_latent_trunk: Number of input neurons in the latent_trunk net.\n",
    "latent_p: Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\"\"\"\n",
    "latent_p = latent_dim*16 # Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\n",
    "input_neurons_latent_branch = (ny, nx) # Specify input size of image as a tuple (height, width)\n",
    "n_channels = 1\n",
    "num_filters = [40, 60, 80, 100]\n",
    "filter_sizes = [3, 3, 3, 3]\n",
    "strides = [1]*len(num_filters)\n",
    "paddings = [0]*len(num_filters)\n",
    "poolings = [('avg', 2, 2), ('avg', 2, 2), ('avg', 2, 2), ('avg', 2, 2)]  # Pooling layer specification (type, kernel_size, stride)\n",
    "end_MLP_layersizes = [150, 150, latent_p]\n",
    "activation = nn.ReLU() # nn.SiLU() #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_branch_net = ConvNet(input_neurons_latent_branch, n_channels, num_filters, filter_sizes, strides, paddings, poolings, end_MLP_layersizes, activation)\n",
    "latent_branch_net.to(device)\n",
    "# print(latent_branch_net)\n",
    "print('LATENT BRANCH-NET SUMMARY:')\n",
    "summary(latent_branch_net, input_size=(n_channels, ny, nx))  # input shape is (channels, height, width)\n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_latent_trunk = 1 # 1 corresponds to t\n",
    "latent_trunk_net = DenseNet(layersizes=[input_neurons_latent_trunk] + [128]*4 + [latent_p], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_trunk_net.to(device)\n",
    "# print(latent_trunk_net)\n",
    "print('LATENT TRUNK-NET SUMMARY:')\n",
    "summary(latent_trunk_net, input_size=(input_neurons_latent_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "\"\"\"\n",
    "input_neurons_reconstruction_branch: Number of input neurons in the reconstruction_branch net.\n",
    "input_neurons_reconstruction_trunk: Number of input neurons in the reconstruction_trunk net.\n",
    "reconstruction_q: Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\"\"\"\n",
    "reconstruction_q = 128 # Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\n",
    "input_neurons_reconstruction_branch = latent_dim # d_z\n",
    "reconstruction_branch_net = DenseNet(layersizes=[input_neurons_reconstruction_branch] + [128]*4 + [reconstruction_q], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_branch_net.to(device)\n",
    "# print(reconstruction_branch_net)\n",
    "print('RECONSTRUCTION BRANCH-NET SUMMARY:')\n",
    "summary(reconstruction_branch_net, input_size=(input_neurons_reconstruction_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_reconstruction_trunk = 2 # 2 corresponds to x and y\n",
    "reconstruction_trunk_net = DenseNet(layersizes=[input_neurons_reconstruction_trunk] + [128]*4 + [reconstruction_q], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_trunk_net.to(device)\n",
    "# print(reconstruction_trunk_net)\n",
    "print('RECONSTRUCTION TRUNK-NET SUMMARY:')\n",
    "summary(reconstruction_trunk_net, input_size=(input_neurons_reconstruction_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Latent_NO_model(latent_branch_net, latent_trunk_net, latent_dim, reconstruction_branch_net, reconstruction_trunk_net)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86a175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:19:07.017065Z",
     "iopub.status.busy": "2024-12-29T08:19:07.016715Z",
     "iopub.status.idle": "2024-12-29T08:19:07.019806Z",
     "shell.execute_reply": "2024-12-29T08:19:07.019369Z"
    },
    "papermill": {
     "duration": 0.008807,
     "end_time": "2024-12-29T08:19:07.020607",
     "exception": false,
     "start_time": "2024-12-29T08:19:07.011800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_learnable_parameters = (count_learnable_parameters(latent_branch_net)\n",
    "                            + count_learnable_parameters(latent_trunk_net)\n",
    "                            + count_learnable_parameters(reconstruction_branch_net)\n",
    "                            + count_learnable_parameters(reconstruction_trunk_net))\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908032c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:19:07.029655Z",
     "iopub.status.busy": "2024-12-29T08:19:07.029335Z",
     "iopub.status.idle": "2024-12-29T08:19:07.032623Z",
     "shell.execute_reply": "2024-12-29T08:19:07.032214Z"
    },
    "papermill": {
     "duration": 0.008694,
     "end_time": "2024-12-29T08:19:07.033411",
     "exception": false,
     "start_time": "2024-12-29T08:19:07.024717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def u_pred(net, inputs, t, x, y):\n",
    "    # enforcing ICs and BCs in hard way by multiplying model output with factor\n",
    "    # factor = t*(x-(-L))*(x-L)*(y-(-L))*(y-L)/(T*(2*L)*(2*L)*(2*L)*(2*L)) calculating this by appropriate broadcasting\n",
    "    t_term = t.unsqueeze(0)  # (1, neval_t, 1)\n",
    "    x_term = ((x-(-L))*(x-L)).squeeze() # (neval_loc,)\n",
    "    y_term = ((y-(-L))*(y-L)).squeeze() # (neval_loc,)\n",
    "    factor = t_term * x_term * y_term / (T*(2*L)*(2*L)*(2*L)*(2*L)) # (1, neval_t, neval_loc) due to broadcasting\n",
    "    latent_prediction, reconstruction_prediction = net(inputs, t, torch.hstack([x, y]))\n",
    "    u = reconstruction_prediction*factor # (bs, neval_t, neval_loc) # broadcasted element-wise\n",
    "    return latent_prediction, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffb765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:19:07.042217Z",
     "iopub.status.busy": "2024-12-29T08:19:07.041888Z",
     "iopub.status.idle": "2024-12-29T08:19:07.046755Z",
     "shell.execute_reply": "2024-12-29T08:19:07.046362Z"
    },
    "papermill": {
     "duration": 0.010128,
     "end_time": "2024-12-29T08:19:07.047550",
     "exception": false,
     "start_time": "2024-12-29T08:19:07.037422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, source_fields_parameters, source_fields, t, x, y):\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x, tangent_y = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device), torch.ones(y.shape).to(device)\n",
    "    ut = FWDAD_first_order_derivative(lambda t: u_pred(net, source_fields, t, x, y)[1], t, tangent_t) # (bs, neval_t, neval_loc)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: u_pred(net, source_fields, t, x, y)[1], x, tangent_x) # (bs, neval_t, neval_loc)\n",
    "    uyy = FWDAD_second_order_derivative(lambda y: u_pred(net, source_fields, t, x, y)[1], y, tangent_y) # (bs, neval_t, neval_loc)\n",
    "    \n",
    "    bs_ = source_fields.shape[0]\n",
    "    sf_values_ = torch.zeros((bs_, x.shape[0])).to(device)\n",
    "    for j in range(bs_):\n",
    "        source_class = Source(a=source_fields_parameters[j][3], r=source_fields_parameters[j][2], \n",
    "                      x=x, y=y, \n",
    "                      xc=0., yc=0.,\n",
    "                      device=device)\n",
    "        shape = get_key_from_value(shape_map, source_fields_parameters[j, 0])\n",
    "        sf_values_[j] = source_class.type_source(shape, num_sides=int(source_fields_parameters[j, 1])).flatten() # source function: s(x, y) values\n",
    "    sf_values__ = sf_values_.unsqueeze(1)\n",
    "    # Repeat elements along neval_loc for neval_t times and reshape\n",
    "    sf_values = sf_values__.repeat(1, t.shape[0], 1) # (bs, neval_t, neval_loc) # s(x, y) values are same for all times \n",
    "\n",
    "    pde_residual = (ut - (D_value*uxx) - (D_value*uyy) - sf_values)**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e24883",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "neval_t = 20  # Number of time points at which latent output field is evaluated for a given input sample.\n",
    "neval_x = 64 \n",
    "# neval_loc = neval_x*neval_y  # Number of locations at which output field is evaluated at each time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e40852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T08:19:07.056426Z",
     "iopub.status.busy": "2024-12-29T08:19:07.056284Z",
     "iopub.status.idle": "2024-12-29T11:56:20.118400Z",
     "shell.execute_reply": "2024-12-29T11:56:20.117977Z"
    },
    "papermill": {
     "duration": 13033.067725,
     "end_time": "2024-12-29T11:56:20.119285",
     "exception": false,
     "start_time": "2024-12-29T08:19:07.051560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 128 # Batch size\n",
    "\n",
    "neval_y = neval_x\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x*neval_y}  # Number of collocation points within the domain.\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "test_iteration_list, test_loss_list = [], []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "    else:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        inputs_train_used_batch = inputs_train_used.reshape(-1, 1, ny, nx)[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used.reshape(-1, nt, nx*ny)[indices_datadriven[0:bs]]\n",
    "        # print(f\"Shape of inputs_train_used_batch:\", inputs_train_used_batch.shape) # (bs, no. of channels, height, width)\n",
    "        # print(f\"Shape of outputs_train_used_batch:\", outputs_train_used_batch.shape) # (bs, nt, nx*ny)\n",
    "\n",
    "        _, reconstruction_predicted_values = u_pred(model, inputs_train_used_batch, \n",
    "                                                          t_span.reshape(-1, 1), \n",
    "                                                          x_span.flatten().reshape(-1,1), \n",
    "                                                          y_span.flatten().reshape(-1,1)) # (bs, nt, latent_dim), (bs, nt, nx*ny)\n",
    "        reconstruction_target_values = outputs_train_used_batch # (bs, nt, nx*ny)\n",
    "        datadriven_loss = nn.MSELoss()(reconstruction_predicted_values, reconstruction_target_values)\n",
    "        # print('*********')\n",
    "    \n",
    "    num_samples = stove_source_fields_only['input_samples'].shape[0]\n",
    "    indices_pinn = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "    input_parameters_batch = stove_source_fields_only['input_parameters'][indices_pinn[0:bs]]\n",
    "    inputs_batch = stove_source_fields_only['input_samples'].reshape(-1, 1, ny, nx)[indices_pinn[0:bs]]\n",
    "    #print(f\"Shape of inputs_train_batch[{i}]:\", inputs_batch.shape) # (bs, no. of channels, height, width)\n",
    "\n",
    "    # points within the domain\n",
    "    tc = td.uniform.Uniform(0., T).sample((neval_c['t'], 1)).to(device)\n",
    "    xc = td.uniform.Uniform(-L, L).sample((neval_c['loc'], 1)).to(device)\n",
    "    yc = td.uniform.Uniform(-L, L).sample((neval_c['loc'], 1)).to(device)\n",
    "\n",
    "    pinn_loss = loss_pde_residual(model, input_parameters_batch, inputs_batch, tc, xc, yc) \n",
    "    # print('*********')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if iteration % 500 == 0:\n",
    "        # Test loss calculation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_predicted_values = u_pred(model, inputs_test.reshape(-1, 1, ny, nx), \n",
    "                                                                  t_span.reshape(-1, 1), \n",
    "                                                                  x_span.flatten().reshape(-1,1), \n",
    "                                                                  y_span.flatten().reshape(-1,1))[1] # (bs, nt, nx*ny)\n",
    "            test_loss = nn.MSELoss()(test_predicted_values, outputs_test.reshape(-1, nt, nx*ny))\n",
    "            test_iteration_list.append(iteration)\n",
    "            test_loss_list.append(test_loss.item())  \n",
    "        model.train()  # Switch back to training mode\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f,' % optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "              'test loss = %f' % test_loss)\n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    mem_used_MB = (total - free) / 1024 ** 2\n",
    "    print(f\"Memory used: {mem_used_MB:.2f} MB\")\n",
    "    \n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "    np.save(os.path.join(resultdir,'test_iteration_list.npy'), np.asarray(test_iteration_list))\n",
    "    np.save(os.path.join(resultdir, 'test_loss_list.npy'), np.asarray(test_loss_list)) \n",
    "\n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "    \n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save)\n",
    "\n",
    "plot_testing_loss(resultdir, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_training_testing_loss(resultdir, iteration_list, loss_list, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)  \n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d2881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T11:56:20.136931Z",
     "iopub.status.busy": "2024-12-29T11:56:20.136541Z",
     "iopub.status.idle": "2024-12-29T11:56:20.145273Z",
     "shell.execute_reply": "2024-12-29T11:56:20.144933Z"
    },
    "papermill": {
     "duration": 0.018242,
     "end_time": "2024-12-29T11:56:20.146051",
     "exception": false,
     "start_time": "2024-12-29T11:56:20.127809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir, 'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c742e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T11:56:20.162773Z",
     "iopub.status.busy": "2024-12-29T11:56:20.162623Z",
     "iopub.status.idle": "2024-12-29T12:01:54.858939Z",
     "shell.execute_reply": "2024-12-29T12:01:54.858591Z"
    },
    "papermill": {
     "duration": 334.842754,
     "end_time": "2024-12-29T12:01:54.996764",
     "exception": false,
     "start_time": "2024-12-29T11:56:20.154010",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "_, reconstruction_predictions_test = u_pred(model, inputs_test.reshape(-1, 1, ny, nx), \n",
    "                                                                  t_span.reshape(-1, 1), \n",
    "                                                                  x_span.flatten().reshape(-1,1), \n",
    "                                                                  y_span.flatten().reshape(-1,1)) # (bs, nt, latent_dim), (bs, nt, nx*ny)\n",
    "# print(reconstruction_predictions_test.shape)\n",
    "\n",
    "mse_list, r2score_list, relerror_list = [], [], []\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "    \n",
    "    reconstruction_prediction_i = reconstruction_predictions_test[i].unsqueeze(0)# (1, nt, nx*ny)\n",
    "    reconstruction_target_i = outputs_test[i].reshape(nt, nx*ny).unsqueeze(0) # (1, nt, nx*ny)\n",
    "\n",
    "    reconstruction_mse_i = F.mse_loss(reconstruction_prediction_i.cpu(), reconstruction_target_i.cpu())\n",
    "    mse_i = reconstruction_mse_i\n",
    "    \n",
    "    r2score_i = metrics.r2_score(reconstruction_target_i.flatten().cpu().detach().numpy(), reconstruction_prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy() - reconstruction_prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy())\n",
    "        \n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item())\n",
    "    \n",
    "    # Plot the full solution-field for few cases (2 groups i.e., 10*2=20):\n",
    "    if (i+1) <= 20:\n",
    "        print(colored('TEST SAMPLE '+str(i+1), 'red'))\n",
    "        shape = get_key_from_value(shape_map, input_parameters_test[i, 0])\n",
    "        print(colored(f\"Shape = {shape}, r = {input_parameters_test[i,2]:.3f}, a_value = {input_parameters_test[i,3]:.3f}\", 'red'))\n",
    "        \n",
    "        r2score_i = float('%.4f'%r2score_i)\n",
    "        relerror_i = float('%.4f'%relerror_i)\n",
    "        print('Rel. L2 Error = '+str(relerror_i)+', R2 score = '+str(r2score_i))\n",
    "        \n",
    "        # Plotting \n",
    "        plot_source(i, x_span, y_span, inputs_test[i], f\"{shape.capitalize()} Source\", 'hot', resultdir, save)\n",
    "        \n",
    "        cmap = 'hot'  # Color map\n",
    "        fontsize = 14  # Font size for labels and titles\n",
    "        levels = 100\n",
    "        plot_solution(i, x_span, y_span, reconstruction_target_i.reshape(nt,ny,nx), t_span, f\"True Solution for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir, save, 'True-Solution')\n",
    "        plot_solution(i, x_span, y_span, reconstruction_prediction_i.reshape(nt,ny,nx), t_span, f\"Predicted Solution for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir, save, 'Predicted-Solution')\n",
    "        plot_solution(i, x_span, y_span, torch.abs(reconstruction_target_i.reshape(nt,ny,nx) - reconstruction_prediction_i.reshape(nt,ny,nx)), t_span, f\"Absolute error for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir, save, 'Absolute error')\n",
    "        print(colored('#'*230, 'green'))\n",
    "        \n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0b218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T12:01:55.644180Z",
     "iopub.status.busy": "2024-12-29T12:01:55.643857Z",
     "iopub.status.idle": "2024-12-29T12:01:55.715833Z",
     "shell.execute_reply": "2024-12-29T12:01:55.715446Z"
    },
    "papermill": {
     "duration": 0.392428,
     "end_time": "2024-12-29T12:01:55.716749",
     "exception": false,
     "start_time": "2024-12-29T12:01:55.324321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"input_parameters_test\": input_parameters_test.cpu(),\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": reconstruction_predictions_test.reshape(-1, nt, ny, nx).cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6d8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T12:01:56.348070Z",
     "iopub.status.busy": "2024-12-29T12:01:56.347527Z",
     "iopub.status.idle": "2024-12-29T12:01:56.350654Z",
     "shell.execute_reply": "2024-12-29T12:01:56.350306Z"
    },
    "papermill": {
     "duration": 0.319508,
     "end_time": "2024-12-29T12:01:56.351569",
     "exception": false,
     "start_time": "2024-12-29T12:01:56.032061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)\n",
    "# GPU memory used\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Memory used (in MB):\\n{mem_used_MB:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a366414",
   "metadata": {
    "papermill": {
     "duration": 0.329542,
     "end_time": "2024-12-29T12:01:56.994060",
     "exception": false,
     "start_time": "2024-12-29T12:01:56.664518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e4665",
   "metadata": {
    "papermill": {
     "duration": 0.313073,
     "end_time": "2024-12-29T12:01:57.621951",
     "exception": false,
     "start_time": "2024-12-29T12:01:57.308878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13390.383989,
   "end_time": "2024-12-29T12:01:59.151877",
   "environment_variables": {},
   "exception": null,
   "input_path": "b_PI-Latent-NO_with-PCA.ipynb",
   "output_path": "results/b_PI-Latent-NO_with-PCA/seed=0_n_used=200/output_seed=0_n_used=200.ipynb",
   "parameters": {
    "n_used": 200,
    "save": true,
    "seed": 0
   },
   "start_time": "2024-12-29T08:18:48.767888",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
