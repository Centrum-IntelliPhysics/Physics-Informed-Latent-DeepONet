{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6a55d2",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "This code can execute the following three variants of losses:  \n",
    "\n",
    "1. **Variant 1:**  Purely Physics  \n",
    "   $L_{\\theta} = L_{\\text{PDE}}$  \n",
    "   Use $n_{\\text{used}} = 0$  \n",
    "\n",
    "2. **Variant 2:**  Physics + Data  \n",
    "   $L_{\\theta} = L_{\\text{PDE}} + \\Sigma_{i=1}^{n_{\\text{used}}}\\| u_i - \\hat{u}_i \\|_2^2$  \n",
    "   Use $n_{\\text{used}} \\in (0, 1000]$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a75ad",
   "metadata": {
    "papermill": {
     "duration": 0.003132,
     "end_time": "2024-07-29T06:13:49.164223",
     "exception": false,
     "start_time": "2024-07-29T06:13:49.161091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1395ed4",
   "metadata": {
    "papermill": {
     "duration": 2.431666,
     "end_time": "2024-07-29T06:13:51.598512",
     "exception": false,
     "start_time": "2024-07-29T06:13:49.166846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from utils.networks import *\n",
    "from utils.deeponet_networks_1d import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.visualizer_1d import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921aabe3",
   "metadata": {
    "papermill": {
     "duration": 0.006396,
     "end_time": "2024-07-29T06:13:51.607820",
     "exception": false,
     "start_time": "2024-07-29T06:13:51.601424",
     "status": "completed"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0 # Seed number.\n",
    "n_used = 0 # Number of full training fields used for estimating the data-driven loss term\n",
    "save = False # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd328d",
   "metadata": {
    "papermill": {
     "duration": 0.006102,
     "end_time": "2024-07-29T06:13:51.624992",
     "exception": false,
     "start_time": "2024-07-29T06:13:51.618890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','ForwardAD_neval_t_constant','b_Latent-NO_ForwardADeinsum') \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7126e6",
   "metadata": {
    "papermill": {
     "duration": 0.006533,
     "end_time": "2024-07-29T06:13:51.634061",
     "exception": false,
     "start_time": "2024-07-29T06:13:51.627528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d11f0e",
   "metadata": {
    "papermill": {
     "duration": 0.065365,
     "end_time": "2024-07-29T06:13:51.702090",
     "exception": false,
     "start_time": "2024-07-29T06:13:51.636725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346e7da",
   "metadata": {
    "papermill": {
     "duration": 0.323942,
     "end_time": "2024-07-29T06:13:52.029286",
     "exception": false,
     "start_time": "2024-07-29T06:13:51.705344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = np.load(os.path.join('..','..','data/1D_Diffusion-reaction_dynamics_t=0to1/Diffusion-reaction_dynamics.npz')) # Load the .npz file\n",
    "print(data)\n",
    "print(data['t_span'].shape)\n",
    "print(data['x_span'].shape)\n",
    "print(data['input_s_samples'].shape) # Random Source fields: Gaussian random fields, Nsamples x 100, each sample is (1 x 100)\n",
    "print(data['output_u_samples'].shape) # Time evolution of the solution field: Nsamples x 101 x 100.\n",
    "                               # Each field is 101 x 100, rows correspond to time and columns respond to location.\n",
    "                               # First row corresponds to solution at t=0 (1st time step)\n",
    "                               # and next  row corresponds to solution at t=0.01 (2nd time step) and so on.\n",
    "                               # last row correspond to solution at t=1 (101th time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18da80",
   "metadata": {
    "papermill": {
     "duration": 1.757504,
     "end_time": "2024-07-29T06:13:53.789852",
     "exception": false,
     "start_time": "2024-07-29T06:13:52.032348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "inputs = torch.from_numpy(data['input_s_samples'][0:1500]).float().to(device)\n",
    "outputs = torch.from_numpy(data['output_u_samples'][0:1500]).float().to(device)\n",
    "\n",
    "t_span = torch.from_numpy(data['t_span']).float().to(device)\n",
    "x_span = torch.from_numpy(data['x_span']).float().to(device)\n",
    "nt, nx = len(t_span), len(x_span) # number of discretizations in time and location.\n",
    "print(\"nt =\",nt, \", nx =\",nx)\n",
    "print(\"Shape of t-span and x-span:\",t_span.shape, x_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "\n",
    "# Estimating grid points\n",
    "T, X = torch.meshgrid(t_span, x_span)\n",
    "# print(T)\n",
    "# print(X)\n",
    "\n",
    "# Split the data into training and testing samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=500, random_state=seed)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca370b",
   "metadata": {
    "papermill": {
     "duration": 0.007401,
     "end_time": "2024-07-29T06:13:53.800437",
     "exception": false,
     "start_time": "2024-07-29T06:13:53.793036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term in the PI-Latent-NO\n",
    "inputs_train_used = inputs_train[:n_used, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee4bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 9 # d_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5719c66",
   "metadata": {
    "papermill": {
     "duration": 0.932606,
     "end_time": "2024-07-29T06:14:01.834555",
     "exception": false,
     "start_time": "2024-07-29T06:14:00.901949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_neurons_latent_branch: Number of input neurons in the latent_branch net.\n",
    "input_neurons_latent_trunk: Number of input neurons in the latent_trunk net.\n",
    "latent_p: Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\"\"\"\n",
    "latent_p = latent_dim*25 # Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\n",
    "input_neurons_latent_branch = nx # m\n",
    "latent_branch_net = DenseNet(layersizes=[input_neurons_latent_branch] + [64]*3 + [latent_p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_branch_net.to(device)\n",
    "# print(latent_branch_net)\n",
    "print('LATENT BRANCH-NET SUMMARY:')\n",
    "summary(latent_branch_net, input_size=(input_neurons_latent_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_latent_trunk = 1 # 1 corresponds to t\n",
    "latent_trunk_net = DenseNet(layersizes=[input_neurons_latent_trunk] + [64]*3 + [latent_p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_trunk_net.to(device)\n",
    "# print(latent_trunk_net)\n",
    "print('LATENT TRUNK-NET SUMMARY:')\n",
    "summary(latent_trunk_net, input_size=(input_neurons_latent_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "\"\"\"\n",
    "input_neurons_reconstruction_branch: Number of input neurons in the reconstruction_branch net.\n",
    "input_neurons_reconstruction_trunk: Number of input neurons in the reconstruction_trunk net.\n",
    "reconstruction_q: Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\"\"\"\n",
    "reconstruction_q = 128 # Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\n",
    "input_neurons_reconstruction_branch = latent_dim # d_z\n",
    "reconstruction_branch_net = DenseNet(layersizes=[input_neurons_reconstruction_branch] + [64]*3 + [reconstruction_q], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_branch_net.to(device)\n",
    "# print(reconstruction_branch_net)\n",
    "print('RECONSTRUCTION BRANCH-NET SUMMARY:')\n",
    "summary(reconstruction_branch_net, input_size=(input_neurons_reconstruction_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_reconstruction_trunk = 1 # 1 corresponds to x\n",
    "reconstruction_trunk_net = DenseNet(layersizes=[input_neurons_reconstruction_trunk] + [64]*3 + [reconstruction_q], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_trunk_net.to(device)\n",
    "# print(reconstruction_trunk_net)\n",
    "print('RECONSTRUCTION TRUNK-NET SUMMARY:')\n",
    "summary(reconstruction_trunk_net, input_size=(input_neurons_reconstruction_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Latent_NO_model(latent_branch_net, latent_trunk_net, latent_dim, reconstruction_branch_net, reconstruction_trunk_net)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86a175",
   "metadata": {
    "papermill": {
     "duration": 0.018728,
     "end_time": "2024-07-29T06:14:01.866434",
     "exception": false,
     "start_time": "2024-07-29T06:14:01.847706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_learnable_parameters = (count_learnable_parameters(latent_branch_net)\n",
    "                            + count_learnable_parameters(latent_trunk_net)\n",
    "                            + count_learnable_parameters(reconstruction_branch_net)\n",
    "                            + count_learnable_parameters(reconstruction_trunk_net))\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7510cc3",
   "metadata": {
    "papermill": {
     "duration": 0.022231,
     "end_time": "2024-07-29T06:14:01.903987",
     "exception": false,
     "start_time": "2024-07-29T06:14:01.881756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, source_fields, t, x):\n",
    "    \n",
    "    _, u = net(source_fields, t, x) # u is (bs, neval_t, neval_x)\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device)\n",
    "    ut  = FWDAD_first_order_derivative(lambda t: net(source_fields, t, x)[1], t, tangent_t) # (bs, neval_t, neval_x)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: net(source_fields, t, x)[1], x, tangent_x) # (bs, neval_t, neval_x)\n",
    "    \n",
    "    bs_ = source_fields.shape[0]\n",
    "    sf_values_ = torch.zeros((bs_, x.shape[0], 1)).to(device)\n",
    "    for j in range(bs_):\n",
    "        sf_values_[j] = linear_interpolation_1D(x, x_span, source_fields[j]) # source function: s(x) values\n",
    "    sf_values__ = sf_values_.reshape(-1, 1, x.shape[0]) # (bs, 1, neval_x)\n",
    "    # Repeat elements along neval_x for neval_t times and reshape\n",
    "    sf_values = sf_values__.repeat(1, neval_t, 1) # (bs, neval_t, neval_x) # s(x) values are same for all times\n",
    "    \n",
    "    pde_residual = (ut - (0.01*uxx) - (0.01*(u**2)) - sf_values)**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0afa6",
   "metadata": {
    "papermill": {
     "duration": 0.023597,
     "end_time": "2024-07-29T06:14:01.945486",
     "exception": false,
     "start_time": "2024-07-29T06:14:01.921889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_bcs(net, source_fields, t, x):\n",
    "    \n",
    "    t_b1, x_b1 = t[0], x[0]\n",
    "    t_b2, x_b2 = t[1], x[1]\n",
    "\n",
    "    _, u_b1 = net(source_fields, t_b1, x_b1) # u is (bs, neval_t, 1)\n",
    "    _, u_b2 = net(source_fields, t_b2, x_b2) # u is (bs, neval_t, 1)\n",
    "\n",
    "    bc1_value, bc2_value = 0., 0.\n",
    "    pde_bc1 = (u_b1 - bc1_value)**2\n",
    "    pde_bc2 = (u_b2 - bc2_value)**2\n",
    "    \n",
    "    return torch.mean(pde_bc1) + torch.mean(pde_bc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48197e0",
   "metadata": {
    "papermill": {
     "duration": 0.02503,
     "end_time": "2024-07-29T06:14:01.991361",
     "exception": false,
     "start_time": "2024-07-29T06:14:01.966331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_ic(net, source_fields, t, x):\n",
    "\n",
    "    _, u_ic = net(source_fields, t, x) # u is (bs, 1, neval_x)\n",
    "    \n",
    "    ic_value = 0.\n",
    "    pde_ic = (u_ic - ic_value)**2\n",
    "    \n",
    "    return torch.mean(pde_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1597b0f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "neval_t = 256  # Number of time points at which latent output field is evaluated for a given input source field sample\n",
    "neval_x = 256 \n",
    "# neval_loc = neval_x  # Number of locations at which output field is evaluated at each time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e40852",
   "metadata": {
    "papermill": {
     "duration": 10179.88177,
     "end_time": "2024-07-29T09:03:41.896072",
     "exception": false,
     "start_time": "2024-07-29T06:14:02.014302",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 64 # Batch size\n",
    "\n",
    "print(f\"neval_t = {neval_t}, neval_x = {neval_x}\")\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x}  # Number of collocation points within the domain.\n",
    "neval_b = {'t': neval_t, 'loc': 1}        # Number of collocation points on each boundary.\n",
    "neval_i = {'t': 1, 'loc': neval_x}        # Number of collocation points at t=0.\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3.5*1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "test_iteration_list, test_loss_list = [], []\n",
    "\n",
    "n_iterations = 1000\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "    else:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        inputs_train_used_batch = inputs_train_used[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used[indices_datadriven[0:bs]]\n",
    "        #print(\"Shape of inputs_train_used_batch:\", inputs_train_used_batch.shape) # (bs, nx)\n",
    "        #print(\"Shape of outputs_train_used_batch:\", outputs_train_used_batch.shape) # (bs, nt, nx)\n",
    "\n",
    "        _, reconstruction_predicted_values = model(inputs_train_used_batch, t_span.reshape(-1, 1), x_span.reshape(-1, 1)) # (bs, nt, latent_dim), (bs, nt, nx)\n",
    "        reconstruction_target_values = outputs_train_used_batch # (bs, nt, nx)\n",
    "        datadriven_loss = nn.MSELoss()(reconstruction_predicted_values, reconstruction_target_values)\n",
    "        # print('*********')\n",
    "    \n",
    "    indices_pinn = torch.randperm(len(inputs_train)).to(device) # Generate random permutation of indices\n",
    "    inputs_batch = inputs_train[indices_pinn[0:bs]]\n",
    "    #print(f\"Shape of inputs_batch:\", inputs_batch.shape) # (bs, nx)\n",
    "    \n",
    "    # points within the domain\n",
    "    tc = td.uniform.Uniform(0., 1.).sample((neval_c['t'], 1)).to(device)\n",
    "    xc = td.uniform.Uniform(0., 1.).sample((neval_c['loc'], 1)).to(device)\n",
    "\n",
    "    # boundary points on the 2 boundaries (hard-coded)\n",
    "    tb = [td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device),\n",
    "          td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device)]\n",
    "    xb = [torch.tensor([[0.]]).to(device), \n",
    "          torch.tensor([[1.]]).to(device)]\n",
    "\n",
    "    # initial points\n",
    "    ti = torch.zeros((1, 1)).to(device)\n",
    "    xi = td.uniform.Uniform(0., 1.).sample((neval_i['loc'], 1)).to(device)\n",
    "\n",
    "    pinn_loss = (loss_pde_residual(model, inputs_batch, tc, xc) \n",
    "               + loss_pde_bcs(model, inputs_batch, tb, xb) \n",
    "               + loss_pde_ic(model, inputs_batch, ti, xi))\n",
    "    # print('*********')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        # Test loss calculation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_predicted_values = model(inputs_test, \n",
    "                              t_span.reshape(-1, 1), \n",
    "                              x_span.reshape(-1, 1))[1]  # (bs, neval) = (bs, nt, nx)\n",
    "            test_loss = nn.MSELoss()(test_predicted_values, outputs_test.reshape(-1, nt, nx))\n",
    "            test_iteration_list.append(iteration)\n",
    "            test_loss_list.append(test_loss.item())  \n",
    "        model.train()  # Switch back to training mode\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f,' % optimizer.state_dict()['param_groups'][0]['lr'], \n",
    "              'test loss = %f' % test_loss)\n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    \n",
    "if device.type == \"cuda\":\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    mem_used_MB = (total - free) / 1024 ** 2\n",
    "    print(f\"Memory used: {mem_used_MB:.2f} MB\")\n",
    "    \n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "    np.save(os.path.join(resultdir,'test_iteration_list.npy'), np.asarray(test_iteration_list))\n",
    "    np.save(os.path.join(resultdir, 'test_loss_list.npy'), np.asarray(test_loss_list)) \n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "    \n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save) \n",
    "\n",
    "plot_testing_loss(resultdir, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_training_testing_loss(resultdir, iteration_list, loss_list, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)  \n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d2881",
   "metadata": {
    "papermill": {
     "duration": 0.014148,
     "end_time": "2024-07-29T09:03:41.917505",
     "exception": false,
     "start_time": "2024-07-29T09:03:41.903357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir,'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c742e64",
   "metadata": {
    "papermill": {
     "duration": 55.025961,
     "end_time": "2024-07-29T09:04:36.950069",
     "exception": false,
     "start_time": "2024-07-29T09:03:41.924108",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "latent_branch_inputs = inputs_test # (bs, m) = (bs, nx) \n",
    "latent_trunk_inputs = t_span.reshape(-1, 1) # (nt, 1)\n",
    "reconstruction_trunk_inputs = x_span.reshape(-1, 1) # (nx, 1)\n",
    "_, reconstruction_predictions_test = model(latent_branch_inputs, latent_trunk_inputs, reconstruction_trunk_inputs)# (bs, nt, latent_dim), (bs, nt, nx)\n",
    "# print(reconstruction_predictions_test.shape)\n",
    "\n",
    "mse_list, r2score_list, relerror_list = [], [], [] \n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "\n",
    "    reconstruction_prediction_i = reconstruction_predictions_test[i].unsqueeze(0) # (1, nt, nx)\n",
    "    reconstruction_target_i = outputs_test[i].unsqueeze(0) # (1, nt, nx)\n",
    "    \n",
    "    reconstruction_mse_i = F.mse_loss(reconstruction_prediction_i.cpu(), reconstruction_target_i.cpu())\n",
    "    mse_i = reconstruction_mse_i\n",
    "    \n",
    "    r2score_i = metrics.r2_score(reconstruction_target_i.flatten().cpu().detach().numpy(), reconstruction_prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy() - reconstruction_prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy())\n",
    "\n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item())\n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        plot_predictions(i, resultdir, reconstruction_target_i, reconstruction_prediction_i, x_span, inputs_test, X, T, nt, nx, r'$s(x)$', 'Source field', 'seismic', save)\n",
    "        \n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": reconstruction_predictions_test.cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6d8df",
   "metadata": {
    "papermill": {
     "duration": 0.066171,
     "end_time": "2024-07-29T09:04:46.458087",
     "exception": false,
     "start_time": "2024-07-29T09:04:46.391916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)\n",
    "# GPU memory used\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Memory used (in MB):\\n{mem_used_MB:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2053e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3a068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10259.747177,
   "end_time": "2024-07-29T09:04:47.765374",
   "environment_variables": {},
   "exception": null,
   "input_path": "b_PI-Latent-NO_with-PCA.ipynb",
   "output_path": "results/b_PI-Latent-NO_with-PCA/seed=0_n_used=200/output_seed=0_n_used=200.ipynb",
   "parameters": {
    "n_used": 200,
    "seed": 0
   },
   "start_time": "2024-07-29T06:13:48.018197",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
