{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6f02c9",
   "metadata": {
    "papermill": {
     "duration": 0.003616,
     "end_time": "2025-03-27T00:29:26.984214",
     "exception": false,
     "start_time": "2025-03-27T00:29:26.980598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Comparisons\n",
    "\n",
    "This code can execute the following three variants of losses:  \n",
    "\n",
    "1. **Variant 1:**  Purely Physics  \n",
    "   $L_{\\theta} = L_{\\text{PDE}}$  \n",
    "   Use $n_{\\text{used}} = 0$  \n",
    "\n",
    "2. **Variant 2:**  Physics + Data  \n",
    "   $L_{\\theta} = L_{\\text{PDE}} + \\Sigma_{i=1}^{n_{\\text{used}}}\\| u_i - \\hat{u}_i \\|_2^2$  \n",
    "   Use $n_{\\text{used}} \\in (0, 1000]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224272da",
   "metadata": {
    "papermill": {
     "duration": 0.002688,
     "end_time": "2025-03-27T00:29:26.990017",
     "exception": false,
     "start_time": "2025-03-27T00:29:26.987329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1395ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:26.996302Z",
     "iopub.status.busy": "2025-03-27T00:29:26.996011Z",
     "iopub.status.idle": "2025-03-27T00:29:29.475686Z",
     "shell.execute_reply": "2025-03-27T00:29:29.475210Z"
    },
    "papermill": {
     "duration": 2.483784,
     "end_time": "2025-03-27T00:29:29.476558",
     "exception": false,
     "start_time": "2025-03-27T00:29:26.992774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from utils.networks import *\n",
    "from utils.deeponet_networks_1d import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.visualizer_1d import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e21cdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:29.483280Z",
     "iopub.status.busy": "2025-03-27T00:29:29.483099Z",
     "iopub.status.idle": "2025-03-27T00:29:29.485589Z",
     "shell.execute_reply": "2025-03-27T00:29:29.485224Z"
    },
    "papermill": {
     "duration": 0.006419,
     "end_time": "2025-03-27T00:29:29.486224",
     "exception": false,
     "start_time": "2025-03-27T00:29:29.479805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0 # Seed number.\n",
    "n_used = 0 # Number of full training fields used for estimating the data-driven loss term\n",
    "save = False # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15f9ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:29.492479Z",
     "iopub.status.busy": "2025-03-27T00:29:29.492314Z",
     "iopub.status.idle": "2025-03-27T00:29:29.494924Z",
     "shell.execute_reply": "2025-03-27T00:29:29.494580Z"
    },
    "papermill": {
     "duration": 0.006444,
     "end_time": "2025-03-27T00:29:29.495534",
     "exception": false,
     "start_time": "2025-03-27T00:29:29.489090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','ForwardAD_neval_c','a_Vanilla-NO_ForwardADeinsum') \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65cb4cab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:29.501663Z",
     "iopub.status.busy": "2025-03-27T00:29:29.501511Z",
     "iopub.status.idle": "2025-03-27T00:29:29.504797Z",
     "shell.execute_reply": "2025-03-27T00:29:29.504446Z"
    },
    "papermill": {
     "duration": 0.007051,
     "end_time": "2025-03-27T00:29:29.505407",
     "exception": false,
     "start_time": "2025-03-27T00:29:29.498356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 0\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d11f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:29.511845Z",
     "iopub.status.busy": "2025-03-27T00:29:29.511680Z",
     "iopub.status.idle": "2025-03-27T00:29:29.879611Z",
     "shell.execute_reply": "2025-03-27T00:29:29.879187Z"
    },
    "papermill": {
     "duration": 0.37186,
     "end_time": "2025-03-27T00:29:29.880255",
     "exception": false,
     "start_time": "2025-03-27T00:29:29.508395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea37c44a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:29.887091Z",
     "iopub.status.busy": "2025-03-27T00:29:29.886908Z",
     "iopub.status.idle": "2025-03-27T00:29:30.049599Z",
     "shell.execute_reply": "2025-03-27T00:29:30.049168Z"
    },
    "papermill": {
     "duration": 0.166815,
     "end_time": "2025-03-27T00:29:30.050213",
     "exception": false,
     "start_time": "2025-03-27T00:29:29.883398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile '../../data/1D_Diffusion-reaction_dynamics_t=0to1/Diffusion-reaction_dynamics.npz' with keys: input_s_samples, output_u_samples, t_span, x_span\n",
      "(101,)\n",
      "(100,)\n",
      "(2500, 100)\n",
      "(2500, 101, 100)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = np.load(os.path.join('..','..','data/1D_Diffusion-reaction_dynamics_t=0to1/Diffusion-reaction_dynamics.npz')) # Load the .npz file\n",
    "print(data)\n",
    "print(data['t_span'].shape)\n",
    "print(data['x_span'].shape)\n",
    "print(data['input_s_samples'].shape) # Random Source fields: Gaussian random fields, Nsamples x 100, each sample is (1 x 100)\n",
    "print(data['output_u_samples'].shape) # Time evolution of the solution field: Nsamples x 101 x 100.\n",
    "                               # Each field is 101 x 100, rows correspond to time and columns respond to location.\n",
    "                               # First row corresponds to solution at t=0 (1st time step)\n",
    "                               # and next  row corresponds to solution at t=0.01 (2nd time step) and so on.\n",
    "                               # last row correspond to solution at t=1 (101th time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6609428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.057183Z",
     "iopub.status.busy": "2025-03-27T00:29:30.056990Z",
     "iopub.status.idle": "2025-03-27T00:29:30.561074Z",
     "shell.execute_reply": "2025-03-27T00:29:30.558490Z"
    },
    "papermill": {
     "duration": 0.509048,
     "end_time": "2025-03-27T00:29:30.562472",
     "exception": false,
     "start_time": "2025-03-27T00:29:30.053424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt = 101 , nx = 100\n",
      "Shape of t-span and x-span: torch.Size([101]) torch.Size([100])\n",
      "t-span: tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
      "        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n",
      "        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n",
      "        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n",
      "        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n",
      "        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n",
      "        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n",
      "        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n",
      "        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n",
      "        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n",
      "        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n",
      "        0.9900, 1.0000], device='cuda:0')\n",
      "x-span: tensor([0.0050, 0.0150, 0.0250, 0.0350, 0.0450, 0.0550, 0.0650, 0.0750, 0.0850,\n",
      "        0.0950, 0.1050, 0.1150, 0.1250, 0.1350, 0.1450, 0.1550, 0.1650, 0.1750,\n",
      "        0.1850, 0.1950, 0.2050, 0.2150, 0.2250, 0.2350, 0.2450, 0.2550, 0.2650,\n",
      "        0.2750, 0.2850, 0.2950, 0.3050, 0.3150, 0.3250, 0.3350, 0.3450, 0.3550,\n",
      "        0.3650, 0.3750, 0.3850, 0.3950, 0.4050, 0.4150, 0.4250, 0.4350, 0.4450,\n",
      "        0.4550, 0.4650, 0.4750, 0.4850, 0.4950, 0.5050, 0.5150, 0.5250, 0.5350,\n",
      "        0.5450, 0.5550, 0.5650, 0.5750, 0.5850, 0.5950, 0.6050, 0.6150, 0.6250,\n",
      "        0.6350, 0.6450, 0.6550, 0.6650, 0.6750, 0.6850, 0.6950, 0.7050, 0.7150,\n",
      "        0.7250, 0.7350, 0.7450, 0.7550, 0.7650, 0.7750, 0.7850, 0.7950, 0.8050,\n",
      "        0.8150, 0.8250, 0.8350, 0.8450, 0.8550, 0.8650, 0.8750, 0.8850, 0.8950,\n",
      "        0.9050, 0.9150, 0.9250, 0.9350, 0.9450, 0.9550, 0.9650, 0.9750, 0.9850,\n",
      "        0.9950], device='cuda:0')\n",
      "Shape of grid: torch.Size([10100, 2])\n",
      "grid: tensor([[0.0000, 0.0050],\n",
      "        [0.0000, 0.0150],\n",
      "        [0.0000, 0.0250],\n",
      "        ...,\n",
      "        [1.0000, 0.9750],\n",
      "        [1.0000, 0.9850],\n",
      "        [1.0000, 0.9950]], device='cuda:0')\n",
      "Shape of inputs_train: torch.Size([1000, 100])\n",
      "Shape of inputs_test: torch.Size([500, 100])\n",
      "Shape of outputs_train: torch.Size([1000, 101, 100])\n",
      "Shape of outputs_test: torch.Size([500, 101, 100])\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "inputs = torch.from_numpy(data['input_s_samples'][0:1500]).float().to(device)\n",
    "outputs = torch.from_numpy(data['output_u_samples'][0:1500]).float().to(device)\n",
    "\n",
    "t_span = torch.from_numpy(data['t_span']).float().to(device)\n",
    "x_span = torch.from_numpy(data['x_span']).float().to(device)\n",
    "nt, nx = len(t_span), len(x_span) # number of discretizations in time and location.\n",
    "print(\"nt =\",nt, \", nx =\",nx)\n",
    "print(\"Shape of t-span and x-span:\",t_span.shape, x_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "\n",
    "# Estimating grid points\n",
    "T, X = torch.meshgrid(t_span, x_span)\n",
    "# print(T)\n",
    "# print(X)\n",
    "grid = torch.vstack((T.flatten(), X.flatten())).T\n",
    "print(\"Shape of grid:\", grid.shape) # (nt*nx, 2)\n",
    "print(\"grid:\", grid) # time, location\n",
    "\n",
    "# Split the data into training and testing samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=500, random_state=seed)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45abcaf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.573143Z",
     "iopub.status.busy": "2025-03-27T00:29:30.572402Z",
     "iopub.status.idle": "2025-03-27T00:29:30.580004Z",
     "shell.execute_reply": "2025-03-27T00:29:30.579527Z"
    },
    "papermill": {
     "duration": 0.014315,
     "end_time": "2025-03-27T00:29:30.580851",
     "exception": false,
     "start_time": "2025-03-27T00:29:30.566536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inputs_train_used: torch.Size([0, 100])\n",
      "Shape of outputs_train_used: torch.Size([0, 101, 100])\n"
     ]
    }
   ],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term in the PI-Latent-NO\n",
    "inputs_train_used = inputs_train[:n_used, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ae139a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.588237Z",
     "iopub.status.busy": "2025-03-27T00:29:30.587916Z",
     "iopub.status.idle": "2025-03-27T00:29:30.658439Z",
     "shell.execute_reply": "2025-03-27T00:29:30.657992Z"
    },
    "papermill": {
     "duration": 0.074906,
     "end_time": "2025-03-27T00:29:30.659075",
     "exception": false,
     "start_time": "2025-03-27T00:29:30.584169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRANCH-NET SUMMARY:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]           6,464\n",
      "              SiLU-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 64]           4,160\n",
      "              SiLU-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 64]           4,160\n",
      "              SiLU-6                   [-1, 64]               0\n",
      "            Linear-7                  [-1, 128]           8,320\n",
      "================================================================\n",
      "Total params: 23,104\n",
      "Trainable params: 23,104\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 0.09\n",
      "----------------------------------------------------------------\n",
      "####################################################################################################\n",
      "TRUNK-NET SUMMARY:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             192\n",
      "              SiLU-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 64]           4,160\n",
      "              SiLU-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 64]           4,160\n",
      "              SiLU-6                   [-1, 64]               0\n",
      "            Linear-7                  [-1, 128]           8,320\n",
      "================================================================\n",
      "Total params: 16,832\n",
      "Trainable params: 16,832\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.07\n",
      "----------------------------------------------------------------\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "input_neurons_branch: Number of input neurons in the branch net.\n",
    "input_neurons_trunk: Number of input neurons in the trunk net.\n",
    "p: Number of output neurons in both the branch and trunk net.\n",
    "\"\"\"\n",
    "p = 128 # Number of output neurons in both the branch and trunk net.\n",
    "\n",
    "input_neurons_branch = nx # m\n",
    "branch_net = DenseNet(layersizes=[input_neurons_branch] + [64]*3 + [p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "branch_net.to(device)\n",
    "# print(branch_net)\n",
    "print('BRANCH-NET SUMMARY:')\n",
    "summary(branch_net, input_size=(input_neurons_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_trunk = 2 # 2 corresponds to t and x\n",
    "trunk_net = DenseNet(layersizes=[input_neurons_trunk] + [64]*3 + [p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "trunk_net.to(device)\n",
    "# print(trunk_net)\n",
    "print('TRUNK-NET SUMMARY:')\n",
    "summary(trunk_net, input_size=(input_neurons_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Vanilla_NO_model(branch_net, trunk_net)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "548e3c67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.666570Z",
     "iopub.status.busy": "2025-03-27T00:29:30.666093Z",
     "iopub.status.idle": "2025-03-27T00:29:30.669100Z",
     "shell.execute_reply": "2025-03-27T00:29:30.668656Z"
    },
    "papermill": {
     "duration": 0.007303,
     "end_time": "2025-03-27T00:29:30.669711",
     "exception": false,
     "start_time": "2025-03-27T00:29:30.662408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of learnable parameters: 39936\n"
     ]
    }
   ],
   "source": [
    "num_learnable_parameters = count_learnable_parameters(branch_net) + count_learnable_parameters(trunk_net)\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00cd97dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.677116Z",
     "iopub.status.busy": "2025-03-27T00:29:30.676963Z",
     "iopub.status.idle": "2025-03-27T00:29:30.681590Z",
     "shell.execute_reply": "2025-03-27T00:29:30.681157Z"
    },
    "papermill": {
     "duration": 0.008978,
     "end_time": "2025-03-27T00:29:30.682208",
     "exception": false,
     "start_time": "2025-03-27T00:29:30.673230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, source_fields, t, x):\n",
    "    \n",
    "    u = net(source_fields, torch.hstack([t, x])) # u is (bs, neval_c)\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device)\n",
    "    ut  = FWDAD_first_order_derivative(lambda t: net(source_fields, torch.hstack([t, x])), t, tangent_t) # (bs, neval_c)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: net(source_fields, torch.hstack([t, x])), x, tangent_x) # (bs, neval_c)\n",
    "    \n",
    "    bs_ = source_fields.shape[0]\n",
    "    sf_values_ = torch.zeros((bs_, x.shape[0], 1)).to(device)\n",
    "    for j in range(bs_):\n",
    "        sf_values_[j] = linear_interpolation_1D(x, x_span, source_fields[j]) # source function: s(x) values\n",
    "    sf_values = sf_values_.reshape(-1, x.shape[0]) # (bs, neval_c)\n",
    "    \n",
    "    pde_residual = (ut - (0.01*uxx) - (0.01*(u**2)) - sf_values)**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48082483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.689525Z",
     "iopub.status.busy": "2025-03-27T00:29:30.689369Z",
     "iopub.status.idle": "2025-03-27T00:29:30.692860Z",
     "shell.execute_reply": "2025-03-27T00:29:30.692375Z"
    },
    "papermill": {
     "duration": 0.007817,
     "end_time": "2025-03-27T00:29:30.693495",
     "exception": false,
     "start_time": "2025-03-27T00:29:30.685678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_bcs(net, source_fields, t, x):\n",
    "    \n",
    "    t_b1, x_b1 = t[0], x[0]\n",
    "    t_b2, x_b2 = t[1], x[1]\n",
    "\n",
    "    u_b1 = net(source_fields, torch.hstack([t_b1, x_b1])) # u is (bs, neval_b)\n",
    "    u_b2 = net(source_fields, torch.hstack([t_b2, x_b2])) # u is (bs, neval_b)\n",
    "\n",
    "    bc1_value, bc2_value = 0., 0.\n",
    "    pde_bc1 = (u_b1 - bc1_value)**2\n",
    "    pde_bc2 = (u_b2 - bc2_value)**2\n",
    "    \n",
    "    return torch.mean(pde_bc1) + torch.mean(pde_bc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "338689a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.700854Z",
     "iopub.status.busy": "2025-03-27T00:29:30.700696Z",
     "iopub.status.idle": "2025-03-27T00:29:30.703335Z",
     "shell.execute_reply": "2025-03-27T00:29:30.702978Z"
    },
    "papermill": {
     "duration": 0.007,
     "end_time": "2025-03-27T00:29:30.703933",
     "exception": false,
     "start_time": "2025-03-27T00:29:30.696933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_ic(net, source_fields, t, x):\n",
    "    \n",
    "    u_ic = net(source_fields, torch.hstack([t, x])) # u is (bs, neval_i)\n",
    "    \n",
    "    ic_value = 0.\n",
    "    pde_ic = (u_ic - ic_value)**2\n",
    "    \n",
    "    return torch.mean(pde_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "653d373d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.711620Z",
     "iopub.status.busy": "2025-03-27T00:29:30.711473Z",
     "iopub.status.idle": "2025-03-27T00:29:30.714069Z",
     "shell.execute_reply": "2025-03-27T00:29:30.713688Z"
    },
    "papermill": {
     "duration": 0.007309,
     "end_time": "2025-03-27T00:29:30.714660",
     "exception": false,
     "start_time": "2025-03-27T00:29:30.707351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collocation_points(tc_span, xc_span, neval_dict):\n",
    "    tc = tc_span.repeat_interleave(neval_dict['loc']).unsqueeze(-1)\n",
    "    xc = xc_span.flatten().repeat(neval_dict['t']).unsqueeze(-1)\n",
    "    return tc, xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b98db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.722087Z",
     "iopub.status.busy": "2025-03-27T00:29:30.721926Z",
     "iopub.status.idle": "2025-03-27T00:29:30.724087Z",
     "shell.execute_reply": "2025-03-27T00:29:30.723727Z"
    },
    "papermill": {
     "duration": 0.006541,
     "end_time": "2025-03-27T00:29:30.724660",
     "exception": false,
     "start_time": "2025-03-27T00:29:30.718119",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "neval_t = 256  # Number of time points at which output field is evaluated.\n",
    "neval_x = 256 \n",
    "# neval_loc = neval_x  # Number of locations at which output field is evaluated at each time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d78c040c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.732084Z",
     "iopub.status.busy": "2025-03-27T00:29:30.731939Z",
     "iopub.status.idle": "2025-03-27T00:29:30.733981Z",
     "shell.execute_reply": "2025-03-27T00:29:30.733607Z"
    },
    "papermill": {
     "duration": 0.006412,
     "end_time": "2025-03-27T00:29:30.734561",
     "exception": false,
     "start_time": "2025-03-27T00:29:30.728149",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "neval_t = 1024\n",
    "neval_x = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a430a632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T00:29:30.742349Z",
     "iopub.status.busy": "2025-03-27T00:29:30.742198Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-03-27T00:29:30.738159",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neval_t = 1024, neval_x = 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - loss = 0.998290, data-driven loss = 0.000000, pinn loss = 0.998290, learning rate = 0.003500, test loss = 0.274835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 - loss = 0.752221, data-driven loss = 0.000000, pinn loss = 0.752221, learning rate = 0.003500, test loss = 0.157529\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 64 # Batch size\n",
    "\n",
    "print(f\"neval_t = {neval_t}, neval_x = {neval_x}\")\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x}  # Number of collocation points within the domain.\n",
    "neval_b = {'t': neval_t, 'loc': 1}        # Number of collocation points on each boundary.\n",
    "neval_i = {'t': 1, 'loc': neval_x}        # Number of collocation points at t=0.\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3.5*1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "test_iteration_list, test_loss_list = [], []\n",
    "\n",
    "n_iterations = 1000\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "    else:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        inputs_train_used_batch = inputs_train_used[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used.reshape(-1, nt*nx)[indices_datadriven[0:bs]]\n",
    "        # print(f\"Shape of inputs_train_used_batch:\", inputs_train_used_batch.shape) # (bs, nx)\n",
    "        # print(f\"Shape of outputs_train_used_batch:\", outputs_train_used_batch.shape) # (bs, nt*nx)\n",
    "\n",
    "        predicted_values = model(inputs_train_used_batch, grid)  # (bs, neval) = (bs, nt*nx)\n",
    "        target_values = outputs_train_used_batch # (bs, nt*nx)\n",
    "        datadriven_loss = nn.MSELoss()(predicted_values, target_values)\n",
    "        # print('*********')\n",
    "    \n",
    "    # Shuffle the train data using the generated indices\n",
    "    num_samples = len(inputs_train)\n",
    "    indices_pinn = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "    inputs_batch = inputs_train[indices_pinn[0:bs]]\n",
    "    #print(f\"Shape of inputs_batch:\", inputs_batch.shape) # (bs, nx)\n",
    "        \n",
    "    # points within the domain\n",
    "    tc_span = td.uniform.Uniform(0., 1.).sample((neval_c['t'], 1)).to(device)\n",
    "    xc_span = td.uniform.Uniform(0., 1.).sample((neval_c['loc'], 1)).to(device)\n",
    "    tc, xc= collocation_points(tc_span, xc_span, neval_c)\n",
    "\n",
    "    # boundary points on the 2 boundaries (hard-coded)\n",
    "    tb = [td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device),\n",
    "          td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device)]\n",
    "\n",
    "    xb = [torch.full((neval_b['t'], 1), 0.).to(device),\n",
    "          torch.full((neval_b['t'], 1), 1.).to(device)]\n",
    "\n",
    "    # initial points\n",
    "    ti = torch.full((neval_i['loc'], 1), 0.).to(device)\n",
    "    xi = td.uniform.Uniform(0., 1.).sample((neval_i['loc'], 1)).to(device)\n",
    "    \n",
    "    pinn_loss = (loss_pde_residual(model, inputs_batch, tc, xc) \n",
    "           + loss_pde_bcs(model, inputs_batch, tb, xb) \n",
    "           + loss_pde_ic(model, inputs_batch, ti, xi))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        # Test loss calculation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_predicted_values = model(inputs_test, grid)  # (bs, neval) = (bs, nt*nx)\n",
    "            test_loss = nn.MSELoss()(test_predicted_values, outputs_test.reshape(-1, nt*nx))\n",
    "            test_iteration_list.append(iteration)\n",
    "            test_loss_list.append(test_loss.item())  \n",
    "        model.train()  # Switch back to training mode\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f,' % optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "              'test loss = %f' % test_loss)\n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    mem_used_MB = (total - free) / 1024 ** 2\n",
    "    print(f\"Memory used: {mem_used_MB:.2f} MB\")\n",
    "\n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "    np.save(os.path.join(resultdir,'test_iteration_list.npy'), np.asarray(test_iteration_list))\n",
    "    np.save(os.path.join(resultdir, 'test_loss_list.npy'), np.asarray(test_loss_list)) \n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "\n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save)\n",
    "\n",
    "plot_testing_loss(resultdir, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_training_testing_loss(resultdir, iteration_list, loss_list, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)\n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time # in sec\n",
    " \n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67807e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir,'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47234a6a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "branch_inputs = inputs_test # (bs, m) = (bs, nx) \n",
    "trunk_inputs = grid # (neval, 2) = (nt*nx, 2)\n",
    "predictions_test = model(branch_inputs, trunk_inputs) # (bs, neval) = (bs, nt*nx)\n",
    "\n",
    "mse_list, r2score_list, relerror_list = [], [], []\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "\n",
    "    prediction_i = predictions_test[i].reshape(1, -1) # (1, nt*nx)\n",
    "    target_i = outputs_test[i].reshape(1, -1) # (1, nt*nx)\n",
    "    \n",
    "    mse_i = F.mse_loss(prediction_i.cpu(), target_i.cpu())\n",
    "    r2score_i = metrics.r2_score(target_i.flatten().cpu().detach().numpy(), prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(target_i.flatten().cpu().detach().numpy() - prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(target_i.flatten().cpu().detach().numpy())\n",
    "    \n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item())\n",
    "    \n",
    "    if (i+1) % 10 == 0:\n",
    "        plot_predictions(i, resultdir, target_i, prediction_i, x_span, inputs_test, X, T, nt, nx, r'$s(x)$', 'Source field', 'seismic', save)\n",
    "\n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1684b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": predictions_test.reshape(-1, nt, nx).cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965287e4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)\n",
    "# GPU memory used\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Memory used (in MB):\\n{mem_used_MB:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0834f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d2cd9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "a_Vanilla-NO_ForwardADeinsum.ipynb",
   "output_path": "results/ForwardAD_neval_c/a_Vanilla-NO_ForwardADeinsum/output_neval_t=1024_neval_x=1024.ipynb",
   "parameters": {
    "neval_t": 1024,
    "neval_x": 1024
   },
   "start_time": "2025-03-27T00:29:26.055876",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}