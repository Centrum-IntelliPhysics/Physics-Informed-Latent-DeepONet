{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccc73df",
   "metadata": {
    "papermill": {
     "duration": 0.003612,
     "end_time": "2025-03-13T18:14:17.700788",
     "exception": false,
     "start_time": "2025-03-13T18:14:17.697176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Comparisons\n",
    "\n",
    "This code can execute the following variants of losses:  \n",
    "\n",
    "1. **Variant 1:**  Purely Physics  \n",
    "   $L_{\\theta} = L_{\\text{PDE}}$  \n",
    "   Use $n_{\\text{used}} = 0$  \n",
    "\n",
    "2. **Variant 2:**  Physics + Data  \n",
    "   $L_{\\theta} = L_{\\text{PDE}} + \\Sigma_{i=1}^{n_{\\text{used}}}\\| u_i - \\hat{u}_i \\|_2^2$  \n",
    "   Use $n_{\\text{used}} \\in (0, 500]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a75ad",
   "metadata": {
    "papermill": {
     "duration": 0.002928,
     "end_time": "2025-03-13T18:14:17.706973",
     "exception": false,
     "start_time": "2025-03-13T18:14:17.704045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1395ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:17.713414Z",
     "iopub.status.busy": "2025-03-13T18:14:17.713197Z",
     "iopub.status.idle": "2025-03-13T18:14:21.037161Z",
     "shell.execute_reply": "2025-03-13T18:14:21.036749Z"
    },
    "papermill": {
     "duration": 3.328116,
     "end_time": "2025-03-13T18:14:21.038007",
     "exception": false,
     "start_time": "2025-03-13T18:14:17.709891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from termcolor import colored\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from utils.networks import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "from utils.deeponet_networks_2d import *\n",
    "from utils.misc_stove import *\n",
    "from utils.visualizer_stove import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921aabe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:21.045146Z",
     "iopub.status.busy": "2025-03-13T18:14:21.044861Z",
     "iopub.status.idle": "2025-03-13T18:14:21.047105Z",
     "shell.execute_reply": "2025-03-13T18:14:21.046791Z"
    },
    "papermill": {
     "duration": 0.006213,
     "end_time": "2025-03-13T18:14:21.047621",
     "exception": false,
     "start_time": "2025-03-13T18:14:21.041408",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Tag this cell with 'parameters'\n",
    "# parameters\n",
    "seed = 0 # Seed number.\n",
    "n_used = 150 # Ensure n_used is a multiple of 10 (as group size is 10) # Number of full training fields used for estimating the data-driven loss term\n",
    "n_iterations = 50000 # Number of iterations.\n",
    "save = True # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32153fe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:21.054102Z",
     "iopub.status.busy": "2025-03-13T18:14:21.053855Z",
     "iopub.status.idle": "2025-03-13T18:14:21.055856Z",
     "shell.execute_reply": "2025-03-13T18:14:21.055544Z"
    },
    "papermill": {
     "duration": 0.005803,
     "end_time": "2025-03-13T18:14:21.056383",
     "exception": false,
     "start_time": "2025-03-13T18:14:21.050580",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 0\n",
    "n_used = 150\n",
    "save = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8bd328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:21.062755Z",
     "iopub.status.busy": "2025-03-13T18:14:21.062539Z",
     "iopub.status.idle": "2025-03-13T18:14:21.065076Z",
     "shell.execute_reply": "2025-03-13T18:14:21.064771Z"
    },
    "papermill": {
     "duration": 0.006279,
     "end_time": "2025-03-13T18:14:21.065595",
     "exception": false,
     "start_time": "2025-03-13T18:14:21.059316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','b_Latent-NO','seed='+str(seed)+'_n_used='+str(n_used)) \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7126e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:21.072084Z",
     "iopub.status.busy": "2025-03-13T18:14:21.071872Z",
     "iopub.status.idle": "2025-03-13T18:14:21.075234Z",
     "shell.execute_reply": "2025-03-13T18:14:21.074925Z"
    },
    "papermill": {
     "duration": 0.007124,
     "end_time": "2025-03-13T18:14:21.075761",
     "exception": false,
     "start_time": "2025-03-13T18:14:21.068637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 0\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d11f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:21.082309Z",
     "iopub.status.busy": "2025-03-13T18:14:21.082102Z",
     "iopub.status.idle": "2025-03-13T18:14:21.227059Z",
     "shell.execute_reply": "2025-03-13T18:14:21.226726Z"
    },
    "papermill": {
     "duration": 0.148863,
     "end_time": "2025-03-13T18:14:21.227613",
     "exception": false,
     "start_time": "2025-03-13T18:14:21.078750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d346e7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:21.234553Z",
     "iopub.status.busy": "2025-03-13T18:14:21.234310Z",
     "iopub.status.idle": "2025-03-13T18:14:21.497998Z",
     "shell.execute_reply": "2025-03-13T18:14:21.497616Z"
    },
    "papermill": {
     "duration": 0.26773,
     "end_time": "2025-03-13T18:14:21.498594",
     "exception": false,
     "start_time": "2025-03-13T18:14:21.230864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of t_span: torch.Size([20])\n",
      "Shape of x_span: torch.Size([64, 64])\n",
      "Shape of y_span: torch.Size([64, 64])\n",
      "shapes is a dictionary with 10 keys.\n"
     ]
    }
   ],
   "source": [
    "metadata_np = np.load(os.path.join('..','..','data/2D_Stove/metadata.npz'), allow_pickle=True)\n",
    "metadata = convert_metadata_to_torch(metadata_np, device)\n",
    "for key, value in metadata.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"Shape of {key}: {value.shape}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"{key} is a dictionary with {len(value)} keys.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02558fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:21.505725Z",
     "iopub.status.busy": "2025-03-13T18:14:21.505555Z",
     "iopub.status.idle": "2025-03-13T18:14:21.590997Z",
     "shell.execute_reply": "2025-03-13T18:14:21.590645Z"
    },
    "papermill": {
     "duration": 0.089544,
     "end_time": "2025-03-13T18:14:21.591567",
     "exception": false,
     "start_time": "2025-03-13T18:14:21.502023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt = 20 , nx = 64 ny = 64\n",
      "Shape of t-span, x-span, and y-span: torch.Size([20]) torch.Size([64, 64]) torch.Size([64, 64])\n",
      "t-span: tensor([0.0500, 0.1000, 0.1500, 0.2000, 0.2500, 0.3000, 0.3500, 0.4000, 0.4500,\n",
      "        0.5000, 0.5500, 0.6000, 0.6500, 0.7000, 0.7500, 0.8000, 0.8500, 0.9000,\n",
      "        0.9500, 1.0000], device='cuda:0')\n",
      "x-span: tensor([[-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688],\n",
      "        [-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688],\n",
      "        [-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688],\n",
      "        ...,\n",
      "        [-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688],\n",
      "        [-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688],\n",
      "        [-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688]],\n",
      "       device='cuda:0')\n",
      "y-span: tensor([[-1.9688, -1.9688, -1.9688,  ..., -1.9688, -1.9688, -1.9688],\n",
      "        [-1.9062, -1.9062, -1.9062,  ..., -1.9062, -1.9062, -1.9062],\n",
      "        [-1.8438, -1.8438, -1.8438,  ..., -1.8438, -1.8438, -1.8438],\n",
      "        ...,\n",
      "        [ 1.8438,  1.8438,  1.8438,  ...,  1.8438,  1.8438,  1.8438],\n",
      "        [ 1.9062,  1.9062,  1.9062,  ...,  1.9062,  1.9062,  1.9062],\n",
      "        [ 1.9688,  1.9688,  1.9688,  ...,  1.9688,  1.9688,  1.9688]],\n",
      "       device='cuda:0')\n",
      "Shape of grid: torch.Size([81920, 3])\n",
      "grid: tensor([[ 0.0500, -1.9688, -1.9688],\n",
      "        [ 0.0500, -1.9062, -1.9688],\n",
      "        [ 0.0500, -1.8438, -1.9688],\n",
      "        ...,\n",
      "        [ 1.0000,  1.8438,  1.9688],\n",
      "        [ 1.0000,  1.9062,  1.9688],\n",
      "        [ 1.0000,  1.9688,  1.9688]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "t_span, x_span, y_span = metadata['t_span'], metadata['x_span'], metadata['y_span']\n",
    "\n",
    "nt, nx, ny = len(t_span), len(x_span), len(y_span) # number of discretizations in time, location_x and location_y.\n",
    "print(\"nt =\",nt, \", nx =\",nx, \"ny =\",ny)\n",
    "print(\"Shape of t-span, x-span, and y-span:\",t_span.shape, x_span.shape, y_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "print(\"y-span:\", y_span)\n",
    "\n",
    "L = 2.         # Simulation domain [-L, L]^2\n",
    "T = 1.         # Simulation time\n",
    "D_value = 1.0  # Diffusion coefficient  \n",
    "\n",
    "grid = torch.vstack((t_span.repeat_interleave(ny*nx), \n",
    "              x_span.flatten().repeat(nt),\n",
    "              y_span.flatten().repeat(nt))).T\n",
    "print(\"Shape of grid:\", grid.shape) # (nt*nx*ny, 3)\n",
    "print(\"grid:\", grid) # time, location_x, location_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d129b4ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:21.598729Z",
     "iopub.status.busy": "2025-03-13T18:14:21.598571Z",
     "iopub.status.idle": "2025-03-13T18:14:21.713693Z",
     "shell.execute_reply": "2025-03-13T18:14:21.713358Z"
    },
    "papermill": {
     "duration": 0.119203,
     "end_time": "2025-03-13T18:14:21.714206",
     "exception": false,
     "start_time": "2025-03-13T18:14:21.595003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(NpzFile '../../data/2D_Stove/stove_full_solution_fields.npz' with keys: full_solution_all_groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(NpzFile '../../data/2D_Stove/stove_source_fields_only.npz' with keys: only_sources_all_groups)\n"
     ]
    }
   ],
   "source": [
    "stove_full_solution_fields_groups_np = np.load(os.path.join('..','..','data/2D_Stove/stove_full_solution_fields.npz'), allow_pickle=True)\n",
    "print(stove_full_solution_fields_groups_np.keys())\n",
    "\n",
    "stove_source_fields_only_groups_np = np.load(os.path.join('..','..','data/2D_Stove/stove_source_fields_only.npz'), allow_pickle=True)\n",
    "print(stove_source_fields_only_groups_np.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b358345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:21.721404Z",
     "iopub.status.busy": "2025-03-13T18:14:21.721190Z",
     "iopub.status.idle": "2025-03-13T18:14:24.399101Z",
     "shell.execute_reply": "2025-03-13T18:14:24.398653Z"
    },
    "papermill": {
     "duration": 2.68215,
     "end_time": "2025-03-13T18:14:24.399777",
     "exception": false,
     "start_time": "2025-03-13T18:14:21.717627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stove_full_solution_fields_groups:\n",
      "Group 0:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "  Output Samples shape: torch.Size([10, 20, 64, 64])\n",
      "Group 1:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "  Output Samples shape: torch.Size([10, 20, 64, 64])\n",
      "\u001b[32m######################################################################################################################################################################################################################################\u001b[0m\n",
      "stove_source_fields_only_groups:\n",
      "Group 0:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "Group 1:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "\u001b[32m######################################################################################################################################################################################################################################\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Convert the NumPy groups to Pytorch\n",
    "stove_full_solution_fields_groups = convert_groupdict_to_torch(stove_full_solution_fields_groups_np, device)\n",
    "stove_source_fields_only_groups = convert_groupdict_to_torch(stove_source_fields_only_groups_np, device)\n",
    "\n",
    "# Check the shapes\n",
    "print('stove_full_solution_fields_groups:')\n",
    "check_shapes(stove_full_solution_fields_groups)\n",
    "print(colored('#' * 230, 'green'))\n",
    "\n",
    "print('stove_source_fields_only_groups:')\n",
    "check_shapes(stove_source_fields_only_groups)\n",
    "print(colored('#' * 230, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2018762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:24.408628Z",
     "iopub.status.busy": "2025-03-13T18:14:24.408408Z",
     "iopub.status.idle": "2025-03-13T18:14:24.413343Z",
     "shell.execute_reply": "2025-03-13T18:14:24.413037Z"
    },
    "papermill": {
     "duration": 0.009591,
     "end_time": "2025-03-13T18:14:24.414032",
     "exception": false,
     "start_time": "2025-03-13T18:14:24.404441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Group:\n",
      "\u001b[31m####################\u001b[0m\n",
      "Group Data Shapes:\n",
      "Group 4:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "  Output Samples shape: torch.Size([10, 20, 64, 64])\n",
      "--------------------------------------------------\n",
      "Group 10:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "  Output Samples shape: torch.Size([10, 20, 64, 64])\n",
      "--------------------------------------------------\n",
      "\u001b[32m######################################################################################################################################################################################################################################\u001b[0m\n",
      "Testing Group:\n",
      "\u001b[31m####################\u001b[0m\n",
      "Group Data Shapes:\n",
      "Group 26:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "  Output Samples shape: torch.Size([10, 20, 64, 64])\n",
      "--------------------------------------------------\n",
      "Group 35:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "  Output Samples shape: torch.Size([10, 20, 64, 64])\n",
      "--------------------------------------------------\n",
      "\u001b[32m######################################################################################################################################################################################################################################\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing groups\n",
    "train_group, test_group = split_groups(stove_full_solution_fields_groups, seed, train_size=50, test_size=10)\n",
    "\n",
    "# Print the shapes for verification\n",
    "print(\"Training Group:\")\n",
    "print(colored('#' * 20, 'red'))\n",
    "print_group_shapes(train_group)\n",
    "print(colored('#' * 230, 'green'))\n",
    "print(\"Testing Group:\")\n",
    "print(colored('#' * 20, 'red'))\n",
    "print_group_shapes(test_group)\n",
    "print(colored('#' * 230, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04d7d29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:24.421604Z",
     "iopub.status.busy": "2025-03-13T18:14:24.421456Z",
     "iopub.status.idle": "2025-03-13T18:14:24.431563Z",
     "shell.execute_reply": "2025-03-13T18:14:24.431254Z"
    },
    "papermill": {
     "duration": 0.014508,
     "end_time": "2025-03-13T18:14:24.432226",
     "exception": false,
     "start_time": "2025-03-13T18:14:24.417718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shapes:\n",
      "  Input Parameters Shape: torch.Size([500, 4])\n",
      "  Input Samples Shape: torch.Size([500, 64, 64])\n",
      "  Output Samples Shape: torch.Size([500, 20, 64, 64])\n",
      "Test Data Shapes:\n",
      "  Input Parameters Shape: torch.Size([100, 4])\n",
      "  Input Samples Shape: torch.Size([100, 64, 64])\n",
      "  Output Samples Shape: torch.Size([100, 20, 64, 64])\n",
      "\u001b[31m####################\u001b[0m\n",
      "Shape of input_parameters_train: torch.Size([500, 4])\n",
      "Shape of input_parameters_test: torch.Size([100, 4])\n",
      "Shape of inputs_train: torch.Size([500, 64, 64])\n",
      "Shape of inputs_test: torch.Size([100, 64, 64])\n",
      "Shape of outputs_train: torch.Size([500, 20, 64, 64])\n",
      "Shape of outputs_test: torch.Size([100, 20, 64, 64])\n",
      "\u001b[32m####################\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data = load_and_combine_groups(train_group, 'Train', combine=True, device=device)\n",
    "input_parameters_train = train_data['input_parameters']\n",
    "inputs_train = train_data['input_samples']\n",
    "outputs_train = train_data['output_samples']\n",
    "\n",
    "test_data = load_and_combine_groups(test_group, 'Test', combine=True, device=device)\n",
    "input_parameters_test = test_data['input_parameters']\n",
    "inputs_test = test_data['input_samples']\n",
    "outputs_test = test_data['output_samples']\n",
    "\n",
    "print(colored('#' * 20, 'red'))\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of input_parameters_train:\", input_parameters_train.shape)\n",
    "print(\"Shape of input_parameters_test:\", input_parameters_test.shape)\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print(colored('#' * 20, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d1bc32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:24.439435Z",
     "iopub.status.busy": "2025-03-13T18:14:24.439297Z",
     "iopub.status.idle": "2025-03-13T18:14:24.449788Z",
     "shell.execute_reply": "2025-03-13T18:14:24.449480Z"
    },
    "papermill": {
     "duration": 0.0149,
     "end_time": "2025-03-13T18:14:24.450468",
     "exception": false,
     "start_time": "2025-03-13T18:14:24.435568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source only Data Shapes:\n",
      "  Input Parameters Shape: torch.Size([2000, 4])\n",
      "  Input Samples Shape: torch.Size([2000, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "stove_source_fields_only = load_and_combine_groups(stove_source_fields_only_groups, 'Source only', combine=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "745cb5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:24.457797Z",
     "iopub.status.busy": "2025-03-13T18:14:24.457658Z",
     "iopub.status.idle": "2025-03-13T18:14:24.461136Z",
     "shell.execute_reply": "2025-03-13T18:14:24.460833Z"
    },
    "papermill": {
     "duration": 0.007914,
     "end_time": "2025-03-13T18:14:24.461796",
     "exception": false,
     "start_time": "2025-03-13T18:14:24.453882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_parameters_train_used: torch.Size([150, 4])\n",
      "Shape of inputs_train_used: torch.Size([150, 64, 64])\n",
      "Shape of outputs_train_used: torch.Size([150, 20, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term\n",
    "input_parameters_train_used = input_parameters_train[:n_used, :]\n",
    "print(\"Shape of input_parameters_train_used:\", input_parameters_train_used.shape)\n",
    "inputs_train_used = inputs_train[:n_used, :, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45b6e869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:24.469087Z",
     "iopub.status.busy": "2025-03-13T18:14:24.468951Z",
     "iopub.status.idle": "2025-03-13T18:14:24.470814Z",
     "shell.execute_reply": "2025-03-13T18:14:24.470483Z"
    },
    "papermill": {
     "duration": 0.006306,
     "end_time": "2025-03-13T18:14:24.471460",
     "exception": false,
     "start_time": "2025-03-13T18:14:24.465154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_dim = 16 # d_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5719c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:24.478922Z",
     "iopub.status.busy": "2025-03-13T18:14:24.478786Z",
     "iopub.status.idle": "2025-03-13T18:14:24.812229Z",
     "shell.execute_reply": "2025-03-13T18:14:24.811849Z"
    },
    "papermill": {
     "duration": 0.338003,
     "end_time": "2025-03-13T18:14:24.812920",
     "exception": false,
     "start_time": "2025-03-13T18:14:24.474917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATENT BRANCH-NET SUMMARY:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 40, 62, 62]             400\n",
      "              ReLU-2           [-1, 40, 62, 62]               0\n",
      "         AvgPool2d-3           [-1, 40, 31, 31]               0\n",
      "            Conv2d-4           [-1, 60, 29, 29]          21,660\n",
      "              ReLU-5           [-1, 60, 29, 29]               0\n",
      "         AvgPool2d-6           [-1, 60, 14, 14]               0\n",
      "            Conv2d-7           [-1, 80, 12, 12]          43,280\n",
      "              ReLU-8           [-1, 80, 12, 12]               0\n",
      "         AvgPool2d-9             [-1, 80, 6, 6]               0\n",
      "           Conv2d-10            [-1, 100, 4, 4]          72,100\n",
      "             ReLU-11            [-1, 100, 4, 4]               0\n",
      "        AvgPool2d-12            [-1, 100, 2, 2]               0\n",
      "          Flatten-13                  [-1, 400]               0\n",
      "           Linear-14                  [-1, 150]          60,150\n",
      "             ReLU-15                  [-1, 150]               0\n",
      "           Linear-16                  [-1, 150]          22,650\n",
      "             ReLU-17                  [-1, 150]               0\n",
      "           Linear-18                  [-1, 256]          38,656\n",
      "================================================================\n",
      "Total params: 258,896\n",
      "Trainable params: 258,896\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 3.73\n",
      "Params size (MB): 0.99\n",
      "Estimated Total Size (MB): 4.74\n",
      "----------------------------------------------------------------\n",
      "####################################################################################################\n",
      "LATENT TRUNK-NET SUMMARY:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]             256\n",
      "              SiLU-2                  [-1, 128]               0\n",
      "            Linear-3                  [-1, 128]          16,512\n",
      "              SiLU-4                  [-1, 128]               0\n",
      "            Linear-5                  [-1, 128]          16,512\n",
      "              SiLU-6                  [-1, 128]               0\n",
      "            Linear-7                  [-1, 128]          16,512\n",
      "              SiLU-8                  [-1, 128]               0\n",
      "            Linear-9                  [-1, 256]          33,024\n",
      "================================================================\n",
      "Total params: 82,816\n",
      "Trainable params: 82,816\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.32\n",
      "Estimated Total Size (MB): 0.33\n",
      "----------------------------------------------------------------\n",
      "####################################################################################################\n",
      "RECONSTRUCTION BRANCH-NET SUMMARY:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]           2,176\n",
      "              SiLU-2                  [-1, 128]               0\n",
      "            Linear-3                  [-1, 128]          16,512\n",
      "              SiLU-4                  [-1, 128]               0\n",
      "            Linear-5                  [-1, 128]          16,512\n",
      "              SiLU-6                  [-1, 128]               0\n",
      "            Linear-7                  [-1, 128]          16,512\n",
      "              SiLU-8                  [-1, 128]               0\n",
      "            Linear-9                  [-1, 128]          16,512\n",
      "================================================================\n",
      "Total params: 68,224\n",
      "Trainable params: 68,224\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.26\n",
      "Estimated Total Size (MB): 0.27\n",
      "----------------------------------------------------------------\n",
      "####################################################################################################\n",
      "RECONSTRUCTION TRUNK-NET SUMMARY:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]             384\n",
      "              SiLU-2                  [-1, 128]               0\n",
      "            Linear-3                  [-1, 128]          16,512\n",
      "              SiLU-4                  [-1, 128]               0\n",
      "            Linear-5                  [-1, 128]          16,512\n",
      "              SiLU-6                  [-1, 128]               0\n",
      "            Linear-7                  [-1, 128]          16,512\n",
      "              SiLU-8                  [-1, 128]               0\n",
      "            Linear-9                  [-1, 128]          16,512\n",
      "================================================================\n",
      "Total params: 66,432\n",
      "Trainable params: 66,432\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 0.26\n",
      "----------------------------------------------------------------\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "input_neurons_latent_branch: Number of input neurons in the latent_branch net.\n",
    "input_neurons_latent_trunk: Number of input neurons in the latent_trunk net.\n",
    "latent_p: Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\"\"\"\n",
    "latent_p = latent_dim*16 # Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\n",
    "input_neurons_latent_branch = (ny, nx) # Specify input size of image as a tuple (height, width)\n",
    "n_channels = 1\n",
    "num_filters = [40, 60, 80, 100]\n",
    "filter_sizes = [3, 3, 3, 3]\n",
    "strides = [1]*len(num_filters)\n",
    "paddings = [0]*len(num_filters)\n",
    "poolings = [('avg', 2, 2), ('avg', 2, 2), ('avg', 2, 2), ('avg', 2, 2)]  # Pooling layer specification (type, kernel_size, stride)\n",
    "end_MLP_layersizes = [150, 150, latent_p]\n",
    "activation = nn.ReLU() # nn.SiLU() #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_branch_net = ConvNet(input_neurons_latent_branch, n_channels, num_filters, filter_sizes, strides, paddings, poolings, end_MLP_layersizes, activation)\n",
    "latent_branch_net.to(device)\n",
    "# print(latent_branch_net)\n",
    "print('LATENT BRANCH-NET SUMMARY:')\n",
    "summary(latent_branch_net, input_size=(n_channels, ny, nx))  # input shape is (channels, height, width)\n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_latent_trunk = 1 # 1 corresponds to t\n",
    "latent_trunk_net = DenseNet(layersizes=[input_neurons_latent_trunk] + [128]*4 + [latent_p], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_trunk_net.to(device)\n",
    "# print(latent_trunk_net)\n",
    "print('LATENT TRUNK-NET SUMMARY:')\n",
    "summary(latent_trunk_net, input_size=(input_neurons_latent_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "\"\"\"\n",
    "input_neurons_reconstruction_branch: Number of input neurons in the reconstruction_branch net.\n",
    "input_neurons_reconstruction_trunk: Number of input neurons in the reconstruction_trunk net.\n",
    "reconstruction_q: Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\"\"\"\n",
    "reconstruction_q = 128 # Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\n",
    "input_neurons_reconstruction_branch = latent_dim # d_z\n",
    "reconstruction_branch_net = DenseNet(layersizes=[input_neurons_reconstruction_branch] + [128]*4 + [reconstruction_q], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_branch_net.to(device)\n",
    "# print(reconstruction_branch_net)\n",
    "print('RECONSTRUCTION BRANCH-NET SUMMARY:')\n",
    "summary(reconstruction_branch_net, input_size=(input_neurons_reconstruction_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_reconstruction_trunk = 2 # 2 corresponds to x and y\n",
    "reconstruction_trunk_net = DenseNet(layersizes=[input_neurons_reconstruction_trunk] + [128]*4 + [reconstruction_q], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_trunk_net.to(device)\n",
    "# print(reconstruction_trunk_net)\n",
    "print('RECONSTRUCTION TRUNK-NET SUMMARY:')\n",
    "summary(reconstruction_trunk_net, input_size=(input_neurons_reconstruction_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Latent_NO_model(latent_branch_net, latent_trunk_net, latent_dim, reconstruction_branch_net, reconstruction_trunk_net)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb86a175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:24.821405Z",
     "iopub.status.busy": "2025-03-13T18:14:24.821124Z",
     "iopub.status.idle": "2025-03-13T18:14:24.823821Z",
     "shell.execute_reply": "2025-03-13T18:14:24.823496Z"
    },
    "papermill": {
     "duration": 0.007336,
     "end_time": "2025-03-13T18:14:24.824376",
     "exception": false,
     "start_time": "2025-03-13T18:14:24.817040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of learnable parameters: 476368\n"
     ]
    }
   ],
   "source": [
    "num_learnable_parameters = (count_learnable_parameters(latent_branch_net)\n",
    "                            + count_learnable_parameters(latent_trunk_net)\n",
    "                            + count_learnable_parameters(reconstruction_branch_net)\n",
    "                            + count_learnable_parameters(reconstruction_trunk_net))\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4908032c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:24.832232Z",
     "iopub.status.busy": "2025-03-13T18:14:24.831988Z",
     "iopub.status.idle": "2025-03-13T18:14:24.835002Z",
     "shell.execute_reply": "2025-03-13T18:14:24.834703Z"
    },
    "papermill": {
     "duration": 0.007448,
     "end_time": "2025-03-13T18:14:24.835492",
     "exception": false,
     "start_time": "2025-03-13T18:14:24.828044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def u_pred(net, inputs, t, x, y):\n",
    "    # enforcing ICs and BCs in hard way by multiplying model output with factor\n",
    "    # factor = t*(x-(-L))*(x-L)*(y-(-L))*(y-L)/(T*(2*L)*(2*L)*(2*L)*(2*L)) calculating this by appropriate broadcasting\n",
    "    t_term = t.unsqueeze(0)  # (1, neval_t, 1)\n",
    "    x_term = ((x-(-L))*(x-L)).squeeze() # (neval_loc,)\n",
    "    y_term = ((y-(-L))*(y-L)).squeeze() # (neval_loc,)\n",
    "    factor = t_term * x_term * y_term / (T*(2*L)*(2*L)*(2*L)*(2*L)) # (1, neval_t, neval_loc) due to broadcasting\n",
    "    latent_prediction, reconstruction_prediction = net(inputs, t, torch.hstack([x, y]))\n",
    "    u = reconstruction_prediction*factor # (bs, neval_t, neval_loc) # broadcasted element-wise\n",
    "    return latent_prediction, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34ffb765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:24.843164Z",
     "iopub.status.busy": "2025-03-13T18:14:24.842930Z",
     "iopub.status.idle": "2025-03-13T18:14:24.847500Z",
     "shell.execute_reply": "2025-03-13T18:14:24.847200Z"
    },
    "papermill": {
     "duration": 0.008958,
     "end_time": "2025-03-13T18:14:24.847999",
     "exception": false,
     "start_time": "2025-03-13T18:14:24.839041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, source_fields_parameters, source_fields, t, x, y):\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x, tangent_y = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device), torch.ones(y.shape).to(device)\n",
    "    ut = FWDAD_first_order_derivative(lambda t: u_pred(net, source_fields, t, x, y)[1], t, tangent_t) # (bs, neval_t, neval_loc)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: u_pred(net, source_fields, t, x, y)[1], x, tangent_x) # (bs, neval_t, neval_loc)\n",
    "    uyy = FWDAD_second_order_derivative(lambda y: u_pred(net, source_fields, t, x, y)[1], y, tangent_y) # (bs, neval_t, neval_loc)\n",
    "    \n",
    "    bs_ = source_fields.shape[0]\n",
    "    sf_values_ = torch.zeros((bs_, x.shape[0])).to(device)\n",
    "    for j in range(bs_):\n",
    "        source_class = Source(a=source_fields_parameters[j][3], r=source_fields_parameters[j][2], \n",
    "                      x=x, y=y, \n",
    "                      xc=0., yc=0.,\n",
    "                      device=device)\n",
    "        shape = get_key_from_value(shape_map, source_fields_parameters[j, 0])\n",
    "        sf_values_[j] = source_class.type_source(shape, num_sides=int(source_fields_parameters[j, 1])).flatten() # source function: s(x, y) values\n",
    "    sf_values__ = sf_values_.unsqueeze(1)\n",
    "    # Repeat elements along neval_loc for neval_t times and reshape\n",
    "    sf_values = sf_values__.repeat(1, t.shape[0], 1) # (bs, neval_t, neval_loc) # s(x, y) values are same for all times \n",
    "\n",
    "    pde_residual = (ut - (D_value*uxx) - (D_value*uyy) - sf_values)**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47e40852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T18:14:24.855859Z",
     "iopub.status.busy": "2025-03-13T18:14:24.855625Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-03-13T18:14:24.851625",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - loss = 0.036290, data-driven loss = 0.002246, pinn loss = 0.034044, learning rate = 0.001000, test loss = 0.001710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500 - loss = 0.009632, data-driven loss = 0.000023, pinn loss = 0.009609, learning rate = 0.001000, test loss = 0.000023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000 - loss = 0.004711, data-driven loss = 0.000005, pinn loss = 0.004706, learning rate = 0.001000, test loss = 0.000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1500 - loss = 0.002648, data-driven loss = 0.000002, pinn loss = 0.002646, learning rate = 0.001000, test loss = 0.000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000 - loss = 0.002241, data-driven loss = 0.000003, pinn loss = 0.002238, learning rate = 0.001000, test loss = 0.000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2500 - loss = 0.001550, data-driven loss = 0.000002, pinn loss = 0.001548, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000 - loss = 0.001685, data-driven loss = 0.000001, pinn loss = 0.001684, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3500 - loss = 0.001701, data-driven loss = 0.000003, pinn loss = 0.001698, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000 - loss = 0.000990, data-driven loss = 0.000001, pinn loss = 0.000990, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4500 - loss = 0.000989, data-driven loss = 0.000002, pinn loss = 0.000988, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000 - loss = 0.000740, data-driven loss = 0.000001, pinn loss = 0.000740, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5500 - loss = 0.000774, data-driven loss = 0.000001, pinn loss = 0.000774, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6000 - loss = 0.000641, data-driven loss = 0.000000, pinn loss = 0.000640, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6500 - loss = 0.000657, data-driven loss = 0.000001, pinn loss = 0.000656, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7000 - loss = 0.000657, data-driven loss = 0.000000, pinn loss = 0.000656, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7500 - loss = 0.000534, data-driven loss = 0.000001, pinn loss = 0.000533, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8000 - loss = 0.000814, data-driven loss = 0.000007, pinn loss = 0.000807, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8500 - loss = 0.000906, data-driven loss = 0.000004, pinn loss = 0.000901, learning rate = 0.001000, test loss = 0.000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9000 - loss = 0.000523, data-driven loss = 0.000001, pinn loss = 0.000523, learning rate = 0.001000, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9500 - loss = 0.000459, data-driven loss = 0.000000, pinn loss = 0.000458, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000 - loss = 0.001543, data-driven loss = 0.000009, pinn loss = 0.001535, learning rate = 0.001000, test loss = 0.000007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10500 - loss = 0.000467, data-driven loss = 0.000001, pinn loss = 0.000466, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11000 - loss = 0.000409, data-driven loss = 0.000000, pinn loss = 0.000408, learning rate = 0.001000, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11500 - loss = 0.000423, data-driven loss = 0.000000, pinn loss = 0.000423, learning rate = 0.001000, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12000 - loss = 0.000421, data-driven loss = 0.000000, pinn loss = 0.000421, learning rate = 0.001000, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12500 - loss = 0.000331, data-driven loss = 0.000000, pinn loss = 0.000330, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13000 - loss = 0.000407, data-driven loss = 0.000001, pinn loss = 0.000406, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13500 - loss = 0.000373, data-driven loss = 0.000000, pinn loss = 0.000373, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14000 - loss = 0.000412, data-driven loss = 0.000001, pinn loss = 0.000412, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14500 - loss = 0.000360, data-driven loss = 0.000001, pinn loss = 0.000360, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15000 - loss = 0.000370, data-driven loss = 0.000002, pinn loss = 0.000368, learning rate = 0.001000, test loss = 0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15500 - loss = 0.000338, data-driven loss = 0.000001, pinn loss = 0.000337, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16000 - loss = 0.000454, data-driven loss = 0.000001, pinn loss = 0.000454, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16500 - loss = 0.000335, data-driven loss = 0.000001, pinn loss = 0.000334, learning rate = 0.001000, test loss = 0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17000 - loss = 0.000423, data-driven loss = 0.000001, pinn loss = 0.000422, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17500 - loss = 0.000353, data-driven loss = 0.000001, pinn loss = 0.000353, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18000 - loss = 0.000556, data-driven loss = 0.000001, pinn loss = 0.000556, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18500 - loss = 0.000364, data-driven loss = 0.000001, pinn loss = 0.000363, learning rate = 0.001000, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19000 - loss = 0.000279, data-driven loss = 0.000000, pinn loss = 0.000279, learning rate = 0.001000, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19500 - loss = 0.000391, data-driven loss = 0.000001, pinn loss = 0.000390, learning rate = 0.001000, test loss = 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20000 - loss = 0.000335, data-driven loss = 0.000000, pinn loss = 0.000335, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20500 - loss = 0.000272, data-driven loss = 0.000000, pinn loss = 0.000272, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21000 - loss = 0.000254, data-driven loss = 0.000000, pinn loss = 0.000254, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21500 - loss = 0.000247, data-driven loss = 0.000000, pinn loss = 0.000247, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22000 - loss = 0.000261, data-driven loss = 0.000000, pinn loss = 0.000261, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22500 - loss = 0.000289, data-driven loss = 0.000000, pinn loss = 0.000289, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23000 - loss = 0.000274, data-driven loss = 0.000000, pinn loss = 0.000274, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23500 - loss = 0.000294, data-driven loss = 0.000000, pinn loss = 0.000294, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24000 - loss = 0.000259, data-driven loss = 0.000000, pinn loss = 0.000259, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24500 - loss = 0.000327, data-driven loss = 0.000000, pinn loss = 0.000327, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25000 - loss = 0.000229, data-driven loss = 0.000000, pinn loss = 0.000229, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25500 - loss = 0.000242, data-driven loss = 0.000000, pinn loss = 0.000242, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26000 - loss = 0.000265, data-driven loss = 0.000000, pinn loss = 0.000265, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26500 - loss = 0.000241, data-driven loss = 0.000000, pinn loss = 0.000240, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27000 - loss = 0.000230, data-driven loss = 0.000000, pinn loss = 0.000230, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27500 - loss = 0.000226, data-driven loss = 0.000000, pinn loss = 0.000226, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28000 - loss = 0.000239, data-driven loss = 0.000000, pinn loss = 0.000239, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28500 - loss = 0.000233, data-driven loss = 0.000000, pinn loss = 0.000233, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29000 - loss = 0.000232, data-driven loss = 0.000000, pinn loss = 0.000232, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29500 - loss = 0.000247, data-driven loss = 0.000000, pinn loss = 0.000247, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30000 - loss = 0.000277, data-driven loss = 0.000000, pinn loss = 0.000277, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30500 - loss = 0.000239, data-driven loss = 0.000000, pinn loss = 0.000239, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31000 - loss = 0.000233, data-driven loss = 0.000000, pinn loss = 0.000233, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31500 - loss = 0.000216, data-driven loss = 0.000000, pinn loss = 0.000216, learning rate = 0.000100, test loss = 0.000000\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 128 # Batch size\n",
    "\n",
    "neval_t = 20  # Number of time points at which latent output field is evaluated for a given input sample.\n",
    "neval_x = 64 \n",
    "neval_y = 64 \n",
    "# neval_loc = neval_x*neval_y  # Number of locations at which output field is evaluated at each time point.\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x*neval_y}  # Number of collocation points within the domain.\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "test_iteration_list, test_loss_list = [], []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "    else:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        inputs_train_used_batch = inputs_train_used.reshape(-1, 1, ny, nx)[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used.reshape(-1, nt, nx*ny)[indices_datadriven[0:bs]]\n",
    "        # print(f\"Shape of inputs_train_used_batch:\", inputs_train_used_batch.shape) # (bs, no. of channels, height, width)\n",
    "        # print(f\"Shape of outputs_train_used_batch:\", outputs_train_used_batch.shape) # (bs, nt, nx*ny)\n",
    "\n",
    "        _, reconstruction_predicted_values = u_pred(model, inputs_train_used_batch, \n",
    "                                                          t_span.reshape(-1, 1), \n",
    "                                                          x_span.flatten().reshape(-1,1), \n",
    "                                                          y_span.flatten().reshape(-1,1)) # (bs, nt, latent_dim), (bs, nt, nx*ny)\n",
    "        reconstruction_target_values = outputs_train_used_batch # (bs, nt, nx*ny)\n",
    "        datadriven_loss = nn.MSELoss()(reconstruction_predicted_values, reconstruction_target_values)\n",
    "        # print('*********')\n",
    "    \n",
    "    num_samples = stove_source_fields_only['input_samples'].shape[0]\n",
    "    indices_pinn = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "    input_parameters_batch = stove_source_fields_only['input_parameters'][indices_pinn[0:bs]]\n",
    "    inputs_batch = stove_source_fields_only['input_samples'].reshape(-1, 1, ny, nx)[indices_pinn[0:bs]]\n",
    "    #print(f\"Shape of inputs_train_batch[{i}]:\", inputs_batch.shape) # (bs, no. of channels, height, width)\n",
    "\n",
    "    # points within the domain\n",
    "    tc = td.uniform.Uniform(0., T).sample((neval_c['t'], 1)).to(device)\n",
    "    xc = td.uniform.Uniform(-L, L).sample((neval_c['loc'], 1)).to(device)\n",
    "    yc = td.uniform.Uniform(-L, L).sample((neval_c['loc'], 1)).to(device)\n",
    "\n",
    "    pinn_loss = loss_pde_residual(model, input_parameters_batch, inputs_batch, tc, xc, yc) \n",
    "    # print('*********')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if iteration % 500 == 0:\n",
    "        # Test loss calculation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_predicted_values = u_pred(model, inputs_test.reshape(-1, 1, ny, nx), \n",
    "                                                                  t_span.reshape(-1, 1), \n",
    "                                                                  x_span.flatten().reshape(-1,1), \n",
    "                                                                  y_span.flatten().reshape(-1,1))[1] # (bs, nt, nx*ny)\n",
    "            test_loss = nn.MSELoss()(test_predicted_values, outputs_test.reshape(-1, nt, nx*ny))\n",
    "            test_iteration_list.append(iteration)\n",
    "            test_loss_list.append(test_loss.item())  \n",
    "        model.train()  # Switch back to training mode\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f,' % optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "              'test loss = %f' % test_loss)\n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    \n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "    np.save(os.path.join(resultdir,'test_iteration_list.npy'), np.asarray(test_iteration_list))\n",
    "    np.save(os.path.join(resultdir, 'test_loss_list.npy'), np.asarray(test_loss_list)) \n",
    "\n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "    \n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save)\n",
    "\n",
    "plot_testing_loss(resultdir, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_training_testing_loss(resultdir, iteration_list, loss_list, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)  \n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d2881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T11:56:20.136931Z",
     "iopub.status.busy": "2024-12-29T11:56:20.136541Z",
     "iopub.status.idle": "2024-12-29T11:56:20.145273Z",
     "shell.execute_reply": "2024-12-29T11:56:20.144933Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir, 'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c742e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T11:56:20.162773Z",
     "iopub.status.busy": "2024-12-29T11:56:20.162623Z",
     "iopub.status.idle": "2024-12-29T12:01:54.858939Z",
     "shell.execute_reply": "2024-12-29T12:01:54.858591Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "_, reconstruction_predictions_test = u_pred(model, inputs_test.reshape(-1, 1, ny, nx), \n",
    "                                                                  t_span.reshape(-1, 1), \n",
    "                                                                  x_span.flatten().reshape(-1,1), \n",
    "                                                                  y_span.flatten().reshape(-1,1)) # (bs, nt, latent_dim), (bs, nt, nx*ny)\n",
    "# print(reconstruction_predictions_test.shape)\n",
    "\n",
    "mse_list, r2score_list, relerror_list = [], [], []\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "    \n",
    "    reconstruction_prediction_i = reconstruction_predictions_test[i].unsqueeze(0)# (1, nt, nx*ny)\n",
    "    reconstruction_target_i = outputs_test[i].reshape(nt, nx*ny).unsqueeze(0) # (1, nt, nx*ny)\n",
    "\n",
    "    reconstruction_mse_i = F.mse_loss(reconstruction_prediction_i.cpu(), reconstruction_target_i.cpu())\n",
    "    mse_i = reconstruction_mse_i\n",
    "    \n",
    "    r2score_i = metrics.r2_score(reconstruction_target_i.flatten().cpu().detach().numpy(), reconstruction_prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy() - reconstruction_prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy())\n",
    "        \n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item())\n",
    "    \n",
    "    # Plot the full solution-field for few cases (2 groups i.e., 10*2=20):\n",
    "    if (i+1) <= 20:\n",
    "        print(colored('TEST SAMPLE '+str(i+1), 'red'))\n",
    "        shape = get_key_from_value(shape_map, input_parameters_test[i, 0])\n",
    "        print(colored(f\"Shape = {shape}, r = {input_parameters_test[i,2]:.3f}, a_value = {input_parameters_test[i,3]:.3f}\", 'red'))\n",
    "        \n",
    "        r2score_i = float('%.4f'%r2score_i)\n",
    "        relerror_i = float('%.4f'%relerror_i)\n",
    "        print('Rel. L2 Error = '+str(relerror_i)+', R2 score = '+str(r2score_i))\n",
    "        \n",
    "        # Plotting \n",
    "        plot_source(i, x_span, y_span, inputs_test[i], f\"{shape.capitalize()} Source\", 'hot', resultdir, save)\n",
    "        \n",
    "        cmap = 'hot'  # Color map\n",
    "        fontsize = 14  # Font size for labels and titles\n",
    "        levels = 100\n",
    "        plot_solution(i, x_span, y_span, reconstruction_target_i.reshape(nt,ny,nx), t_span, f\"True Solution for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir, save, 'True-Solution')\n",
    "        plot_solution(i, x_span, y_span, reconstruction_prediction_i.reshape(nt,ny,nx), t_span, f\"Predicted Solution for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir, save, 'Predicted-Solution')\n",
    "        plot_solution(i, x_span, y_span, torch.abs(reconstruction_target_i.reshape(nt,ny,nx) - reconstruction_prediction_i.reshape(nt,ny,nx)), t_span, f\"Absolute error for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir, save, 'Absolute error')\n",
    "        print(colored('#'*230, 'green'))\n",
    "        \n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0b218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T12:01:55.644180Z",
     "iopub.status.busy": "2024-12-29T12:01:55.643857Z",
     "iopub.status.idle": "2024-12-29T12:01:55.715833Z",
     "shell.execute_reply": "2024-12-29T12:01:55.715446Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"input_parameters_test\": input_parameters_test.cpu(),\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": reconstruction_predictions_test.reshape(-1, nt, ny, nx).cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6d8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T12:01:56.348070Z",
     "iopub.status.busy": "2024-12-29T12:01:56.347527Z",
     "iopub.status.idle": "2024-12-29T12:01:56.350654Z",
     "shell.execute_reply": "2024-12-29T12:01:56.350306Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a366414",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e4665",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "b_Latent-NO.ipynb",
   "output_path": "results/b_Latent-NO/seed=0_n_used=150/output_seed=0_n_used=150.ipynb",
   "parameters": {
    "n_used": 150,
    "save": true,
    "seed": 0
   },
   "start_time": "2025-03-13T18:14:16.553012",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}