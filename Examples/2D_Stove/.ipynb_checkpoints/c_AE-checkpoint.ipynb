{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08a75ad",
   "metadata": {
    "papermill": {
     "duration": 0.002435,
     "end_time": "2025-01-09T04:57:55.507845",
     "exception": false,
     "start_time": "2025-01-09T04:57:55.505410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1395ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:57:55.512915Z",
     "iopub.status.busy": "2025-01-09T04:57:55.512515Z",
     "iopub.status.idle": "2025-01-09T04:57:58.070128Z",
     "shell.execute_reply": "2025-01-09T04:57:58.069633Z"
    },
    "papermill": {
     "duration": 2.561218,
     "end_time": "2025-01-09T04:57:58.071229",
     "exception": false,
     "start_time": "2025-01-09T04:57:55.510011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from termcolor import colored\n",
    "from sklearn import metrics\n",
    "import contextlib\n",
    "import io\n",
    "import logging\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from utils.networks import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "from utils.deeponet_networks_2d import *\n",
    "from utils.misc_stove import *\n",
    "from utils.visualizer_stove import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921aabe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:57:58.076330Z",
     "iopub.status.busy": "2025-01-09T04:57:58.076101Z",
     "iopub.status.idle": "2025-01-09T04:57:58.078438Z",
     "shell.execute_reply": "2025-01-09T04:57:58.078097Z"
    },
    "papermill": {
     "duration": 0.005711,
     "end_time": "2025-01-09T04:57:58.079218",
     "exception": false,
     "start_time": "2025-01-09T04:57:58.073507",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Tag this cell with 'parameters'\n",
    "# parameters\n",
    "save = True # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8bd328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:57:58.083963Z",
     "iopub.status.busy": "2025-01-09T04:57:58.083660Z",
     "iopub.status.idle": "2025-01-09T04:57:58.088493Z",
     "shell.execute_reply": "2025-01-09T04:57:58.088156Z"
    },
    "papermill": {
     "duration": 0.008193,
     "end_time": "2025-01-09T04:57:58.089417",
     "exception": false,
     "start_time": "2025-01-09T04:57:58.081224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','c_AE') \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d11f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:57:58.094032Z",
     "iopub.status.busy": "2025-01-09T04:57:58.093803Z",
     "iopub.status.idle": "2025-01-09T04:57:58.235146Z",
     "shell.execute_reply": "2025-01-09T04:57:58.234616Z"
    },
    "papermill": {
     "duration": 0.144609,
     "end_time": "2025-01-09T04:57:58.236010",
     "exception": false,
     "start_time": "2025-01-09T04:57:58.091401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d346e7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:57:58.241166Z",
     "iopub.status.busy": "2025-01-09T04:57:58.240889Z",
     "iopub.status.idle": "2025-01-09T04:57:58.505010Z",
     "shell.execute_reply": "2025-01-09T04:57:58.504612Z"
    },
    "papermill": {
     "duration": 0.267606,
     "end_time": "2025-01-09T04:57:58.505864",
     "exception": false,
     "start_time": "2025-01-09T04:57:58.238258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of t_span: torch.Size([20])\n",
      "Shape of x_span: torch.Size([64, 64])\n",
      "Shape of y_span: torch.Size([64, 64])\n",
      "shapes is a dictionary with 10 keys.\n"
     ]
    }
   ],
   "source": [
    "metadata_np = np.load(os.path.join('..','..','data/Stove/metadata.npz'), allow_pickle=True)\n",
    "metadata = convert_metadata_to_torch(metadata_np, device)\n",
    "for key, value in metadata.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"Shape of {key}: {value.shape}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"{key} is a dictionary with {len(value)} keys.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02558fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:57:58.511448Z",
     "iopub.status.busy": "2025-01-09T04:57:58.511040Z",
     "iopub.status.idle": "2025-01-09T04:57:58.595570Z",
     "shell.execute_reply": "2025-01-09T04:57:58.595165Z"
    },
    "papermill": {
     "duration": 0.088142,
     "end_time": "2025-01-09T04:57:58.596432",
     "exception": false,
     "start_time": "2025-01-09T04:57:58.508290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt = 20 , nx = 64 ny = 64\n",
      "Shape of t-span, x-span, and y-span: torch.Size([20]) torch.Size([64, 64]) torch.Size([64, 64])\n",
      "t-span: tensor([0.0500, 0.1000, 0.1500, 0.2000, 0.2500, 0.3000, 0.3500, 0.4000, 0.4500,\n",
      "        0.5000, 0.5500, 0.6000, 0.6500, 0.7000, 0.7500, 0.8000, 0.8500, 0.9000,\n",
      "        0.9500, 1.0000], device='cuda:0')\n",
      "x-span: tensor([[-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688],\n",
      "        [-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688],\n",
      "        [-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688],\n",
      "        ...,\n",
      "        [-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688],\n",
      "        [-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688],\n",
      "        [-1.9688, -1.9062, -1.8438,  ...,  1.8438,  1.9062,  1.9688]],\n",
      "       device='cuda:0')\n",
      "y-span: tensor([[-1.9688, -1.9688, -1.9688,  ..., -1.9688, -1.9688, -1.9688],\n",
      "        [-1.9062, -1.9062, -1.9062,  ..., -1.9062, -1.9062, -1.9062],\n",
      "        [-1.8438, -1.8438, -1.8438,  ..., -1.8438, -1.8438, -1.8438],\n",
      "        ...,\n",
      "        [ 1.8438,  1.8438,  1.8438,  ...,  1.8438,  1.8438,  1.8438],\n",
      "        [ 1.9062,  1.9062,  1.9062,  ...,  1.9062,  1.9062,  1.9062],\n",
      "        [ 1.9688,  1.9688,  1.9688,  ...,  1.9688,  1.9688,  1.9688]],\n",
      "       device='cuda:0')\n",
      "Shape of grid: torch.Size([81920, 3])\n",
      "grid: tensor([[ 0.0500, -1.9688, -1.9688],\n",
      "        [ 0.0500, -1.9062, -1.9688],\n",
      "        [ 0.0500, -1.8438, -1.9688],\n",
      "        ...,\n",
      "        [ 1.0000,  1.8438,  1.9688],\n",
      "        [ 1.0000,  1.9062,  1.9688],\n",
      "        [ 1.0000,  1.9688,  1.9688]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "t_span, x_span, y_span = metadata['t_span'], metadata['x_span'], metadata['y_span']\n",
    "\n",
    "nt, nx, ny = len(t_span), len(x_span), len(y_span) # number of discretizations in time, location_x and location_y.\n",
    "print(\"nt =\",nt, \", nx =\",nx, \"ny =\",ny)\n",
    "print(\"Shape of t-span, x-span, and y-span:\",t_span.shape, x_span.shape, y_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "print(\"y-span:\", y_span)\n",
    "\n",
    "L = 2.         # Simulation domain [-L, L]^2\n",
    "T = 1.         # Simulation time\n",
    "D_value = 1.0  # Diffusion coefficient  \n",
    "\n",
    "grid = torch.vstack((t_span.repeat_interleave(ny*nx), \n",
    "              x_span.flatten().repeat(nt),\n",
    "              y_span.flatten().repeat(nt))).T\n",
    "print(\"Shape of grid:\", grid.shape) # (nt*nx*ny, 3)\n",
    "print(\"grid:\", grid) # time, location_x, location_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d129b4ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:57:58.601885Z",
     "iopub.status.busy": "2025-01-09T04:57:58.601652Z",
     "iopub.status.idle": "2025-01-09T04:57:58.722670Z",
     "shell.execute_reply": "2025-01-09T04:57:58.722298Z"
    },
    "papermill": {
     "duration": 0.124714,
     "end_time": "2025-01-09T04:57:58.723484",
     "exception": false,
     "start_time": "2025-01-09T04:57:58.598770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(NpzFile '../../data/Stove/stove_full_solution_fields.npz' with keys: full_solution_all_groups)\n",
      "KeysView(NpzFile '../../data/Stove/stove_source_fields_only.npz' with keys: only_sources_all_groups)\n"
     ]
    }
   ],
   "source": [
    "stove_full_solution_fields_groups_np = np.load(os.path.join('..','..','data/Stove/stove_full_solution_fields.npz'), allow_pickle=True)\n",
    "print(stove_full_solution_fields_groups_np.keys())\n",
    "\n",
    "stove_source_fields_only_groups_np = np.load(os.path.join('..','..','data/Stove/stove_source_fields_only.npz'), allow_pickle=True)\n",
    "print(stove_source_fields_only_groups_np.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b358345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:57:58.729025Z",
     "iopub.status.busy": "2025-01-09T04:57:58.728818Z",
     "iopub.status.idle": "2025-01-09T04:58:01.325093Z",
     "shell.execute_reply": "2025-01-09T04:58:01.324602Z"
    },
    "papermill": {
     "duration": 2.60003,
     "end_time": "2025-01-09T04:58:01.326027",
     "exception": false,
     "start_time": "2025-01-09T04:57:58.725997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stove_full_solution_fields_groups:\n",
      "Group 0:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "  Output Samples shape: torch.Size([10, 20, 64, 64])\n",
      "Group 1:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "  Output Samples shape: torch.Size([10, 20, 64, 64])\n",
      "\u001b[32m######################################################################################################################################################################################################################################\u001b[0m\n",
      "stove_source_fields_only_groups:\n",
      "Group 0:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "Group 1:\n",
      "  Input Parameters shape: torch.Size([10, 4])\n",
      "  Input Samples shape: torch.Size([10, 64, 64])\n",
      "\u001b[32m######################################################################################################################################################################################################################################\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Convert the NumPy groups to Pytorch\n",
    "stove_full_solution_fields_groups = convert_groupdict_to_torch(stove_full_solution_fields_groups_np, device)\n",
    "stove_source_fields_only_groups = convert_groupdict_to_torch(stove_source_fields_only_groups_np, device)\n",
    "\n",
    "# Check the shapes\n",
    "print('stove_full_solution_fields_groups:')\n",
    "check_shapes(stove_full_solution_fields_groups)\n",
    "print(colored('#' * 230, 'green'))\n",
    "\n",
    "print('stove_source_fields_only_groups:')\n",
    "check_shapes(stove_source_fields_only_groups)\n",
    "print(colored('#' * 230, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9d1bc32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:58:01.332425Z",
     "iopub.status.busy": "2025-01-09T04:58:01.332234Z",
     "iopub.status.idle": "2025-01-09T04:58:01.345243Z",
     "shell.execute_reply": "2025-01-09T04:58:01.344787Z"
    },
    "papermill": {
     "duration": 0.017547,
     "end_time": "2025-01-09T04:58:01.346344",
     "exception": false,
     "start_time": "2025-01-09T04:58:01.328797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source only Data Shapes:\n",
      "  Input Parameters Shape: torch.Size([2000, 4])\n",
      "  Input Samples Shape: torch.Size([2000, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "stove_source_fields_only = load_and_combine_groups(stove_source_fields_only_groups, 'Source only', combine=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9103e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:58:01.352098Z",
     "iopub.status.busy": "2025-01-09T04:58:01.351929Z",
     "iopub.status.idle": "2025-01-09T04:58:01.356746Z",
     "shell.execute_reply": "2025-01-09T04:58:01.356328Z"
    },
    "papermill": {
     "duration": 0.009021,
     "end_time": "2025-01-09T04:58:01.357776",
     "exception": false,
     "start_time": "2025-01-09T04:58:01.348755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrapper to suppress print statements globally\n",
    "@contextlib.contextmanager\n",
    "def suppress_prints():\n",
    "    \"\"\"\n",
    "    Context manager to suppress all print statements globally.\n",
    "    \"\"\"\n",
    "    with open(os.devnull, 'w') as fnull:\n",
    "        original_stdout = sys.stdout  # Backup original stdout\n",
    "        original_stderr = sys.stderr  # Backup original stderr\n",
    "        sys.stdout = fnull  # Redirect stdout to dev/null\n",
    "        sys.stderr = fnull  # Redirect stderr to dev/null\n",
    "        try:\n",
    "            yield  # Run the wrapped code\n",
    "        finally:\n",
    "            sys.stdout = original_stdout  # Restore original stdout\n",
    "            sys.stderr = original_stderr  # Restore original stderr\n",
    "\n",
    "# Context manager to suppress matplotlib shows\n",
    "@contextlib.contextmanager\n",
    "def suppress_shows():\n",
    "    \"\"\"\n",
    "    Context manager to suppress matplotlib plots (prevents them from being displayed).\n",
    "    \"\"\"\n",
    "    original_backend = plt.get_backend()  # Backup current matplotlib backend\n",
    "    plt.switch_backend('Agg')            # Use a non-interactive backend to suppress plots\n",
    "    \n",
    "    try:\n",
    "        yield  # Run the wrapped code\n",
    "    finally:\n",
    "        plt.switch_backend(original_backend)  # Restore the original matplotlib backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d372dba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:58:01.363480Z",
     "iopub.status.busy": "2025-01-09T04:58:01.363320Z",
     "iopub.status.idle": "2025-01-09T04:58:01.367288Z",
     "shell.execute_reply": "2025-01-09T04:58:01.366880Z"
    },
    "papermill": {
     "duration": 0.008109,
     "end_time": "2025-01-09T04:58:01.368283",
     "exception": false,
     "start_time": "2025-01-09T04:58:01.360174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(stove_full_solution_fields_groups, seed):\n",
    "    # Split the data into training and testing groups\n",
    "    train_group, test_group = split_groups(stove_full_solution_fields_groups, seed, train_size=50, test_size=10)\n",
    "    \n",
    "    with suppress_prints():\n",
    "        train_data = load_and_combine_groups(groups=train_group, \n",
    "                                    title='Train', \n",
    "                                    combine=True, \n",
    "                                    device=device)\n",
    "        test_data = load_and_combine_groups(groups=test_group, \n",
    "                                    title='Test', \n",
    "                                    combine=True, \n",
    "                                    device=device)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64398ffa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:58:01.373869Z",
     "iopub.status.busy": "2025-01-09T04:58:01.373719Z",
     "iopub.status.idle": "2025-01-09T04:58:01.381173Z",
     "shell.execute_reply": "2025-01-09T04:58:01.380712Z"
    },
    "papermill": {
     "duration": 0.011769,
     "end_time": "2025-01-09T04:58:01.382455",
     "exception": false,
     "start_time": "2025-01-09T04:58:01.370686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Autoencoder class\n",
    "class Autoencoder_MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_net, decoder_net):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_net = encoder_net\n",
    "        self.decoder_net = decoder_net\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        encoded = self.encoder_net(x)\n",
    "        decoded = self.decoder_net(encoded)\n",
    "\n",
    "        return decoded\n",
    "    \n",
    "def train_autoencoder(model_AE, data_train, resultdir_, save, n_iterations_AE=20000, bs=256): \n",
    "    print (\"------STARTED-TRAINING------\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model_AE.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "    iteration_list, loss_list, learningrates_list = [], [], []\n",
    "    model_AE.train()\n",
    "    for iteration in range(n_iterations_AE):\n",
    "        \n",
    "        num_samples = len(data_train)\n",
    "        indices = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "        data_batch = data_train[indices[0:bs]]\n",
    "        #print(f\"Shape of data_batch:\", data_batch.shape) # (bs, nx*ny)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_AE(data_batch)\n",
    "        loss = nn.MSELoss()(outputs, data_batch)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_value_(model_AE.parameters(), clip_value=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if iteration % 500 == 0:\n",
    "            print('Iteration %s -' % iteration, 'loss = %.8f,' % loss,\n",
    "              'learning rate = %.8f' % optimizer.state_dict()['param_groups'][0]['lr']) \n",
    "            \n",
    "        iteration_list.append(iteration)\n",
    "        loss_list.append(loss.item())\n",
    "        learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    print (\"------ENDED-TRAINING------\")\n",
    "    \n",
    "    with suppress_shows():\n",
    "        plot_training_loss(resultdir_, iteration_list, loss_list, save)\n",
    "        plot_learningrates(resultdir_, iteration_list, learningrates_list, save)\n",
    "\n",
    "    # end timer\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time # time for AE network to train\n",
    "    print(\"Time (sec) to complete:\\n\" +str(training_time)) # time for AE network to train\n",
    "\n",
    "    return model_AE\n",
    "\n",
    "\n",
    "def evaluate_autoencoder(model_AE, data_test, mean, std):\n",
    "    model_AE.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_AE((data_test- mean)/std) #(test_size*nt, nx*ny)\n",
    "        outputs_rescaled = (outputs * std) + mean # Rescale to Original Range\n",
    "        reconstruction_loss_test = nn.MSELoss()(outputs_rescaled, data_test)\n",
    "    return reconstruction_loss_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a366414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:58:01.388445Z",
     "iopub.status.busy": "2025-01-09T04:58:01.388285Z",
     "iopub.status.idle": "2025-01-09T04:58:01.398519Z",
     "shell.execute_reply": "2025-01-09T04:58:01.398087Z"
    },
    "papermill": {
     "duration": 0.014091,
     "end_time": "2025-01-09T04:58:01.399248",
     "exception": false,
     "start_time": "2025-01-09T04:58:01.385157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main function to perform training and evaluation\n",
    "def main(stove_full_solution_fields_groups, n_used_list=[50, 100, 150, 200, 250], latent_dim=16, n_seeds=5):\n",
    "    input_dim = nx*ny\n",
    "    all_results = []  # List to store seed, n_used, and reconstruction test error\n",
    "\n",
    "    for seed in range(n_seeds):\n",
    "        print(colored('*'*115, 'red'))\n",
    "        set_seed(seed)\n",
    "        print(colored('*'*115, 'red'))\n",
    "\n",
    "        # Split data into train and test\n",
    "        train_data, test_data = split_data(stove_full_solution_fields_groups, seed)\n",
    "        \n",
    "        input_parameters_train = train_data['input_parameters']\n",
    "        inputs_train = train_data['input_samples']\n",
    "        outputs_train = train_data['output_samples']\n",
    "        \n",
    "        input_parameters_test = test_data['input_parameters']\n",
    "        inputs_test = test_data['input_samples']\n",
    "        outputs_test = test_data['output_samples']\n",
    "\n",
    "        for n_used in n_used_list:\n",
    "            # Select n_used samples\n",
    "            print(f\"n_used = {n_used}\")\n",
    "            \n",
    "            # Of these full training fields available I am using only n_used fields for learning the AE\n",
    "            input_parameters_train_used = input_parameters_train[:n_used, :]\n",
    "            # print(\"Shape of input_parameters_train_used:\", input_parameters_train_used.shape)\n",
    "            inputs_train_used = inputs_train[:n_used, :, :]\n",
    "            # print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "            outputs_train_used = outputs_train[:n_used, :, :, :]\n",
    "            print(\"Shape of outputs_train_used:\", outputs_train_used.shape)\n",
    "            \n",
    "            data_used_for_AE = outputs_train_used.reshape(-1, nx*ny)\n",
    "            print(\"Shape of data_used_for_AE:\", data_used_for_AE.shape)\n",
    "\n",
    "            # Compute mean and std across the dataset\n",
    "            mean = data_used_for_AE.mean(axis=0)\n",
    "            std = data_used_for_AE.std(axis=0)\n",
    "            std[std == 0] = 1e-16 # Avoid division by zero\n",
    "            # Scale the training data\n",
    "            data_train = (data_used_for_AE - mean) / std\n",
    "            \n",
    "            set_seed(seed)\n",
    "\n",
    "            encoder_net = DenseNet(layersizes=[input_dim] + [2048, 1024, 512, 256] + [latent_dim], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "            encoder_net.to(device)\n",
    "            # print(encoder_net)\n",
    "            # print('ENCODER-NET SUMMARY:')\n",
    "            # summary(encoder_net, input_size=(input_dim,))  \n",
    "            # print('#'*100)\n",
    "\n",
    "            decoder_net = DenseNet(layersizes=[latent_dim] + [256, 512, 1024, 2048] + [input_dim], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "            decoder_net.to(device)\n",
    "            # print(decoder_net)\n",
    "            # print('DECODER-NET SUMMARY:')\n",
    "            # summary(decoder_net, input_size=(latent_dim,))\n",
    "            # print('#'*100)\n",
    "\n",
    "            model_AE = Autoencoder_MLP(encoder_net, decoder_net)\n",
    "            model_AE.to(device);\n",
    "            \n",
    "            if save == True:\n",
    "                resultdir_ = os.path.join(resultdir,'seed='+str(seed)+'_n_used='+str(n_used)) \n",
    "                if not os.path.exists(resultdir_):\n",
    "                    os.makedirs(resultdir_)\n",
    "            else:\n",
    "                resultdir_ = None\n",
    "\n",
    "            # Train the autoencoder\n",
    "            model_AE = train_autoencoder(model_AE, data_train, resultdir_, save)\n",
    "\n",
    "            # Evaluate on test data\n",
    "            data_test = outputs_test.reshape(-1, nx*ny) #(test_size*nt, nx*ny)\n",
    "            reconstruction_test_error = evaluate_autoencoder(model_AE, data_test, mean, std)\n",
    "            print(colored(f\"Seed {seed}, n_used {n_used}: Reconstruction Test Error = {reconstruction_test_error:.2e}\", 'red'))\n",
    "            \n",
    "            # Store results for each seed and n_used\n",
    "            all_results.append({\n",
    "                'seed': seed,\n",
    "                'n_used': n_used,\n",
    "                'reconstruction_test_error': reconstruction_test_error\n",
    "            })\n",
    "\n",
    "            \n",
    "            for i in range(outputs_test.shape[0]):\n",
    "\n",
    "                # Plot the full solution-field for few cases (1 group i.e., 10*1=10):\n",
    "                if (i+1) <= 10:\n",
    "                    data_i = outputs_test[i].reshape(-1, nx*ny) #(nt, nx*ny)\n",
    "                    reconstructed_i = (model_AE((data_i-mean)/std) * std) + mean #(nt, nx*ny)\n",
    "\n",
    "                    # print(colored('TEST SAMPLE '+str(i+1), 'red'))\n",
    "                    shape = get_key_from_value(shape_map, input_parameters_test[i, 0])\n",
    "                    # print(colored(f\"Shape = {shape}, r = {input_parameters_test[i,2]:.3f}, a_value = {input_parameters_test[i,3]:.3f}\", 'red'))\n",
    "\n",
    "                    # Plotting \n",
    "                    with suppress_shows():\n",
    "                        plot_source(i, x_span, y_span, inputs_test[i], f\"{shape.capitalize()} Source\", 'hot', resultdir_, save)\n",
    "\n",
    "                        cmap = 'hot'  # Color map\n",
    "                        fontsize = 14  # Font size for labels and titles\n",
    "                        levels = 100\n",
    "                        plot_solution(i, x_span, y_span, data_i.reshape(nt,ny,nx), t_span, f\"True Solution for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir_, save, 'True-Solution')\n",
    "                        plot_solution(i, x_span, y_span, reconstructed_i.reshape(nt,ny,nx), t_span, f\"Reconstructed Solution for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir_, save, 'Reconstructed-Solution')\n",
    "                        # print(colored('#'*230, 'green'))\n",
    "\n",
    "            print(colored('*'*115, 'green'))\n",
    "            \n",
    "        print(colored('#'*115, 'blue'))\n",
    "\n",
    "    \n",
    "    # Convert results into a pandas DataFrame\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb3e4665",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T04:58:01.404622Z",
     "iopub.status.busy": "2025-01-09T04:58:01.404474Z",
     "iopub.status.idle": "2025-01-09T06:04:52.276628Z",
     "shell.execute_reply": "2025-01-09T06:04:52.276190Z"
    },
    "papermill": {
     "duration": 4010.911608,
     "end_time": "2025-01-09T06:04:52.313262",
     "exception": false,
     "start_time": "2025-01-09T04:58:01.401654",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m*******************************************************************************************************************\u001b[0m\n",
      "seed = 0\n",
      "\u001b[31m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 50\n",
      "Shape of outputs_train_used: torch.Size([50, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([1000, 4096])\n",
      "seed = 0\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.00931692, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03248397, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00670414, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00325298, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00207232, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00070969, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00077953, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00049085, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00052594, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00020744, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00017018, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00011385, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00026759, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00007805, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00008007, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00005935, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00005070, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00004082, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00003616, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00003627, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00003624, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00003516, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00003170, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00003141, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00002843, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00002609, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00003098, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00002657, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00002528, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00002740, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00002228, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00001881, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00001812, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00001835, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00002310, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00001929, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00001742, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00001859, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00001652, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00001989, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.01390337944031\n",
      "\u001b[31mSeed 0, n_used 50: Reconstruction Test Error = 2.61e-06\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 100\n",
      "Shape of outputs_train_used: torch.Size([100, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([2000, 4096])\n",
      "seed = 0\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.99323207, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03935616, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00900911, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00366349, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00258517, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00147695, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00058340, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00037577, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00033175, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00037400, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00028063, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00020686, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00020561, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00025587, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00143531, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00014809, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00012454, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00010807, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00010217, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00011945, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00009888, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00009418, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00009028, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00009014, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00009147, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00007295, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00008249, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00006905, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00008325, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00007278, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00007465, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00007695, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00006571, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00006679, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00005924, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00005885, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00006046, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00006524, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00005730, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00005795, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "86.76560163497925\n",
      "\u001b[31mSeed 0, n_used 100: Reconstruction Test Error = 5.25e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 150\n",
      "Shape of outputs_train_used: torch.Size([150, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([3000, 4096])\n",
      "seed = 0\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.85922754, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.04073173, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00873145, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00353158, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00225298, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00156842, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00120818, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00053901, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00050208, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00036825, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00034982, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00033634, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00017024, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00019422, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00122196, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00016031, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00016532, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00012184, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00010402, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00009282, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00009990, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00009714, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00008887, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00009280, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00008687, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00008483, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00008178, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00007005, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00006855, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00006197, learning rate = 0.00001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15000 - loss = 0.00007044, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00006114, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00005598, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00005720, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00005028, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00005268, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00005045, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00005416, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00004574, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00005192, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.1970443725586\n",
      "\u001b[31mSeed 0, n_used 150: Reconstruction Test Error = 9.05e-08\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 200\n",
      "Shape of outputs_train_used: torch.Size([200, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([4000, 4096])\n",
      "seed = 0\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.95385242, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.04597775, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00882534, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00323185, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00259874, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00220584, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00136414, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00145001, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00050555, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00037676, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00030183, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00046351, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00033906, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00032666, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00017620, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00060105, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00017214, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00010417, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00010736, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00010997, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00011355, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00010113, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00009815, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00009671, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00010210, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00009385, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00008801, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00007972, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00008122, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00006604, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00007642, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00006938, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00006362, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00006090, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00006665, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00005970, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00006718, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00006837, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00006693, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00006411, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.25721096992493\n",
      "\u001b[31mSeed 0, n_used 200: Reconstruction Test Error = 8.83e-08\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 250\n",
      "Shape of outputs_train_used: torch.Size([250, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([5000, 4096])\n",
      "seed = 0\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.86846197, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.04791332, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00926262, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00386330, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00240163, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00153232, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00077991, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00071093, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00049680, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00075989, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00051303, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00041881, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00029641, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00026550, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00024631, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00013378, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00029938, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00012635, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00012017, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00012796, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00012954, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00010694, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00010443, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00010691, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00010402, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00009409, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00009730, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00008937, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00007730, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00007942, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00007572, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00007149, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00006662, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00007015, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00006649, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00006524, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00005720, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00006207, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00006295, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00006127, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.4573495388031\n",
      "\u001b[31mSeed 0, n_used 250: Reconstruction Test Error = 6.75e-08\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "\u001b[34m###################################################################################################################\u001b[0m\n",
      "\u001b[31m*******************************************************************************************************************\u001b[0m\n",
      "seed = 1\n",
      "\u001b[31m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 50\n",
      "Shape of outputs_train_used: torch.Size([50, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([1000, 4096])\n",
      "seed = 1\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.94096923, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03753708, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00624054, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00169063, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00139756, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00086396, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00071851, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00039833, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00044480, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00065707, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00026221, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00037912, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00023338, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00018371, learning rate = 0.00010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7000 - loss = 0.00012803, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00009682, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00013544, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00008340, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00007740, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00006861, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00008052, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00006955, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00006238, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00006363, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00005344, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00006429, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00004664, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00005620, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00004912, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00004858, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00004961, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00004792, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00004391, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00003688, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00004057, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00004234, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00003453, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00003472, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00004670, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00003675, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "86.38913702964783\n",
      "\u001b[31mSeed 1, n_used 50: Reconstruction Test Error = 1.78e-06\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 100\n",
      "Shape of outputs_train_used: torch.Size([100, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([2000, 4096])\n",
      "seed = 1\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.99660373, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.06647539, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00676858, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00342322, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00229669, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00193431, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00080162, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00051277, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00053951, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00036849, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00035952, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00035128, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00035658, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00020705, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00033093, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00019369, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00012767, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00011391, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00011868, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00010463, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00009110, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00008851, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00009333, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00008712, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00009658, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00008470, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00007941, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00008122, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00007724, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00007232, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00008219, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00007329, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00006630, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00006470, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00005613, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00006765, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00006732, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00007235, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00005536, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00005436, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "86.7782838344574\n",
      "\u001b[31mSeed 1, n_used 100: Reconstruction Test Error = 3.94e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 150\n",
      "Shape of outputs_train_used: torch.Size([150, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([3000, 4096])\n",
      "seed = 1\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.06831908, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.04278829, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.01098459, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00353904, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00248337, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00317213, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00152944, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00054740, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00052586, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00047492, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00054566, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00034917, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00062576, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00114612, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00026648, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00016190, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00021343, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00012095, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00012680, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00011986, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00012436, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00013292, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00011621, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00011116, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00010619, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00009706, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00010978, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00009515, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00008227, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00008299, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00008479, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00007572, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00006993, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00007671, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00007675, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00007264, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00008279, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00007448, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00006623, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00006351, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "86.99261832237244\n",
      "\u001b[31mSeed 1, n_used 150: Reconstruction Test Error = 1.28e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 200\n",
      "Shape of outputs_train_used: torch.Size([200, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([4000, 4096])\n",
      "seed = 1\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.06397796, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.04102962, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00994447, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00462612, learning rate = 0.00010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000 - loss = 0.00293941, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00148159, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00069270, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00174382, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00049466, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00055905, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00028431, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00038649, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00025511, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00026518, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00046041, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00086861, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00016682, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00013588, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00014218, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00010972, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00011349, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00011566, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00010807, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00011509, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00009762, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00009662, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00010979, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00009381, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00009713, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00009101, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00008561, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00008488, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00009820, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00008443, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00007690, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00008735, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00007630, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00007188, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00007981, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00008198, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.22672295570374\n",
      "\u001b[31mSeed 1, n_used 200: Reconstruction Test Error = 9.86e-08\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 250\n",
      "Shape of outputs_train_used: torch.Size([250, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([5000, 4096])\n",
      "seed = 1\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.06272900, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03496261, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.01124262, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00340829, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00142248, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00087670, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00053960, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00045287, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00048552, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00048800, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00040294, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00041795, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00281384, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00070146, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00024090, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00029804, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00022915, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00015451, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00013621, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00013392, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00012430, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00012872, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00012072, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00011468, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00011617, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00010411, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00009354, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00009968, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00008776, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00009473, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00009534, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00007870, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00008773, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00007773, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00008769, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00007590, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00008773, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00008633, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00006786, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00007746, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.47552800178528\n",
      "\u001b[31mSeed 1, n_used 250: Reconstruction Test Error = 7.33e-08\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "\u001b[34m###################################################################################################################\u001b[0m\n",
      "\u001b[31m*******************************************************************************************************************\u001b[0m\n",
      "seed = 2\n",
      "\u001b[31m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 50\n",
      "Shape of outputs_train_used: torch.Size([50, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([1000, 4096])\n",
      "seed = 2\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.98359346, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03421661, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.01150132, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00407399, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00169175, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00147654, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00134763, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00050478, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00051029, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00040230, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00025753, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00016866, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00013258, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00012976, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00030494, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00010190, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00019815, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00006053, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00006370, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00005939, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00005877, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00005271, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00005013, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00004701, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00004917, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00004396, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00004080, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00003545, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00004049, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00003658, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00003246, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00003748, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00003248, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00003027, learning rate = 0.00000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17000 - loss = 0.00002684, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00002873, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00002593, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00002886, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00002404, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00002797, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "86.50762844085693\n",
      "\u001b[31mSeed 2, n_used 50: Reconstruction Test Error = 7.19e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 100\n",
      "Shape of outputs_train_used: torch.Size([100, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([2000, 4096])\n",
      "seed = 2\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.10827601, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.04468040, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00983849, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00301686, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00240801, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00184834, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00137603, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00058479, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00046015, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00044353, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00038576, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00033695, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00016377, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00030470, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00020740, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00012295, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00010209, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00007926, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00009305, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00008100, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00007865, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00008017, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00007148, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00007193, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00007134, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00006190, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00006033, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00005602, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00005282, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00004500, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00004533, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00004631, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00004830, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00003904, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00003877, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00004204, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00004118, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00004411, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00003606, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00003999, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "86.74567890167236\n",
      "\u001b[31mSeed 2, n_used 100: Reconstruction Test Error = 1.49e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 150\n",
      "Shape of outputs_train_used: torch.Size([150, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([3000, 4096])\n",
      "seed = 2\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.11719751, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.04919924, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.01031676, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00322413, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00152709, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00077742, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00120549, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00051684, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00054689, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00051524, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00039458, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00036603, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00034011, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00114047, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00070308, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00032659, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00015757, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00011450, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00011293, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00011115, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00011851, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00012392, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00010616, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00010511, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00010574, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00010786, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00010470, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00011635, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00009318, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00008889, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00010144, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00008593, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00008549, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00007629, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00009002, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00009321, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00007568, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00008223, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00006118, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00007905, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.08215689659119\n",
      "\u001b[31mSeed 2, n_used 150: Reconstruction Test Error = 8.42e-08\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 200\n",
      "Shape of outputs_train_used: torch.Size([200, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([4000, 4096])\n",
      "seed = 2\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.98498011, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.05733298, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.01121838, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00437744, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00193072, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00101119, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00298420, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00054463, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00046688, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00041708, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00036755, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00037291, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00050411, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00042033, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00031823, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00031212, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00050933, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00026734, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00022895, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00024549, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00024868, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00018855, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00019959, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00019785, learning rate = 0.00001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12000 - loss = 0.00019134, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00016560, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00014387, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00011884, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00012627, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00012491, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00011810, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00010467, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00010076, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00009889, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00008771, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00009317, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00008566, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00008243, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00009663, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00009080, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.55721068382263\n",
      "\u001b[31mSeed 2, n_used 200: Reconstruction Test Error = 9.23e-08\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 250\n",
      "Shape of outputs_train_used: torch.Size([250, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([5000, 4096])\n",
      "seed = 2\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.94451112, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03931880, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.01079766, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00542387, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00203939, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00087699, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00049914, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00054856, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00083041, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00060115, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00039097, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00062498, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00043546, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00036019, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00030953, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00044409, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00219969, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00027444, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00032509, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00031375, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00027303, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00032935, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00023208, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00024943, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00027855, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00027720, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00024307, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00023472, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00020447, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00025113, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00021246, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00016854, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00014726, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00013944, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00016032, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00013989, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00014285, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00013127, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00014507, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00013017, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.60279178619385\n",
      "\u001b[31mSeed 2, n_used 250: Reconstruction Test Error = 1.16e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "\u001b[34m###################################################################################################################\u001b[0m\n",
      "\u001b[31m*******************************************************************************************************************\u001b[0m\n",
      "seed = 3\n",
      "\u001b[31m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 50\n",
      "Shape of outputs_train_used: torch.Size([50, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([1000, 4096])\n",
      "seed = 3\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.02785957, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.02767776, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00686591, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00380213, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00182507, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00189849, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00072745, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00040452, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00054132, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00028853, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00030207, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00011228, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00017137, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00007506, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00017355, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00005622, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00005382, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00004037, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00003760, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00003973, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00004695, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00003744, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00003635, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00003288, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00003306, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00003120, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00002806, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00003162, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00002713, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00002699, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00002404, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00002434, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00002148, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00002026, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00002125, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00001953, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00002022, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00002075, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00001934, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00001952, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "86.5430166721344\n",
      "\u001b[31mSeed 3, n_used 50: Reconstruction Test Error = 2.60e-06\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 100\n",
      "Shape of outputs_train_used: torch.Size([100, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([2000, 4096])\n",
      "seed = 3\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.02099752, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.05905136, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00641871, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00375297, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00209012, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00159257, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00065678, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00048554, learning rate = 0.00010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000 - loss = 0.00046903, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00032953, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00051674, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00050571, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00017470, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00019093, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00012107, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00012262, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00030171, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00010457, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00009326, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00008257, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00007498, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00006982, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00007733, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00006548, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00006134, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00005587, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00005480, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00005259, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00004330, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00004497, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00004426, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00004576, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00003812, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00003447, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00003269, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00003444, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00003491, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00003810, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00003523, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00003349, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.13096237182617\n",
      "\u001b[31mSeed 3, n_used 100: Reconstruction Test Error = 2.28e-06\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 150\n",
      "Shape of outputs_train_used: torch.Size([150, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([3000, 4096])\n",
      "seed = 3\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.93665755, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03997190, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00768928, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00304794, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00218520, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00119484, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00133997, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00047659, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00044041, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00053382, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00035631, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00039898, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00039581, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00015916, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00015517, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00023397, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00017848, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00009061, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00011456, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00009293, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00009319, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00010219, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00009649, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00010193, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00009282, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00007534, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00007752, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00007119, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00005806, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00006306, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00006046, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00005698, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00005546, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00005264, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00004581, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00005362, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00005553, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00005004, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00005187, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00004949, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.26194286346436\n",
      "\u001b[31mSeed 3, n_used 150: Reconstruction Test Error = 1.97e-06\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 200\n",
      "Shape of outputs_train_used: torch.Size([200, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([4000, 4096])\n",
      "seed = 3\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 0.92055118, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03857075, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00898855, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00406994, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00231937, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00137443, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00078682, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00059027, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00071024, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00046774, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00053040, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00039995, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00073884, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00035977, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00024845, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00021877, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00027168, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00013944, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00013065, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00013673, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00012424, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00012789, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00013045, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00012770, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00011962, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00010242, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00010344, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00010416, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00009081, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00009341, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00008977, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00008289, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00007308, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00007254, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00007047, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00007511, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00006315, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00007729, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00006761, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00006336, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.30474710464478\n",
      "\u001b[31mSeed 3, n_used 200: Reconstruction Test Error = 2.56e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 250\n",
      "Shape of outputs_train_used: torch.Size([250, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([5000, 4096])\n",
      "seed = 3\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.09526348, learning rate = 0.00010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500 - loss = 0.04281767, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00866982, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00447078, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00214086, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00121513, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00101046, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00055074, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00057031, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00040511, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00049436, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00076947, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00047934, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00035045, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00020040, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00031256, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00013151, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00012439, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00012356, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00010785, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00012175, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00010084, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00010724, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00011267, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00009381, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00008901, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00009754, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00008248, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00009216, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00008724, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00007296, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00008415, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00007303, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00008266, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00007660, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00007145, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00007718, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00006739, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00007433, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00006943, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.58235764503479\n",
      "\u001b[31mSeed 3, n_used 250: Reconstruction Test Error = 1.07e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "\u001b[34m###################################################################################################################\u001b[0m\n",
      "\u001b[31m*******************************************************************************************************************\u001b[0m\n",
      "seed = 4\n",
      "\u001b[31m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 50\n",
      "Shape of outputs_train_used: torch.Size([50, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([1000, 4096])\n",
      "seed = 4\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.00869775, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03377015, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00458446, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00189396, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00117633, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00068292, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00062547, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00034184, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00025126, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00031775, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00020583, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00019509, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00021112, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00009909, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00012756, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00007158, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00022831, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00005568, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00005509, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00004671, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00004889, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00004218, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00005511, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00004718, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00005028, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00004464, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00003721, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00003900, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00003924, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00003902, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00003341, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00003539, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00003417, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00002474, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00003371, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00003353, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00002615, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00003490, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00002745, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00003366, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "86.48924517631531\n",
      "\u001b[31mSeed 4, n_used 50: Reconstruction Test Error = 3.81e-06\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 100\n",
      "Shape of outputs_train_used: torch.Size([100, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([2000, 4096])\n",
      "seed = 4\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.01723039, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03486144, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.01229519, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00305391, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00146918, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00076500, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00056113, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00098939, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00059052, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00040285, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00030627, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00042477, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00045160, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00026222, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00014594, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00013400, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00020766, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00009415, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00009645, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00010459, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00010475, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00010236, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00007965, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00008458, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00007525, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00006797, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00008503, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00007913, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00007653, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00006759, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00006541, learning rate = 0.00001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15500 - loss = 0.00005667, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00007300, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00006048, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00005744, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00006501, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00006767, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00005146, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00005724, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00006757, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.1818163394928\n",
      "\u001b[31mSeed 4, n_used 100: Reconstruction Test Error = 1.22e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 150\n",
      "Shape of outputs_train_used: torch.Size([150, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([3000, 4096])\n",
      "seed = 4\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.10144949, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.05251411, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.01046233, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00378127, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00158200, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00186067, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00068679, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00066570, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00045686, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00045008, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00038941, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00184495, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00032552, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00029051, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00031433, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00020372, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00013154, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00009785, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00009579, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00009651, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00008895, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00009844, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00009756, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00007644, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00007527, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00008268, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00007131, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00006827, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00006196, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00006410, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00005955, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00006343, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00006455, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00006651, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00006645, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00006030, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00005890, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00005399, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00005582, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00005691, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.01293802261353\n",
      "\u001b[31mSeed 4, n_used 150: Reconstruction Test Error = 1.23e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 200\n",
      "Shape of outputs_train_used: torch.Size([200, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([4000, 4096])\n",
      "seed = 4\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.01668632, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.04583140, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.01102785, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00430479, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00186271, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00141599, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00059500, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00060157, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00044216, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00048857, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00034081, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00037200, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00062847, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00038100, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00016990, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00025667, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00031004, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00012017, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00012407, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00010813, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00011829, learning rate = 0.00001000\n",
      "Iteration 10500 - loss = 0.00011761, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00011919, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00010642, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00010418, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00009104, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00008577, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00008841, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00008845, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00007696, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00007981, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00006682, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00006505, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00005201, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00006104, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00005277, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00005536, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00005152, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00005517, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00005185, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.24965620040894\n",
      "\u001b[31mSeed 4, n_used 200: Reconstruction Test Error = 1.04e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "n_used = 250\n",
      "Shape of outputs_train_used: torch.Size([250, 20, 64, 64])\n",
      "Shape of data_used_for_AE: torch.Size([5000, 4096])\n",
      "seed = 4\n",
      "------STARTED-TRAINING------\n",
      "Iteration 0 - loss = 1.03923607, learning rate = 0.00010000\n",
      "Iteration 500 - loss = 0.03953494, learning rate = 0.00010000\n",
      "Iteration 1000 - loss = 0.00929962, learning rate = 0.00010000\n",
      "Iteration 1500 - loss = 0.00379488, learning rate = 0.00010000\n",
      "Iteration 2000 - loss = 0.00155345, learning rate = 0.00010000\n",
      "Iteration 2500 - loss = 0.00077808, learning rate = 0.00010000\n",
      "Iteration 3000 - loss = 0.00062796, learning rate = 0.00010000\n",
      "Iteration 3500 - loss = 0.00047009, learning rate = 0.00010000\n",
      "Iteration 4000 - loss = 0.00045724, learning rate = 0.00010000\n",
      "Iteration 4500 - loss = 0.00311684, learning rate = 0.00010000\n",
      "Iteration 5000 - loss = 0.00103145, learning rate = 0.00010000\n",
      "Iteration 5500 - loss = 0.00070989, learning rate = 0.00010000\n",
      "Iteration 6000 - loss = 0.00037519, learning rate = 0.00010000\n",
      "Iteration 6500 - loss = 0.00031411, learning rate = 0.00010000\n",
      "Iteration 7000 - loss = 0.00018646, learning rate = 0.00010000\n",
      "Iteration 7500 - loss = 0.00064355, learning rate = 0.00010000\n",
      "Iteration 8000 - loss = 0.00022119, learning rate = 0.00001000\n",
      "Iteration 8500 - loss = 0.00014263, learning rate = 0.00001000\n",
      "Iteration 9000 - loss = 0.00011771, learning rate = 0.00001000\n",
      "Iteration 9500 - loss = 0.00010899, learning rate = 0.00001000\n",
      "Iteration 10000 - loss = 0.00010945, learning rate = 0.00001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10500 - loss = 0.00010910, learning rate = 0.00001000\n",
      "Iteration 11000 - loss = 0.00011414, learning rate = 0.00001000\n",
      "Iteration 11500 - loss = 0.00011339, learning rate = 0.00001000\n",
      "Iteration 12000 - loss = 0.00009808, learning rate = 0.00001000\n",
      "Iteration 12500 - loss = 0.00009970, learning rate = 0.00001000\n",
      "Iteration 13000 - loss = 0.00009536, learning rate = 0.00001000\n",
      "Iteration 13500 - loss = 0.00009473, learning rate = 0.00001000\n",
      "Iteration 14000 - loss = 0.00010666, learning rate = 0.00001000\n",
      "Iteration 14500 - loss = 0.00008948, learning rate = 0.00001000\n",
      "Iteration 15000 - loss = 0.00010280, learning rate = 0.00001000\n",
      "Iteration 15500 - loss = 0.00009365, learning rate = 0.00001000\n",
      "Iteration 16000 - loss = 0.00009431, learning rate = 0.00000100\n",
      "Iteration 16500 - loss = 0.00008921, learning rate = 0.00000100\n",
      "Iteration 17000 - loss = 0.00007490, learning rate = 0.00000100\n",
      "Iteration 17500 - loss = 0.00009365, learning rate = 0.00000100\n",
      "Iteration 18000 - loss = 0.00007980, learning rate = 0.00000100\n",
      "Iteration 18500 - loss = 0.00009979, learning rate = 0.00000100\n",
      "Iteration 19000 - loss = 0.00007801, learning rate = 0.00000100\n",
      "Iteration 19500 - loss = 0.00007139, learning rate = 0.00000100\n",
      "------ENDED-TRAINING------\n",
      "Time (sec) to complete:\n",
      "87.50244116783142\n",
      "\u001b[31mSeed 4, n_used 250: Reconstruction Test Error = 1.02e-07\u001b[0m\n",
      "\u001b[32m*******************************************************************************************************************\u001b[0m\n",
      "\u001b[34m###################################################################################################################\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 16 # d_z  \n",
    "df_results = main(stove_full_solution_fields_groups, n_used_list=[50, 100, 150, 200, 250], latent_dim=latent_dim, n_seeds=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b9454a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T06:04:52.390405Z",
     "iopub.status.busy": "2025-01-09T06:04:52.390044Z",
     "iopub.status.idle": "2025-01-09T06:04:52.829810Z",
     "shell.execute_reply": "2025-01-09T06:04:52.829431Z"
    },
    "papermill": {
     "duration": 0.479239,
     "end_time": "2025-01-09T06:04:52.830748",
     "exception": false,
     "start_time": "2025-01-09T06:04:52.351509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>n_used</th>\n",
       "      <th>reconstruction_test_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2.613841e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>5.247503e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>9.053730e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>8.832792e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>6.753652e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.782690e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>3.944882e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>1.284841e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>9.862215e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>7.332444e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>7.186783e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1.488275e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>8.418476e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>9.225397e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>1.162070e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>2.596205e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>2.283083e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>1.965679e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>2.562679e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>1.074506e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>3.805835e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>1.219446e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>1.234755e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>1.037375e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>250</td>\n",
       "      <td>1.016023e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seed  n_used  reconstruction_test_error\n",
       "0      0      50               2.613841e-06\n",
       "1      0     100               5.247503e-07\n",
       "2      0     150               9.053730e-08\n",
       "3      0     200               8.832792e-08\n",
       "4      0     250               6.753652e-08\n",
       "5      1      50               1.782690e-06\n",
       "6      1     100               3.944882e-07\n",
       "7      1     150               1.284841e-07\n",
       "8      1     200               9.862215e-08\n",
       "9      1     250               7.332444e-08\n",
       "10     2      50               7.186783e-07\n",
       "11     2     100               1.488275e-07\n",
       "12     2     150               8.418476e-08\n",
       "13     2     200               9.225397e-08\n",
       "14     2     250               1.162070e-07\n",
       "15     3      50               2.596205e-06\n",
       "16     3     100               2.283083e-06\n",
       "17     3     150               1.965679e-06\n",
       "18     3     200               2.562679e-07\n",
       "19     3     250               1.074506e-07\n",
       "20     4      50               3.805835e-06\n",
       "21     4     100               1.219446e-07\n",
       "22     4     150               1.234755e-07\n",
       "23     4     200               1.037375e-07\n",
       "24     4     250               1.016023e-07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_results)\n",
    "df_results.to_csv(os.path.join(resultdir, \"reconstruction_test_errors.csv\"), index=False)\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "# Box plot of reconstruction_test_error vs n_used\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='n_used', y='reconstruction_test_error', data=df_results)\n",
    "plt.xlabel('Number of Training Samples') # (n_used)\n",
    "plt.ylabel('Reconstruction Test Error')\n",
    "plt.savefig(os.path.join(resultdir,'reconstruction_test_error-vs-n_used.pdf'))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045170bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9383dd38",
   "metadata": {
    "papermill": {
     "duration": 0.03497,
     "end_time": "2025-01-09T06:04:52.901543",
     "exception": false,
     "start_time": "2025-01-09T06:04:52.866573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Define result directory (update this with the correct path)\n",
    "resultdir = os.path.join(os.getcwd(),'results','c_AE') \n",
    "\n",
    "# Load the data from the CSV file\n",
    "df_results = pd.read_csv(os.path.join(resultdir, \"reconstruction_test_errors.csv\"))\n",
    "\n",
    "# Compute the logarithm of reconstruction test error\n",
    "df_results['log_reconstruction_test_error'] = np.log(df_results['reconstruction_test_error'])\n",
    "\n",
    "# Plot log reconstruction test error vs ntrain (n_used)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='n_used', y='log_reconstruction_test_error', data=df_results)\n",
    "plt.xlabel('Number of Training Samples')  # (n_used)\n",
    "plt.ylabel('Log of Reconstruction Test Error')\n",
    "plt.title('Log Reconstruction Test Error vs Number of Training Samples')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(os.path.join(resultdir, 'log_reconstruction_test_error-vs-n_used.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd503e-1350-4877-81fa-84e5c8b54f37",
   "metadata": {
    "papermill": {
     "duration": 0.034732,
     "end_time": "2025-01-09T06:04:52.970967",
     "exception": false,
     "start_time": "2025-01-09T06:04:52.936235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4021.027068,
   "end_time": "2025-01-09T06:04:55.614015",
   "environment_variables": {},
   "exception": null,
   "input_path": "AE.ipynb",
   "output_path": "AE_output.ipynb",
   "parameters": {
    "n_used": 200,
    "save": true,
    "seed": 0
   },
   "start_time": "2025-01-09T04:57:54.586947",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
