{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f610c5",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "This code can execute the following variant of loss:  \n",
    "\n",
    "\n",
    "   **Variant 4:**  Physics + Data with Latent space constrained \n",
    "   \n",
    "   $L_{\\theta} = L_{\\text{PDE}} + \\| u - \\hat{u} \\|_2^2 + \\| z - \\hat{z} \\|_2^2$  \n",
    "   Use $n_{\\text{used}}  = 150$ (as Auto-encoder reconstruction test error stabilises at this value) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a75ad",
   "metadata": {
    "papermill": {
     "duration": 0.003366,
     "end_time": "2024-12-30T23:48:25.656163",
     "exception": false,
     "start_time": "2024-12-30T23:48:25.652797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1395ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:25.662580Z",
     "iopub.status.busy": "2024-12-30T23:48:25.662220Z",
     "iopub.status.idle": "2024-12-30T23:48:28.410415Z",
     "shell.execute_reply": "2024-12-30T23:48:28.409886Z"
    },
    "papermill": {
     "duration": 2.752627,
     "end_time": "2024-12-30T23:48:28.411685",
     "exception": false,
     "start_time": "2024-12-30T23:48:25.659058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from termcolor import colored\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from utils.networks import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "from utils.deeponet_networks_2d import *\n",
    "from utils.misc_stove import *\n",
    "from utils.visualizer_stove import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921aabe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:28.418508Z",
     "iopub.status.busy": "2024-12-30T23:48:28.418173Z",
     "iopub.status.idle": "2024-12-30T23:48:28.420850Z",
     "shell.execute_reply": "2024-12-30T23:48:28.420440Z"
    },
    "papermill": {
     "duration": 0.006885,
     "end_time": "2024-12-30T23:48:28.421668",
     "exception": false,
     "start_time": "2024-12-30T23:48:28.414783",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Tag this cell with 'parameters'\n",
    "# parameters\n",
    "seed = 0 # Seed number.\n",
    "n_used = 150 # Ensure n_used is a multiple of 10 (as group size is 10) # Number of full training fields used for estimating the data-driven loss term\n",
    "n_iterations = 50000 # Number of iterations.\n",
    "save = True # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:28.436792Z",
     "iopub.status.busy": "2024-12-30T23:48:28.436447Z",
     "iopub.status.idle": "2024-12-30T23:48:28.439190Z",
     "shell.execute_reply": "2024-12-30T23:48:28.438801Z"
    },
    "papermill": {
     "duration": 0.006717,
     "end_time": "2024-12-30T23:48:28.439969",
     "exception": false,
     "start_time": "2024-12-30T23:48:28.433252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','c_Latent-NO_with-AE','seed='+str(seed)+'_n_used='+str(n_used)) \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7126e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:28.446141Z",
     "iopub.status.busy": "2024-12-30T23:48:28.445812Z",
     "iopub.status.idle": "2024-12-30T23:48:28.448915Z",
     "shell.execute_reply": "2024-12-30T23:48:28.448488Z"
    },
    "papermill": {
     "duration": 0.007059,
     "end_time": "2024-12-30T23:48:28.449742",
     "exception": false,
     "start_time": "2024-12-30T23:48:28.442683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d11f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:28.456147Z",
     "iopub.status.busy": "2024-12-30T23:48:28.455798Z",
     "iopub.status.idle": "2024-12-30T23:48:29.350165Z",
     "shell.execute_reply": "2024-12-30T23:48:29.349705Z"
    },
    "papermill": {
     "duration": 0.898518,
     "end_time": "2024-12-30T23:48:29.351028",
     "exception": false,
     "start_time": "2024-12-30T23:48:28.452510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346e7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:29.357819Z",
     "iopub.status.busy": "2024-12-30T23:48:29.357414Z",
     "iopub.status.idle": "2024-12-30T23:48:30.020065Z",
     "shell.execute_reply": "2024-12-30T23:48:30.019688Z"
    },
    "papermill": {
     "duration": 0.666753,
     "end_time": "2024-12-30T23:48:30.020839",
     "exception": false,
     "start_time": "2024-12-30T23:48:29.354086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata_np = np.load(os.path.join('..','..','data/Stove/metadata.npz'), allow_pickle=True)\n",
    "metadata = convert_metadata_to_torch(metadata_np, device)\n",
    "for key, value in metadata.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"Shape of {key}: {value.shape}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"{key} is a dictionary with {len(value)} keys.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02558fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:30.027790Z",
     "iopub.status.busy": "2024-12-30T23:48:30.027461Z",
     "iopub.status.idle": "2024-12-30T23:48:30.145734Z",
     "shell.execute_reply": "2024-12-30T23:48:30.145357Z"
    },
    "papermill": {
     "duration": 0.122552,
     "end_time": "2024-12-30T23:48:30.146562",
     "exception": false,
     "start_time": "2024-12-30T23:48:30.024010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_span, x_span, y_span = metadata['t_span'], metadata['x_span'], metadata['y_span']\n",
    "\n",
    "nt, nx, ny = len(t_span), len(x_span), len(y_span) # number of discretizations in time, location_x and location_y.\n",
    "print(\"nt =\",nt, \", nx =\",nx, \"ny =\",ny)\n",
    "print(\"Shape of t-span, x-span, and y-span:\",t_span.shape, x_span.shape, y_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "print(\"y-span:\", y_span)\n",
    "\n",
    "L = 2.         # Simulation domain [-L, L]^2\n",
    "T = 1.         # Simulation time\n",
    "D_value = 1.0  # Diffusion coefficient  \n",
    "\n",
    "grid = torch.vstack((t_span.repeat_interleave(ny*nx), \n",
    "              x_span.flatten().repeat(nt),\n",
    "              y_span.flatten().repeat(nt))).T\n",
    "print(\"Shape of grid:\", grid.shape) # (nt*nx*ny, 3)\n",
    "print(\"grid:\", grid) # time, location_x, location_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129b4ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:30.153835Z",
     "iopub.status.busy": "2024-12-30T23:48:30.153428Z",
     "iopub.status.idle": "2024-12-30T23:48:30.179205Z",
     "shell.execute_reply": "2024-12-30T23:48:30.178848Z"
    },
    "papermill": {
     "duration": 0.030133,
     "end_time": "2024-12-30T23:48:30.180000",
     "exception": false,
     "start_time": "2024-12-30T23:48:30.149867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stove_full_solution_fields_groups_np = np.load(os.path.join('..','..','data/Stove/stove_full_solution_fields.npz'), allow_pickle=True)\n",
    "print(stove_full_solution_fields_groups_np.keys())\n",
    "\n",
    "stove_source_fields_only_groups_np = np.load(os.path.join('..','..','data/Stove/stove_source_fields_only.npz'), allow_pickle=True)\n",
    "print(stove_source_fields_only_groups_np.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b358345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:30.186999Z",
     "iopub.status.busy": "2024-12-30T23:48:30.186596Z",
     "iopub.status.idle": "2024-12-30T23:48:32.659628Z",
     "shell.execute_reply": "2024-12-30T23:48:32.659199Z"
    },
    "papermill": {
     "duration": 2.477485,
     "end_time": "2024-12-30T23:48:32.660584",
     "exception": false,
     "start_time": "2024-12-30T23:48:30.183099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the NumPy groups to Pytorch\n",
    "stove_full_solution_fields_groups = convert_groupdict_to_torch(stove_full_solution_fields_groups_np, device)\n",
    "stove_source_fields_only_groups = convert_groupdict_to_torch(stove_source_fields_only_groups_np, device)\n",
    "\n",
    "# Check the shapes\n",
    "print('stove_full_solution_fields_groups:')\n",
    "check_shapes(stove_full_solution_fields_groups)\n",
    "print(colored('#' * 230, 'green'))\n",
    "\n",
    "print('stove_source_fields_only_groups:')\n",
    "check_shapes(stove_source_fields_only_groups)\n",
    "print(colored('#' * 230, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2018762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:32.668917Z",
     "iopub.status.busy": "2024-12-30T23:48:32.668664Z",
     "iopub.status.idle": "2024-12-30T23:48:32.675771Z",
     "shell.execute_reply": "2024-12-30T23:48:32.675402Z"
    },
    "papermill": {
     "duration": 0.012503,
     "end_time": "2024-12-30T23:48:32.676655",
     "exception": false,
     "start_time": "2024-12-30T23:48:32.664152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing groups\n",
    "train_group, test_group = split_groups(stove_full_solution_fields_groups, seed, train_size=50, test_size=10)\n",
    "\n",
    "# Print the shapes for verification\n",
    "print(\"Training Group:\")\n",
    "print(colored('#' * 20, 'red'))\n",
    "print_group_shapes(train_group)\n",
    "print(colored('#' * 230, 'green'))\n",
    "print(\"Testing Group:\")\n",
    "print(colored('#' * 20, 'red'))\n",
    "print_group_shapes(test_group)\n",
    "print(colored('#' * 230, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7d29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:32.683672Z",
     "iopub.status.busy": "2024-12-30T23:48:32.683522Z",
     "iopub.status.idle": "2024-12-30T23:48:32.696944Z",
     "shell.execute_reply": "2024-12-30T23:48:32.696620Z"
    },
    "papermill": {
     "duration": 0.018071,
     "end_time": "2024-12-30T23:48:32.697846",
     "exception": false,
     "start_time": "2024-12-30T23:48:32.679775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = load_and_combine_groups(train_group, 'Train', combine=True, device=device)\n",
    "input_parameters_train = train_data['input_parameters']\n",
    "inputs_train = train_data['input_samples']\n",
    "outputs_train = train_data['output_samples']\n",
    "\n",
    "test_data = load_and_combine_groups(test_group, 'Test', combine=True, device=device)\n",
    "input_parameters_test = test_data['input_parameters']\n",
    "inputs_test = test_data['input_samples']\n",
    "outputs_test = test_data['output_samples']\n",
    "\n",
    "print(colored('#' * 20, 'red'))\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of input_parameters_train:\", input_parameters_train.shape)\n",
    "print(\"Shape of input_parameters_test:\", input_parameters_test.shape)\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print(colored('#' * 20, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1bc32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:32.705069Z",
     "iopub.status.busy": "2024-12-30T23:48:32.704926Z",
     "iopub.status.idle": "2024-12-30T23:48:32.716754Z",
     "shell.execute_reply": "2024-12-30T23:48:32.716428Z"
    },
    "papermill": {
     "duration": 0.016538,
     "end_time": "2024-12-30T23:48:32.717696",
     "exception": false,
     "start_time": "2024-12-30T23:48:32.701158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stove_source_fields_only = load_and_combine_groups(stove_source_fields_only_groups, 'Source only', combine=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745cb5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:32.724898Z",
     "iopub.status.busy": "2024-12-30T23:48:32.724751Z",
     "iopub.status.idle": "2024-12-30T23:48:32.728173Z",
     "shell.execute_reply": "2024-12-30T23:48:32.727816Z"
    },
    "papermill": {
     "duration": 0.008089,
     "end_time": "2024-12-30T23:48:32.729077",
     "exception": false,
     "start_time": "2024-12-30T23:48:32.720988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term\n",
    "input_parameters_train_used = input_parameters_train[:n_used, :]\n",
    "print(\"Shape of input_parameters_train_used:\", input_parameters_train_used.shape)\n",
    "inputs_train_used = inputs_train[:n_used, :, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544dc72c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:48:32.736503Z",
     "iopub.status.busy": "2024-12-30T23:48:32.736361Z",
     "iopub.status.idle": "2024-12-30T23:53:39.158977Z",
     "shell.execute_reply": "2024-12-30T23:53:39.158564Z"
    },
    "papermill": {
     "duration": 306.431235,
     "end_time": "2024-12-30T23:53:39.163666",
     "exception": false,
     "start_time": "2024-12-30T23:48:32.732431",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_dim = 16 # d_z  \n",
    "\n",
    "if n_used > 0:\n",
    "    # Learning latent fields using Autoencoder\n",
    "\n",
    "    class Autoencoder_MLP(nn.Module):\n",
    "\n",
    "        def __init__(self, encoder_net, decoder_net):\n",
    "            super().__init__()\n",
    "\n",
    "            self.encoder_net = encoder_net\n",
    "            self.decoder_net = decoder_net\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            encoded = self.encoder_net(x)\n",
    "            decoded = self.decoder_net(encoded)\n",
    "\n",
    "            return decoded\n",
    "\n",
    "    input_dim = nx*ny\n",
    "\n",
    "    encoder_net = DenseNet(layersizes=[input_dim] + [2048, 1024, 512, 256] + [latent_dim], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "    encoder_net.to(device)\n",
    "    # print(encoder_net)\n",
    "    # print('ENCODER-NET SUMMARY:')\n",
    "    # summary(encoder_net, input_size=(input_dim,))  \n",
    "    # print('#'*100)\n",
    "\n",
    "    decoder_net = DenseNet(layersizes=[latent_dim] + [256, 512, 1024, 2048] + [input_dim], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "    decoder_net.to(device)\n",
    "    # print(decoder_net)\n",
    "    # print('DECODER-NET SUMMARY:')\n",
    "    # summary(decoder_net, input_size=(latent_dim,))\n",
    "    # print('#'*100)\n",
    "\n",
    "    model_AE = Autoencoder_MLP(encoder_net, decoder_net)\n",
    "    model_AE.to(device);\n",
    "\n",
    "    if save == True:\n",
    "        resultdir_ = os.path.join(resultdir, 'pretrained-AE_latent_dim='+str(latent_dim)) \n",
    "        if not os.path.exists(resultdir_):\n",
    "            os.makedirs(resultdir_)\n",
    "    else:\n",
    "        resultdir_ = None\n",
    "\n",
    "    data_used_for_AE = outputs_train_used.reshape(-1, nx*ny)\n",
    "    print(\"Shape of data_used_for_AE:\", data_used_for_AE.shape)\n",
    "\n",
    "    n_iterations_AE = 20000\n",
    "\n",
    "    # Compute mean and std across the entire dataset\n",
    "    mean = data_used_for_AE.mean(axis=0)\n",
    "    std = data_used_for_AE.std(axis=0)\n",
    "    std[std == 0] = 1e-16 # Avoid division by zero\n",
    "    if save == True:\n",
    "        np.savez(os.path.join(resultdir_,'normalization_params.npz'), mean=mean.cpu().numpy(), std=std.cpu().numpy())\n",
    "    # Scale the training data\n",
    "    data_train = (data_used_for_AE - mean) / std\n",
    "\n",
    "    print(colored('LATENT DIMENSION = '+str(latent_dim), 'red'))\n",
    "    print (\"------STARTED-TRAINING------\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    bs = 256 # Batch size\n",
    "\n",
    "    # Training\n",
    "    optimizer = torch.optim.Adam(model_AE.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "    iteration_list, loss_list, learningrates_list = [], [], []\n",
    "\n",
    "    for iteration in range(n_iterations_AE):\n",
    "        \n",
    "        num_samples = len(data_train)\n",
    "        indices = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "        data_batch = data_train[indices[0:bs]]\n",
    "        #print(f\"Shape of data_train_batch[{i}]:\", data_batch.shape) # (bs, nx*ny)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_AE(data_batch)\n",
    "        loss = nn.MSELoss()(outputs, data_batch)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_value_(model_AE.parameters(), clip_value=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if iteration % 500 == 0:\n",
    "            print('Iteration %s -' % iteration, 'loss = %.8f,' % loss,\n",
    "                  'learning rate = %.8f' % optimizer.state_dict()['param_groups'][0]['lr']) \n",
    "\n",
    "        iteration_list.append(iteration)\n",
    "        loss_list.append(loss.item())\n",
    "        learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    print (\"------ENDED-TRAINING------\")\n",
    "\n",
    "    if save == True:\n",
    "        np.save(os.path.join(resultdir_,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "        np.save(os.path.join(resultdir_,'loss_list.npy'), np.asarray(loss_list))\n",
    "        np.save(os.path.join(resultdir_,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "\n",
    "    plot_training_loss(resultdir_, iteration_list, loss_list, save)\n",
    "\n",
    "    plot_learningrates(resultdir_, iteration_list, learningrates_list, save)\n",
    "\n",
    "    # end timer\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time # time for AE network to train\n",
    "    print(\"Time (sec) to complete:\\n\" +str(training_time)) # time for AE network to train\n",
    "    print('*'*10)\n",
    "\n",
    "    if save == True:\n",
    "        torch.save(model_AE.state_dict(), os.path.join(resultdir_,'model_state_dict.pt'))\n",
    "\n",
    "    # Evaluate on test data\n",
    "    data_test = outputs_test.reshape(-1, nx*ny) #(test_size*nt, nx*ny)\n",
    "    outputs = model_AE((data_test- mean)/std) #(test_size*nt, nx*ny)\n",
    "    outputs_rescaled = (outputs * std) + mean # Rescale to Original Range\n",
    "    reconstruction_loss_test = nn.MSELoss()(outputs_rescaled, data_test)\n",
    "    print(f\"TEST DATA RECONSTRUCTION ERROR FOR LATENT DIMENSION {latent_dim}: {reconstruction_loss_test.item():.2e}\")\n",
    "    print('*'*10)\n",
    "\n",
    "    for i in range(outputs_test.shape[0]):\n",
    "\n",
    "        # Plot the full solution-field for few cases (2 groups i.e., 10*2=20):\n",
    "        if (i+1) <= 20:\n",
    "            data_i = outputs_test[i].reshape(-1, nx*ny) #(nt, nx*ny)\n",
    "            reconstructed_i = (model_AE((data_i-mean)/std) * std) + mean #(nt, nx*ny)\n",
    "\n",
    "            print(colored('TEST SAMPLE '+str(i+1), 'red'))\n",
    "            shape = get_key_from_value(shape_map, input_parameters_test[i, 0])\n",
    "            print(colored(f\"Shape = {shape}, r = {input_parameters_test[i,2]:.3f}, a_value = {input_parameters_test[i,3]:.3f}\", 'red'))\n",
    "\n",
    "            # Plotting \n",
    "            plot_source(i, x_span, y_span, inputs_test[i], f\"{shape.capitalize()} Source\", 'hot', resultdir_, save)\n",
    "\n",
    "            cmap = 'hot'  # Color map\n",
    "            fontsize = 14  # Font size for labels and titles\n",
    "            levels = 100\n",
    "            plot_solution(i, x_span, y_span, data_i.reshape(nt,ny,nx), t_span, f\"True Solution for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir_, save, 'True-Solution')\n",
    "            plot_solution(i, x_span, y_span, reconstructed_i.reshape(nt,ny,nx), t_span, f\"Reconstructed Solution for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir_, save, 'Reconstructed-Solution')\n",
    "            print(colored('#'*230, 'green'))\n",
    "\n",
    "    print(colored('*'*115, 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52da89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:53:39.584752Z",
     "iopub.status.busy": "2024-12-30T23:53:39.584373Z",
     "iopub.status.idle": "2024-12-30T23:53:39.938073Z",
     "shell.execute_reply": "2024-12-30T23:53:39.937685Z"
    },
    "papermill": {
     "duration": 0.564676,
     "end_time": "2024-12-30T23:53:39.938891",
     "exception": false,
     "start_time": "2024-12-30T23:53:39.374215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if n_used > 0:\n",
    "    # Load the pretrained autoencoder\n",
    "    pretrained_autoencoder_model = Autoencoder_MLP(encoder_net, decoder_net)\n",
    "    pretrained_autoencoder_model.to(device) \n",
    "    if save == True:\n",
    "        pretrained_autoencoder_model.load_state_dict(torch.load(os.path.join(resultdir,\n",
    "                                                                         'pretrained-AE_latent_dim='+str(latent_dim),\n",
    "                                                                         'model_state_dict.pt'), map_location=device))\n",
    "    # params = np.load(os.path.join(resultdir,'pretrained-AE_latent_dim='+str(latent_dim),'normalization_params.npz'))\n",
    "    # mean, std = torch.tensor(params['mean']).to(device), torch.tensor(params['std']).to(device) \n",
    "    if save == False:\n",
    "        pretrained_autoencoder_model.load_state_dict(model_AE.state_dict())\n",
    "\n",
    "    # Evaluating latent data and detach autoencoder (detaching is important, otherwise graph will be retained and takes lot of time to train)\n",
    "    latent_outputs_train = torch.zeros((outputs_train.shape[0], outputs_train.shape[1], latent_dim)).to(device)\n",
    "    latent_outputs_train_used = torch.zeros((outputs_train_used.shape[0], outputs_train_used.shape[1], latent_dim)).to(device)\n",
    "    latent_outputs_test = torch.zeros((outputs_test.shape[0], outputs_test.shape[1], latent_dim)).to(device)\n",
    "    for i in range(latent_outputs_train.shape[0]):\n",
    "        latent_outputs_train[i] = pretrained_autoencoder_model.encoder_net((outputs_train[i].reshape(-1, nx*ny)- mean)/std).detach()\n",
    "    for i in range(latent_outputs_train_used.shape[0]):\n",
    "        latent_outputs_train_used[i] = pretrained_autoencoder_model.encoder_net((outputs_train_used[i].reshape(-1, nx*ny)- mean)/std).detach()\n",
    "    for i in range(latent_outputs_test.shape[0]):\n",
    "        latent_outputs_test[i] = pretrained_autoencoder_model.encoder_net((outputs_test[i].reshape(-1, nx*ny)- mean)/std).detach()\n",
    "\n",
    "    # Check the shapes of the subsets\n",
    "    print(\"Shape of latent_outputs_train:\", latent_outputs_train.shape)\n",
    "    print(\"Shape of latent_outputs_train_used:\", latent_outputs_train_used.shape)\n",
    "    print(\"Shape of latent_outputs_test:\", latent_outputs_test.shape)\n",
    "    print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5719c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:53:40.369313Z",
     "iopub.status.busy": "2024-12-30T23:53:40.369053Z",
     "iopub.status.idle": "2024-12-30T23:53:40.668588Z",
     "shell.execute_reply": "2024-12-30T23:53:40.668212Z"
    },
    "papermill": {
     "duration": 0.515942,
     "end_time": "2024-12-30T23:53:40.669416",
     "exception": false,
     "start_time": "2024-12-30T23:53:40.153474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_neurons_latent_branch: Number of input neurons in the latent_branch net.\n",
    "input_neurons_latent_trunk: Number of input neurons in the latent_trunk net.\n",
    "latent_p: Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\"\"\"\n",
    "latent_p = latent_dim*16 # Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\n",
    "input_neurons_latent_branch = (ny, nx) # Specify input size of image as a tuple (height, width)\n",
    "n_channels = 1\n",
    "num_filters = [40, 60, 80, 100]\n",
    "filter_sizes = [3, 3, 3, 3]\n",
    "strides = [1]*len(num_filters)\n",
    "paddings = [0]*len(num_filters)\n",
    "poolings = [('avg', 2, 2), ('avg', 2, 2), ('avg', 2, 2), ('avg', 2, 2)]  # Pooling layer specification (type, kernel_size, stride)\n",
    "end_MLP_layersizes = [150, 150, latent_p]\n",
    "activation = nn.ReLU() # nn.SiLU() #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_branch_net = ConvNet(input_neurons_latent_branch, n_channels, num_filters, filter_sizes, strides, paddings, poolings, end_MLP_layersizes, activation)\n",
    "latent_branch_net.to(device)\n",
    "# print(latent_branch_net)\n",
    "print('LATENT BRANCH-NET SUMMARY:')\n",
    "summary(latent_branch_net, input_size=(n_channels, ny, nx))  # input shape is (channels, height, width)\n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_latent_trunk = 1 # 1 corresponds to t\n",
    "latent_trunk_net = DenseNet(layersizes=[input_neurons_latent_trunk] + [128]*4 + [latent_p], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_trunk_net.to(device)\n",
    "# print(latent_trunk_net)\n",
    "print('LATENT TRUNK-NET SUMMARY:')\n",
    "summary(latent_trunk_net, input_size=(input_neurons_latent_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "\"\"\"\n",
    "input_neurons_reconstruction_branch: Number of input neurons in the reconstruction_branch net.\n",
    "input_neurons_reconstruction_trunk: Number of input neurons in the reconstruction_trunk net.\n",
    "reconstruction_q: Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\"\"\"\n",
    "reconstruction_q = 128 # Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\n",
    "input_neurons_reconstruction_branch = latent_dim # d_z\n",
    "reconstruction_branch_net = DenseNet(layersizes=[input_neurons_reconstruction_branch] + [128]*4 + [reconstruction_q], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_branch_net.to(device)\n",
    "# print(reconstruction_branch_net)\n",
    "print('RECONSTRUCTION BRANCH-NET SUMMARY:')\n",
    "summary(reconstruction_branch_net, input_size=(input_neurons_reconstruction_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_reconstruction_trunk = 2 # 2 corresponds to x and y\n",
    "reconstruction_trunk_net = DenseNet(layersizes=[input_neurons_reconstruction_trunk] + [128]*4 + [reconstruction_q], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_trunk_net.to(device)\n",
    "# print(reconstruction_trunk_net)\n",
    "print('RECONSTRUCTION TRUNK-NET SUMMARY:')\n",
    "summary(reconstruction_trunk_net, input_size=(input_neurons_reconstruction_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Latent_NO_model(latent_branch_net, latent_trunk_net, latent_dim, reconstruction_branch_net, reconstruction_trunk_net)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86a175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:53:41.091096Z",
     "iopub.status.busy": "2024-12-30T23:53:41.090821Z",
     "iopub.status.idle": "2024-12-30T23:53:41.093878Z",
     "shell.execute_reply": "2024-12-30T23:53:41.093532Z"
    },
    "papermill": {
     "duration": 0.210669,
     "end_time": "2024-12-30T23:53:41.094687",
     "exception": false,
     "start_time": "2024-12-30T23:53:40.884018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_learnable_parameters = (count_learnable_parameters(latent_branch_net)\n",
    "                            + count_learnable_parameters(latent_trunk_net)\n",
    "                            + count_learnable_parameters(reconstruction_branch_net)\n",
    "                            + count_learnable_parameters(reconstruction_trunk_net))\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908032c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:53:41.536237Z",
     "iopub.status.busy": "2024-12-30T23:53:41.536013Z",
     "iopub.status.idle": "2024-12-30T23:53:41.539458Z",
     "shell.execute_reply": "2024-12-30T23:53:41.539128Z"
    },
    "papermill": {
     "duration": 0.223992,
     "end_time": "2024-12-30T23:53:41.540306",
     "exception": false,
     "start_time": "2024-12-30T23:53:41.316314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def u_pred(net, inputs, t, x, y):\n",
    "    # enforcing ICs and BCs in hard way by multiplying model output with factor\n",
    "    # factor = t*(x-(-L))*(x-L)*(y-(-L))*(y-L)/(T*(2*L)*(2*L)*(2*L)*(2*L)) calculating this by appropriate broadcasting\n",
    "    t_term = t.unsqueeze(0)  # (1, neval_t, 1)\n",
    "    x_term = ((x-(-L))*(x-L)).squeeze() # (neval_loc,)\n",
    "    y_term = ((y-(-L))*(y-L)).squeeze() # (neval_loc,)\n",
    "    factor = t_term * x_term * y_term / (T*(2*L)*(2*L)*(2*L)*(2*L)) # (1, neval_t, neval_loc) due to broadcasting\n",
    "    latent_prediction, reconstruction_prediction = net(inputs, t, torch.hstack([x, y]))\n",
    "    u = reconstruction_prediction*factor # (bs, neval_t, neval_loc) # broadcasted element-wise\n",
    "    return latent_prediction, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffb765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:53:41.972530Z",
     "iopub.status.busy": "2024-12-30T23:53:41.972280Z",
     "iopub.status.idle": "2024-12-30T23:53:41.977497Z",
     "shell.execute_reply": "2024-12-30T23:53:41.977164Z"
    },
    "papermill": {
     "duration": 0.222509,
     "end_time": "2024-12-30T23:53:41.978306",
     "exception": false,
     "start_time": "2024-12-30T23:53:41.755797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, source_fields_parameters, source_fields, t, x, y):\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x, tangent_y = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device), torch.ones(y.shape).to(device)\n",
    "    ut = FWDAD_first_order_derivative(lambda t: u_pred(net, source_fields, t, x, y)[1], t, tangent_t) # (bs, neval_t, neval_loc)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: u_pred(net, source_fields, t, x, y)[1], x, tangent_x) # (bs, neval_t, neval_loc)\n",
    "    uyy = FWDAD_second_order_derivative(lambda y: u_pred(net, source_fields, t, x, y)[1], y, tangent_y) # (bs, neval_t, neval_loc)\n",
    "    \n",
    "    bs_ = source_fields.shape[0]\n",
    "    sf_values_ = torch.zeros((bs_, x.shape[0])).to(device)\n",
    "    for j in range(bs_):\n",
    "        source_class = Source(a=source_fields_parameters[j][3], r=source_fields_parameters[j][2], \n",
    "                      x=x, y=y, \n",
    "                      xc=0., yc=0.,\n",
    "                      device=device)\n",
    "        shape = get_key_from_value(shape_map, source_fields_parameters[j, 0])\n",
    "        sf_values_[j] = source_class.type_source(shape, num_sides=int(source_fields_parameters[j, 1])).flatten() # source function: s(x, y) values\n",
    "    sf_values__ = sf_values_.unsqueeze(1)\n",
    "    # Repeat elements along neval_loc for neval_t times and reshape\n",
    "    sf_values = sf_values__.repeat(1, t.shape[0], 1) # (bs, neval_t, neval_loc) # s(x, y) values are same for all times \n",
    "\n",
    "    pde_residual = (ut - (D_value*uxx) - (D_value*uyy) - sf_values)**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e40852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T23:53:42.410163Z",
     "iopub.status.busy": "2024-12-30T23:53:42.409890Z",
     "iopub.status.idle": "2024-12-31T03:25:57.063904Z",
     "shell.execute_reply": "2024-12-31T03:25:57.063554Z"
    },
    "papermill": {
     "duration": 12734.871364,
     "end_time": "2024-12-31T03:25:57.065035",
     "exception": false,
     "start_time": "2024-12-30T23:53:42.193671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 128 # Batch size\n",
    "\n",
    "neval_t = 20  # Number of time points at which latent output field is evaluated for a given input sample.\n",
    "neval_x = 64 \n",
    "neval_y = 64 \n",
    "# neval_loc = neval_x*neval_y  # Number of locations at which output field is evaluated at each time point.\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x*neval_y}  # Number of collocation points within the domain.\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used > 0:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        inputs_train_used_batch = inputs_train_used.reshape(-1, 1, ny, nx)[indices_datadriven[0:bs]]\n",
    "        latent_outputs_train_used_batch = latent_outputs_train_used[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used.reshape(-1, nt, nx*ny)[indices_datadriven[0:bs]]\n",
    "        #print(f\"Shape of inputs_train_used_batch[{i}]:\", inputs_train_used_batch.shape) # (bs, no. of channels, height, width)\n",
    "        #print(f\"Shape of latent_outputs_train_used_batch[{i}]:\", latent_outputs_train_used_batch.shape) # (bs, nt, latent_dim)\n",
    "        #print(f\"Shape of outputs_train_used_batch[{i}]:\", outputs_train_used_batch.shape) # (bs, nt, nx*ny)\n",
    "\n",
    "        latent_predicted_values, reconstruction_predicted_values = u_pred(model, inputs_train_used_batch, \n",
    "                                                                          t_span.reshape(-1, 1), \n",
    "                                                                          x_span.flatten().reshape(-1,1), \n",
    "                                                                          y_span.flatten().reshape(-1,1)) # (bs, nt, latent_dim), (bs, nt, nx*ny)\n",
    "        latent_target_values = latent_outputs_train_used_batch # (bs, nt, latent_dim)\n",
    "        reconstruction_target_values = outputs_train_used_batch # (bs, nt, nx*ny)\n",
    "        datadriven_loss = nn.MSELoss()(latent_predicted_values, latent_target_values) + nn.MSELoss()(reconstruction_predicted_values, reconstruction_target_values)\n",
    "        # print('*********')\n",
    "    \n",
    "    elif n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "\n",
    "    num_samples = stove_source_fields_only['input_samples'].shape[0]\n",
    "    indices_pinn = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "    input_parameters_batch = stove_source_fields_only['input_parameters'][indices_pinn[0:bs]]\n",
    "    inputs_batch = stove_source_fields_only['input_samples'].reshape(-1, 1, ny, nx)[indices_pinn[0:bs]]\n",
    "    #print(f\"Shape of inputs_train_batch[{i}]:\", inputs_batch.shape) # (bs, no. of channels, height, width)\n",
    "\n",
    "    # points within the domain\n",
    "    tc = td.uniform.Uniform(0., T).sample((neval_c['t'], 1)).to(device)\n",
    "    xc = td.uniform.Uniform(-L, L).sample((neval_c['loc'], 1)).to(device)\n",
    "    yc = td.uniform.Uniform(-L, L).sample((neval_c['loc'], 1)).to(device)\n",
    "\n",
    "    pinn_loss = loss_pde_residual(model, input_parameters_batch, inputs_batch, tc, xc, yc) \n",
    "    # print('*********')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if iteration % 500 == 0:\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f' % optimizer.state_dict()['param_groups'][0]['lr']) \n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    \n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "    \n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save)  \n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)  \n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d2881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T03:25:57.487394Z",
     "iopub.status.busy": "2024-12-31T03:25:57.487085Z",
     "iopub.status.idle": "2024-12-31T03:25:57.496118Z",
     "shell.execute_reply": "2024-12-31T03:25:57.495769Z"
    },
    "papermill": {
     "duration": 0.221759,
     "end_time": "2024-12-31T03:25:57.496923",
     "exception": false,
     "start_time": "2024-12-31T03:25:57.275164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir, 'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c742e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T03:25:57.951797Z",
     "iopub.status.busy": "2024-12-31T03:25:57.951464Z",
     "iopub.status.idle": "2024-12-31T03:31:25.042832Z",
     "shell.execute_reply": "2024-12-31T03:31:25.042489Z"
    },
    "papermill": {
     "duration": 327.698166,
     "end_time": "2024-12-31T03:31:25.415879",
     "exception": false,
     "start_time": "2024-12-31T03:25:57.717713",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "latent_predictions_test, reconstruction_predictions_test = u_pred(model, inputs_test.reshape(-1, 1, ny, nx), \n",
    "                                                                  t_span.reshape(-1, 1), \n",
    "                                                                  x_span.flatten().reshape(-1,1), \n",
    "                                                                  y_span.flatten().reshape(-1,1)) # (bs, nt, latent_dim), (bs, nt, nx*ny)\n",
    "# print(latent_predictions_test.shape, reconstruction_predictions_test.shape)\n",
    "\n",
    "mse_list, latent_mse_list, reconstruction_mse_list, r2score_list, relerror_list = [], [], [], [], []\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "    \n",
    "    latent_prediction_i, reconstruction_prediction_i = latent_predictions_test[i].unsqueeze(0), reconstruction_predictions_test[i].unsqueeze(0)# (1, nt, latent_dim), (1, nt, nx*ny)\n",
    "    \n",
    "    if n_used > 0:\n",
    "        latent_target_i = latent_outputs_test[i].unsqueeze(0) # (1, nt, latent_dim)\n",
    "    reconstruction_target_i = outputs_test[i].reshape(nt, nx*ny).unsqueeze(0) # (1, nt, nx*ny)\n",
    "    \n",
    "    if n_used > 0:\n",
    "        latent_mse_i = F.mse_loss(latent_prediction_i.cpu(), latent_target_i.cpu())\n",
    "    elif n_used == 0:\n",
    "        latent_mse_i = torch.tensor([0.])\n",
    "    reconstruction_mse_i = F.mse_loss(reconstruction_prediction_i.cpu(), reconstruction_target_i.cpu())\n",
    "    mse_i = latent_mse_i + reconstruction_mse_i\n",
    "    \n",
    "    r2score_i = metrics.r2_score(reconstruction_target_i.flatten().cpu().detach().numpy(), reconstruction_prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy() - reconstruction_prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy())\n",
    "        \n",
    "    latent_mse_list.append(latent_mse_i.item())\n",
    "    reconstruction_mse_list.append(reconstruction_mse_i.item())\n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item())\n",
    "    \n",
    "    # Plot the full solution-field for few cases (2 groups i.e., 10*2=20):\n",
    "    if (i+1) <= 20:\n",
    "        print(colored('TEST SAMPLE '+str(i+1), 'red'))\n",
    "        shape = get_key_from_value(shape_map, input_parameters_test[i, 0])\n",
    "        print(colored(f\"Shape = {shape}, r = {input_parameters_test[i,2]:.3f}, a_value = {input_parameters_test[i,3]:.3f}\", 'red'))\n",
    "        \n",
    "        r2score_i = float('%.4f'%r2score_i)\n",
    "        relerror_i = float('%.4f'%relerror_i)\n",
    "        print('Rel. L2 Error = '+str(relerror_i)+', R2 score = '+str(r2score_i))\n",
    "        \n",
    "        # Plotting \n",
    "        plot_source(i, x_span, y_span, inputs_test[i], f\"{shape.capitalize()} Source\", 'hot', resultdir, save)\n",
    "        \n",
    "        cmap = 'hot'  # Color map\n",
    "        fontsize = 14  # Font size for labels and titles\n",
    "        levels = 100\n",
    "        plot_solution(i, x_span, y_span, reconstruction_target_i.reshape(nt,ny,nx), t_span, f\"True Solution for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir, save, 'True-Solution')\n",
    "        plot_solution(i, x_span, y_span, reconstruction_prediction_i.reshape(nt,ny,nx), t_span, f\"Predicted Solution for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir, save, 'Predicted-Solution')\n",
    "        plot_solution(i, x_span, y_span, torch.abs(reconstruction_target_i.reshape(nt,ny,nx) - reconstruction_prediction_i.reshape(nt,ny,nx)), t_span, f\"Absolute error for {shape.capitalize()} Source\", cmap, fontsize, levels, resultdir, save, 'Absolute error')\n",
    "        print(colored('#'*230, 'green'))\n",
    "        \n",
    "latent_mse = sum(latent_mse_list) / len(latent_mse_list)\n",
    "print(\"Latent Mean Squared Error Test:\\n\", latent_mse)\n",
    "reconstruction_mse = sum(reconstruction_mse_list) / len(reconstruction_mse_list)\n",
    "print(\"Reconstruction Mean Squared Error Test:\\n\", reconstruction_mse)\n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0b218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T03:31:26.496963Z",
     "iopub.status.busy": "2024-12-31T03:31:26.496648Z",
     "iopub.status.idle": "2024-12-31T03:31:26.600679Z",
     "shell.execute_reply": "2024-12-31T03:31:26.600301Z"
    },
    "papermill": {
     "duration": 0.644919,
     "end_time": "2024-12-31T03:31:26.601501",
     "exception": false,
     "start_time": "2024-12-31T03:31:25.956582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"input_parameters_test\": input_parameters_test.cpu(),\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": reconstruction_predictions_test.reshape(-1, nt, ny, nx).cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6d8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T03:31:27.692514Z",
     "iopub.status.busy": "2024-12-31T03:31:27.692247Z",
     "iopub.status.idle": "2024-12-31T03:31:27.695745Z",
     "shell.execute_reply": "2024-12-31T03:31:27.695288Z"
    },
    "papermill": {
     "duration": 0.550425,
     "end_time": "2024-12-31T03:31:27.696621",
     "exception": false,
     "start_time": "2024-12-31T03:31:27.146196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(reconstruction_mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a366414",
   "metadata": {
    "papermill": {
     "duration": 0.554642,
     "end_time": "2024-12-31T03:31:28.795196",
     "exception": false,
     "start_time": "2024-12-31T03:31:28.240554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e4665",
   "metadata": {
    "papermill": {
     "duration": 0.543094,
     "end_time": "2024-12-31T03:31:29.883909",
     "exception": false,
     "start_time": "2024-12-31T03:31:29.340815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13387.301707,
   "end_time": "2024-12-31T03:31:31.845135",
   "environment_variables": {},
   "exception": null,
   "input_path": "c_PI-Latent-NO_with-AE.ipynb",
   "output_path": "results/c_PI-Latent-NO_with-AE/seed=0_n_used=200/output_seed=0_n_used=200.ipynb",
   "parameters": {
    "n_used": 200,
    "save": true,
    "seed": 0
   },
   "start_time": "2024-12-30T23:48:24.543428",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
