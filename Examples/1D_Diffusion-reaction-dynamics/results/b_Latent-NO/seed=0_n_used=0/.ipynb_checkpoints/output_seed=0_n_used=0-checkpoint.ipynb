{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6a55d2",
   "metadata": {
    "papermill": {
     "duration": 0.003854,
     "end_time": "2025-03-09T06:47:55.670341",
     "exception": false,
     "start_time": "2025-03-09T06:47:55.666487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Comparisons\n",
    "\n",
    "This code can execute the following three variants of losses:  \n",
    "\n",
    "1. **Variant 1:**  Purely Physics  \n",
    "   $L_{\\theta} = L_{\\text{PDE}}$  \n",
    "   Use $n_{\\text{used}} = 0$  \n",
    "\n",
    "2. **Variant 2:**  Physics + Data  \n",
    "   $L_{\\theta} = L_{\\text{PDE}} + \\Sigma_{i=1}^{n_{\\text{used}}}\\| u_i - \\hat{u}_i \\|_2^2$  \n",
    "   Use $n_{\\text{used}} \\in (0, 1000]$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a75ad",
   "metadata": {
    "papermill": {
     "duration": 0.002307,
     "end_time": "2025-03-09T06:47:55.675490",
     "exception": false,
     "start_time": "2025-03-09T06:47:55.673183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1395ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:47:55.681092Z",
     "iopub.status.busy": "2025-03-09T06:47:55.680763Z",
     "iopub.status.idle": "2025-03-09T06:48:04.003785Z",
     "shell.execute_reply": "2025-03-09T06:48:04.003132Z"
    },
    "papermill": {
     "duration": 8.327156,
     "end_time": "2025-03-09T06:48:04.004978",
     "exception": false,
     "start_time": "2025-03-09T06:47:55.677822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from utils.networks import *\n",
    "from utils.deeponet_networks_1d import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.visualizer_1d import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921aabe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:04.012030Z",
     "iopub.status.busy": "2025-03-09T06:48:04.011755Z",
     "iopub.status.idle": "2025-03-09T06:48:04.014528Z",
     "shell.execute_reply": "2025-03-09T06:48:04.014049Z"
    },
    "papermill": {
     "duration": 0.006498,
     "end_time": "2025-03-09T06:48:04.015195",
     "exception": false,
     "start_time": "2025-03-09T06:48:04.008697",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Tag this cell with 'parameters'\n",
    "# parameters\n",
    "seed = 0 # Seed number.\n",
    "n_used = 100 # Number of full training fields used for estimating the data-driven loss term\n",
    "save = True # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b7fd648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:04.020895Z",
     "iopub.status.busy": "2025-03-09T06:48:04.020488Z",
     "iopub.status.idle": "2025-03-09T06:48:04.022808Z",
     "shell.execute_reply": "2025-03-09T06:48:04.022402Z"
    },
    "papermill": {
     "duration": 0.005796,
     "end_time": "2025-03-09T06:48:04.023430",
     "exception": false,
     "start_time": "2025-03-09T06:48:04.017634",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 0\n",
    "n_used = 0\n",
    "save = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8bd328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:04.028989Z",
     "iopub.status.busy": "2025-03-09T06:48:04.028695Z",
     "iopub.status.idle": "2025-03-09T06:48:04.031561Z",
     "shell.execute_reply": "2025-03-09T06:48:04.031133Z"
    },
    "papermill": {
     "duration": 0.006316,
     "end_time": "2025-03-09T06:48:04.032203",
     "exception": false,
     "start_time": "2025-03-09T06:48:04.025887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','b_Latent-NO','seed='+str(seed)+'_n_used='+str(n_used)) \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7126e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:04.037926Z",
     "iopub.status.busy": "2025-03-09T06:48:04.037557Z",
     "iopub.status.idle": "2025-03-09T06:48:04.041558Z",
     "shell.execute_reply": "2025-03-09T06:48:04.041118Z"
    },
    "papermill": {
     "duration": 0.007462,
     "end_time": "2025-03-09T06:48:04.042164",
     "exception": false,
     "start_time": "2025-03-09T06:48:04.034702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 0\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d11f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:04.047947Z",
     "iopub.status.busy": "2025-03-09T06:48:04.047667Z",
     "iopub.status.idle": "2025-03-09T06:48:04.222914Z",
     "shell.execute_reply": "2025-03-09T06:48:04.222395Z"
    },
    "papermill": {
     "duration": 0.178889,
     "end_time": "2025-03-09T06:48:04.223675",
     "exception": false,
     "start_time": "2025-03-09T06:48:04.044786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d346e7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:04.230628Z",
     "iopub.status.busy": "2025-03-09T06:48:04.230172Z",
     "iopub.status.idle": "2025-03-09T06:48:04.413810Z",
     "shell.execute_reply": "2025-03-09T06:48:04.413266Z"
    },
    "papermill": {
     "duration": 0.18771,
     "end_time": "2025-03-09T06:48:04.414524",
     "exception": false,
     "start_time": "2025-03-09T06:48:04.226814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile '../../data/1D_Diffusion-reaction_dynamics_t=0to1/Diffusion-reaction_dynamics.npz' with keys: input_s_samples, output_u_samples, t_span, x_span\n",
      "(101,)\n",
      "(100,)\n",
      "(2500, 100)\n",
      "(2500, 101, 100)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = np.load(os.path.join('..','..','data/1D_Diffusion-reaction_dynamics_t=0to1/Diffusion-reaction_dynamics.npz')) # Load the .npz file\n",
    "print(data)\n",
    "print(data['t_span'].shape)\n",
    "print(data['x_span'].shape)\n",
    "print(data['input_s_samples'].shape) # Random Source fields: Gaussian random fields, Nsamples x 100, each sample is (1 x 100)\n",
    "print(data['output_u_samples'].shape) # Time evolution of the solution field: Nsamples x 101 x 100.\n",
    "                               # Each field is 101 x 100, rows correspond to time and columns respond to location.\n",
    "                               # First row corresponds to solution at t=0 (1st time step)\n",
    "                               # and next  row corresponds to solution at t=0.01 (2nd time step) and so on.\n",
    "                               # last row correspond to solution at t=1 (101th time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b18da80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:04.421292Z",
     "iopub.status.busy": "2025-03-09T06:48:04.420797Z",
     "iopub.status.idle": "2025-03-09T06:48:05.701976Z",
     "shell.execute_reply": "2025-03-09T06:48:05.701474Z"
    },
    "papermill": {
     "duration": 1.285129,
     "end_time": "2025-03-09T06:48:05.702657",
     "exception": false,
     "start_time": "2025-03-09T06:48:04.417528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt = 101 , nx = 100\n",
      "Shape of t-span and x-span: torch.Size([101]) torch.Size([100])\n",
      "t-span: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
      "        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n",
      "        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n",
      "        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n",
      "        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n",
      "        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n",
      "        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n",
      "        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n",
      "        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n",
      "        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n",
      "        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n",
      "        0.9900, 1.0000], device='cuda:0')\n",
      "x-span: tensor([0.0050, 0.0150, 0.0250, 0.0350, 0.0450, 0.0550, 0.0650, 0.0750, 0.0850,\n",
      "        0.0950, 0.1050, 0.1150, 0.1250, 0.1350, 0.1450, 0.1550, 0.1650, 0.1750,\n",
      "        0.1850, 0.1950, 0.2050, 0.2150, 0.2250, 0.2350, 0.2450, 0.2550, 0.2650,\n",
      "        0.2750, 0.2850, 0.2950, 0.3050, 0.3150, 0.3250, 0.3350, 0.3450, 0.3550,\n",
      "        0.3650, 0.3750, 0.3850, 0.3950, 0.4050, 0.4150, 0.4250, 0.4350, 0.4450,\n",
      "        0.4550, 0.4650, 0.4750, 0.4850, 0.4950, 0.5050, 0.5150, 0.5250, 0.5350,\n",
      "        0.5450, 0.5550, 0.5650, 0.5750, 0.5850, 0.5950, 0.6050, 0.6150, 0.6250,\n",
      "        0.6350, 0.6450, 0.6550, 0.6650, 0.6750, 0.6850, 0.6950, 0.7050, 0.7150,\n",
      "        0.7250, 0.7350, 0.7450, 0.7550, 0.7650, 0.7750, 0.7850, 0.7950, 0.8050,\n",
      "        0.8150, 0.8250, 0.8350, 0.8450, 0.8550, 0.8650, 0.8750, 0.8850, 0.8950,\n",
      "        0.9050, 0.9150, 0.9250, 0.9350, 0.9450, 0.9550, 0.9650, 0.9750, 0.9850,\n",
      "        0.9950], device='cuda:0')\n",
      "Shape of inputs_train: torch.Size([1000, 100])\n",
      "Shape of inputs_test: torch.Size([500, 100])\n",
      "Shape of outputs_train: torch.Size([1000, 101, 100])\n",
      "Shape of outputs_test: torch.Size([500, 101, 100])\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "inputs = torch.from_numpy(data['input_s_samples'][0:1500]).float().to(device)\n",
    "outputs = torch.from_numpy(data['output_u_samples'][0:1500]).float().to(device)\n",
    "\n",
    "t_span = torch.from_numpy(data['t_span']).float().to(device)\n",
    "x_span = torch.from_numpy(data['x_span']).float().to(device)\n",
    "nt, nx = len(t_span), len(x_span) # number of discretizations in time and location.\n",
    "print(\"nt =\",nt, \", nx =\",nx)\n",
    "print(\"Shape of t-span and x-span:\",t_span.shape, x_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "\n",
    "# Estimating grid points\n",
    "T, X = torch.meshgrid(t_span, x_span)\n",
    "# print(T)\n",
    "# print(X)\n",
    "\n",
    "# Split the data into training and testing samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=500, random_state=seed)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eca370b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:05.710226Z",
     "iopub.status.busy": "2025-03-09T06:48:05.710047Z",
     "iopub.status.idle": "2025-03-09T06:48:05.735117Z",
     "shell.execute_reply": "2025-03-09T06:48:05.734631Z"
    },
    "papermill": {
     "duration": 0.029016,
     "end_time": "2025-03-09T06:48:05.735766",
     "exception": false,
     "start_time": "2025-03-09T06:48:05.706750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inputs_train_used: torch.Size([0, 100])\n",
      "Shape of outputs_train_used: torch.Size([0, 101, 100])\n"
     ]
    }
   ],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term in the PI-Latent-NO\n",
    "inputs_train_used = inputs_train[:n_used, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ee4bf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:05.742442Z",
     "iopub.status.busy": "2025-03-09T06:48:05.742085Z",
     "iopub.status.idle": "2025-03-09T06:48:05.744365Z",
     "shell.execute_reply": "2025-03-09T06:48:05.743954Z"
    },
    "papermill": {
     "duration": 0.006165,
     "end_time": "2025-03-09T06:48:05.744892",
     "exception": false,
     "start_time": "2025-03-09T06:48:05.738727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_dim = 9 # d_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5719c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:05.751170Z",
     "iopub.status.busy": "2025-03-09T06:48:05.751007Z",
     "iopub.status.idle": "2025-03-09T06:48:06.008110Z",
     "shell.execute_reply": "2025-03-09T06:48:06.007613Z"
    },
    "papermill": {
     "duration": 0.261099,
     "end_time": "2025-03-09T06:48:06.008784",
     "exception": false,
     "start_time": "2025-03-09T06:48:05.747685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATENT BRANCH-NET SUMMARY:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]           6,464\n",
      "              SiLU-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 64]           4,160\n",
      "              SiLU-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 64]           4,160\n",
      "              SiLU-6                   [-1, 64]               0\n",
      "            Linear-7                  [-1, 225]          14,625\n",
      "================================================================\n",
      "Total params: 29,409\n",
      "Trainable params: 29,409\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 0.12\n",
      "----------------------------------------------------------------\n",
      "####################################################################################################\n",
      "LATENT TRUNK-NET SUMMARY:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             128\n",
      "              SiLU-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 64]           4,160\n",
      "              SiLU-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 64]           4,160\n",
      "              SiLU-6                   [-1, 64]               0\n",
      "            Linear-7                  [-1, 225]          14,625\n",
      "================================================================\n",
      "Total params: 23,073\n",
      "Trainable params: 23,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 0.09\n",
      "----------------------------------------------------------------\n",
      "####################################################################################################\n",
      "RECONSTRUCTION BRANCH-NET SUMMARY:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             640\n",
      "              SiLU-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 64]           4,160\n",
      "              SiLU-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 64]           4,160\n",
      "              SiLU-6                   [-1, 64]               0\n",
      "            Linear-7                  [-1, 128]           8,320\n",
      "================================================================\n",
      "Total params: 17,280\n",
      "Trainable params: 17,280\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 0.07\n",
      "----------------------------------------------------------------\n",
      "####################################################################################################\n",
      "RECONSTRUCTION TRUNK-NET SUMMARY:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             128\n",
      "              SiLU-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 64]           4,160\n",
      "              SiLU-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 64]           4,160\n",
      "              SiLU-6                   [-1, 64]               0\n",
      "            Linear-7                  [-1, 128]           8,320\n",
      "================================================================\n",
      "Total params: 16,768\n",
      "Trainable params: 16,768\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.07\n",
      "----------------------------------------------------------------\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "input_neurons_latent_branch: Number of input neurons in the latent_branch net.\n",
    "input_neurons_latent_trunk: Number of input neurons in the latent_trunk net.\n",
    "latent_p: Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\"\"\"\n",
    "latent_p = latent_dim*25 # Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\n",
    "input_neurons_latent_branch = nx # m\n",
    "latent_branch_net = DenseNet(layersizes=[input_neurons_latent_branch] + [64]*3 + [latent_p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_branch_net.to(device)\n",
    "# print(latent_branch_net)\n",
    "print('LATENT BRANCH-NET SUMMARY:')\n",
    "summary(latent_branch_net, input_size=(input_neurons_latent_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_latent_trunk = 1 # 1 corresponds to t\n",
    "latent_trunk_net = DenseNet(layersizes=[input_neurons_latent_trunk] + [64]*3 + [latent_p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_trunk_net.to(device)\n",
    "# print(latent_trunk_net)\n",
    "print('LATENT TRUNK-NET SUMMARY:')\n",
    "summary(latent_trunk_net, input_size=(input_neurons_latent_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "\"\"\"\n",
    "input_neurons_reconstruction_branch: Number of input neurons in the reconstruction_branch net.\n",
    "input_neurons_reconstruction_trunk: Number of input neurons in the reconstruction_trunk net.\n",
    "reconstruction_q: Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\"\"\"\n",
    "reconstruction_q = 128 # Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\n",
    "input_neurons_reconstruction_branch = latent_dim # d_z\n",
    "reconstruction_branch_net = DenseNet(layersizes=[input_neurons_reconstruction_branch] + [64]*3 + [reconstruction_q], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_branch_net.to(device)\n",
    "# print(reconstruction_branch_net)\n",
    "print('RECONSTRUCTION BRANCH-NET SUMMARY:')\n",
    "summary(reconstruction_branch_net, input_size=(input_neurons_reconstruction_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_reconstruction_trunk = 1 # 1 corresponds to x\n",
    "reconstruction_trunk_net = DenseNet(layersizes=[input_neurons_reconstruction_trunk] + [64]*3 + [reconstruction_q], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_trunk_net.to(device)\n",
    "# print(reconstruction_trunk_net)\n",
    "print('RECONSTRUCTION TRUNK-NET SUMMARY:')\n",
    "summary(reconstruction_trunk_net, input_size=(input_neurons_reconstruction_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Latent_NO_model(latent_branch_net, latent_trunk_net, latent_dim, reconstruction_branch_net, reconstruction_trunk_net)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb86a175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:06.016173Z",
     "iopub.status.busy": "2025-03-09T06:48:06.015808Z",
     "iopub.status.idle": "2025-03-09T06:48:06.018871Z",
     "shell.execute_reply": "2025-03-09T06:48:06.018447Z"
    },
    "papermill": {
     "duration": 0.007187,
     "end_time": "2025-03-09T06:48:06.019471",
     "exception": false,
     "start_time": "2025-03-09T06:48:06.012284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of learnable parameters: 86530\n"
     ]
    }
   ],
   "source": [
    "num_learnable_parameters = (count_learnable_parameters(latent_branch_net)\n",
    "                            + count_learnable_parameters(latent_trunk_net)\n",
    "                            + count_learnable_parameters(reconstruction_branch_net)\n",
    "                            + count_learnable_parameters(reconstruction_trunk_net))\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7510cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:06.026407Z",
     "iopub.status.busy": "2025-03-09T06:48:06.026036Z",
     "iopub.status.idle": "2025-03-09T06:48:06.030383Z",
     "shell.execute_reply": "2025-03-09T06:48:06.029968Z"
    },
    "papermill": {
     "duration": 0.008315,
     "end_time": "2025-03-09T06:48:06.030949",
     "exception": false,
     "start_time": "2025-03-09T06:48:06.022634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, source_fields, t, x):\n",
    "    \n",
    "    _, u = net(source_fields, t, x) # u is (bs, neval_t, neval_x)\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device)\n",
    "    ut  = FWDAD_first_order_derivative(lambda t: net(source_fields, t, x)[1], t, tangent_t) # (bs, neval_t, neval_x)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: net(source_fields, t, x)[1], x, tangent_x) # (bs, neval_t, neval_x)\n",
    "    \n",
    "    bs_ = source_fields.shape[0]\n",
    "    sf_values_ = torch.zeros((bs_, x.shape[0], 1)).to(device)\n",
    "    for j in range(bs_):\n",
    "        sf_values_[j] = linear_interpolation_1D(x, x_span, source_fields[j]) # source function: s(x) values\n",
    "    sf_values__ = sf_values_.reshape(-1, 1, x.shape[0]) # (bs, 1, neval_x)\n",
    "    # Repeat elements along neval_x for neval_t times and reshape\n",
    "    sf_values = sf_values__.repeat(1, neval_t, 1) # (bs, neval_t, neval_x) # s(x) values are same for all times\n",
    "    \n",
    "    pde_residual = (ut - (0.01*uxx) - (0.01*(u**2)) - sf_values)**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ead0afa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:06.037842Z",
     "iopub.status.busy": "2025-03-09T06:48:06.037551Z",
     "iopub.status.idle": "2025-03-09T06:48:06.040737Z",
     "shell.execute_reply": "2025-03-09T06:48:06.040331Z"
    },
    "papermill": {
     "duration": 0.007281,
     "end_time": "2025-03-09T06:48:06.041332",
     "exception": false,
     "start_time": "2025-03-09T06:48:06.034051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_bcs(net, source_fields, t, x):\n",
    "    \n",
    "    t_b1, x_b1 = t[0], x[0]\n",
    "    t_b2, x_b2 = t[1], x[1]\n",
    "\n",
    "    _, u_b1 = net(source_fields, t_b1, x_b1) # u is (bs, neval_t, 1)\n",
    "    _, u_b2 = net(source_fields, t_b2, x_b2) # u is (bs, neval_t, 1)\n",
    "\n",
    "    bc1_value, bc2_value = 0., 0.\n",
    "    pde_bc1 = (u_b1 - bc1_value)**2\n",
    "    pde_bc2 = (u_b2 - bc2_value)**2\n",
    "    \n",
    "    return torch.mean(pde_bc1) + torch.mean(pde_bc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c48197e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:06.048617Z",
     "iopub.status.busy": "2025-03-09T06:48:06.048289Z",
     "iopub.status.idle": "2025-03-09T06:48:06.050904Z",
     "shell.execute_reply": "2025-03-09T06:48:06.050507Z"
    },
    "papermill": {
     "duration": 0.007067,
     "end_time": "2025-03-09T06:48:06.051492",
     "exception": false,
     "start_time": "2025-03-09T06:48:06.044425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_ic(net, source_fields, t, x):\n",
    "\n",
    "    _, u_ic = net(source_fields, t, x) # u is (bs, 1, neval_x)\n",
    "    \n",
    "    ic_value = 0.\n",
    "    pde_ic = (u_ic - ic_value)**2\n",
    "    \n",
    "    return torch.mean(pde_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47e40852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T06:48:06.058290Z",
     "iopub.status.busy": "2025-03-09T06:48:06.058138Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-03-09T06:48:06.054539",
     "status": "running"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - loss = 1.099594, data-driven loss = 0.000000, pinn loss = 1.099594, learning rate = 0.003500, test loss = 0.296187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500 - loss = 0.195588, data-driven loss = 0.000000, pinn loss = 0.195588, learning rate = 0.003500, test loss = 0.015260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000 - loss = 0.081195, data-driven loss = 0.000000, pinn loss = 0.081195, learning rate = 0.003500, test loss = 0.014489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1500 - loss = 0.088828, data-driven loss = 0.000000, pinn loss = 0.088828, learning rate = 0.003500, test loss = 0.012748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000 - loss = 0.032802, data-driven loss = 0.000000, pinn loss = 0.032802, learning rate = 0.003500, test loss = 0.004670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2500 - loss = 0.033411, data-driven loss = 0.000000, pinn loss = 0.033411, learning rate = 0.003500, test loss = 0.005459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000 - loss = 0.012121, data-driven loss = 0.000000, pinn loss = 0.012121, learning rate = 0.003500, test loss = 0.001936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3500 - loss = 0.014082, data-driven loss = 0.000000, pinn loss = 0.014082, learning rate = 0.003500, test loss = 0.001558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000 - loss = 0.013300, data-driven loss = 0.000000, pinn loss = 0.013300, learning rate = 0.003500, test loss = 0.001176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4500 - loss = 0.008476, data-driven loss = 0.000000, pinn loss = 0.008476, learning rate = 0.003500, test loss = 0.001076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000 - loss = 0.013364, data-driven loss = 0.000000, pinn loss = 0.013364, learning rate = 0.003500, test loss = 0.001914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5500 - loss = 0.009058, data-driven loss = 0.000000, pinn loss = 0.009058, learning rate = 0.003500, test loss = 0.001926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6000 - loss = 0.004448, data-driven loss = 0.000000, pinn loss = 0.004448, learning rate = 0.003500, test loss = 0.000511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6500 - loss = 0.004021, data-driven loss = 0.000000, pinn loss = 0.004021, learning rate = 0.003500, test loss = 0.000452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7000 - loss = 0.004543, data-driven loss = 0.000000, pinn loss = 0.004543, learning rate = 0.003500, test loss = 0.000550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7500 - loss = 0.003693, data-driven loss = 0.000000, pinn loss = 0.003693, learning rate = 0.003500, test loss = 0.000524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8000 - loss = 0.005485, data-driven loss = 0.000000, pinn loss = 0.005485, learning rate = 0.003500, test loss = 0.001077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8500 - loss = 0.001771, data-driven loss = 0.000000, pinn loss = 0.001771, learning rate = 0.003500, test loss = 0.000177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9000 - loss = 0.002135, data-driven loss = 0.000000, pinn loss = 0.002135, learning rate = 0.003500, test loss = 0.000232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9500 - loss = 0.002175, data-driven loss = 0.000000, pinn loss = 0.002175, learning rate = 0.003500, test loss = 0.000355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000 - loss = 0.011335, data-driven loss = 0.000000, pinn loss = 0.011335, learning rate = 0.003500, test loss = 0.003458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10500 - loss = 0.001738, data-driven loss = 0.000000, pinn loss = 0.001738, learning rate = 0.003500, test loss = 0.000397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11000 - loss = 0.001562, data-driven loss = 0.000000, pinn loss = 0.001562, learning rate = 0.003500, test loss = 0.000176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11500 - loss = 0.003503, data-driven loss = 0.000000, pinn loss = 0.003503, learning rate = 0.003500, test loss = 0.000719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12000 - loss = 0.002016, data-driven loss = 0.000000, pinn loss = 0.002016, learning rate = 0.003500, test loss = 0.000176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12500 - loss = 0.009186, data-driven loss = 0.000000, pinn loss = 0.009186, learning rate = 0.003500, test loss = 0.003088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13000 - loss = 0.001734, data-driven loss = 0.000000, pinn loss = 0.001734, learning rate = 0.003500, test loss = 0.000136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13500 - loss = 0.001057, data-driven loss = 0.000000, pinn loss = 0.001057, learning rate = 0.003500, test loss = 0.000193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14000 - loss = 0.001966, data-driven loss = 0.000000, pinn loss = 0.001966, learning rate = 0.003500, test loss = 0.000709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14500 - loss = 0.001474, data-driven loss = 0.000000, pinn loss = 0.001474, learning rate = 0.003500, test loss = 0.000137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15000 - loss = 0.001107, data-driven loss = 0.000000, pinn loss = 0.001107, learning rate = 0.000350, test loss = 0.000128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15500 - loss = 0.000362, data-driven loss = 0.000000, pinn loss = 0.000362, learning rate = 0.000350, test loss = 0.000024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16000 - loss = 0.000373, data-driven loss = 0.000000, pinn loss = 0.000373, learning rate = 0.000350, test loss = 0.000023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16500 - loss = 0.000353, data-driven loss = 0.000000, pinn loss = 0.000353, learning rate = 0.000350, test loss = 0.000023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17000 - loss = 0.000319, data-driven loss = 0.000000, pinn loss = 0.000319, learning rate = 0.000350, test loss = 0.000022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17500 - loss = 0.000370, data-driven loss = 0.000000, pinn loss = 0.000370, learning rate = 0.000350, test loss = 0.000022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18000 - loss = 0.000370, data-driven loss = 0.000000, pinn loss = 0.000370, learning rate = 0.000350, test loss = 0.000022\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 64 # Batch size\n",
    "neval_t = 256  # Number of time points at which latent output field is evaluated for a given input source field sample\n",
    "neval_x = 256 \n",
    "# neval_loc = neval_x  # Number of locations at which output field is evaluated at each time point.\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x}  # Number of collocation points within the domain.\n",
    "neval_b = {'t': neval_t, 'loc': 1}        # Number of collocation points on each boundary.\n",
    "neval_i = {'t': 1, 'loc': neval_x}        # Number of collocation points at t=0.\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3.5*1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "test_iteration_list, test_loss_list = [], []\n",
    "\n",
    "n_iterations = 50000\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "    else:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        inputs_train_used_batch = inputs_train_used[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used[indices_datadriven[0:bs]]\n",
    "        #print(\"Shape of inputs_train_used_batch:\", inputs_train_used_batch.shape) # (bs, nx)\n",
    "        #print(\"Shape of outputs_train_used_batch:\", outputs_train_used_batch.shape) # (bs, nt, nx)\n",
    "\n",
    "        _, reconstruction_predicted_values = model(inputs_train_used_batch, t_span.reshape(-1, 1), x_span.reshape(-1, 1)) # (bs, nt, latent_dim), (bs, nt, nx)\n",
    "        reconstruction_target_values = outputs_train_used_batch # (bs, nt, nx)\n",
    "        datadriven_loss = nn.MSELoss()(reconstruction_predicted_values, reconstruction_target_values)\n",
    "        # print('*********')\n",
    "    \n",
    "    indices_pinn = torch.randperm(len(inputs_train)).to(device) # Generate random permutation of indices\n",
    "    inputs_batch = inputs_train[indices_pinn[0:bs]]\n",
    "    #print(f\"Shape of inputs_batch:\", inputs_batch.shape) # (bs, nx)\n",
    "    \n",
    "    # points within the domain\n",
    "    tc = td.uniform.Uniform(0., 1.).sample((neval_c['t'], 1)).to(device)\n",
    "    xc = td.uniform.Uniform(0., 1.).sample((neval_c['loc'], 1)).to(device)\n",
    "\n",
    "    # boundary points on the 2 boundaries (hard-coded)\n",
    "    tb = [td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device),\n",
    "          td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device)]\n",
    "    xb = [torch.tensor([[0.]]).to(device), \n",
    "          torch.tensor([[1.]]).to(device)]\n",
    "\n",
    "    # initial points\n",
    "    ti = torch.zeros((1, 1)).to(device)\n",
    "    xi = td.uniform.Uniform(0., 1.).sample((neval_i['loc'], 1)).to(device)\n",
    "\n",
    "    pinn_loss = (loss_pde_residual(model, inputs_batch, tc, xc) \n",
    "               + loss_pde_bcs(model, inputs_batch, tb, xb) \n",
    "               + loss_pde_ic(model, inputs_batch, ti, xi))\n",
    "    # print('*********')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if iteration % 500 == 0:\n",
    "        # Test loss calculation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_predicted_values = model(inputs_test, \n",
    "                              t_span.reshape(-1, 1), \n",
    "                              x_span.reshape(-1, 1))[1]  # (bs, neval) = (bs, nt, nx)\n",
    "            test_loss = nn.MSELoss()(test_predicted_values, outputs_test.reshape(-1, nt, nx))\n",
    "            test_iteration_list.append(iteration)\n",
    "            test_loss_list.append(test_loss.item())  \n",
    "        model.train()  # Switch back to training mode\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f,' % optimizer.state_dict()['param_groups'][0]['lr'], \n",
    "              'test loss = %f' % test_loss)\n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    \n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "    np.save(os.path.join(resultdir,'test_iteration_list.npy'), np.asarray(test_iteration_list))\n",
    "    np.save(os.path.join(resultdir, 'test_loss_list.npy'), np.asarray(test_loss_list)) \n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "    \n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save) \n",
    "\n",
    "plot_testing_loss(resultdir, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_training_testing_loss(resultdir, iteration_list, loss_list, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)  \n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d2881",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir,'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c742e64",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "latent_branch_inputs = inputs_test # (bs, m) = (bs, nx) \n",
    "latent_trunk_inputs = t_span.reshape(-1, 1) # (nt, 1)\n",
    "reconstruction_trunk_inputs = x_span.reshape(-1, 1) # (nx, 1)\n",
    "_, reconstruction_predictions_test = model(latent_branch_inputs, latent_trunk_inputs, reconstruction_trunk_inputs)# (bs, nt, latent_dim), (bs, nt, nx)\n",
    "# print(reconstruction_predictions_test.shape)\n",
    "\n",
    "mse_list, r2score_list, relerror_list = [], [], [] \n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "\n",
    "    reconstruction_prediction_i = reconstruction_predictions_test[i].unsqueeze(0) # (1, nt, nx)\n",
    "    reconstruction_target_i = outputs_test[i].unsqueeze(0) # (1, nt, nx)\n",
    "    \n",
    "    reconstruction_mse_i = F.mse_loss(reconstruction_prediction_i.cpu(), reconstruction_target_i.cpu())\n",
    "    mse_i = reconstruction_mse_i\n",
    "    \n",
    "    r2score_i = metrics.r2_score(reconstruction_target_i.flatten().cpu().detach().numpy(), reconstruction_prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy() - reconstruction_prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy())\n",
    "\n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item())\n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        plot_predictions(i, resultdir, reconstruction_target_i, reconstruction_prediction_i, x_span, inputs_test, X, T, nt, nx, r'$s(x)$', 'Source field', 'seismic', save)\n",
    "        \n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61651e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": reconstruction_predictions_test.cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6d8df",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2053e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3a068",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "b_Latent-NO.ipynb",
   "output_path": "results/b_Latent-NO/seed=0_n_used=0/output_seed=0_n_used=0.ipynb",
   "parameters": {
    "n_used": 0,
    "save": true,
    "seed": 0
   },
   "start_time": "2025-03-09T06:47:53.864047",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}