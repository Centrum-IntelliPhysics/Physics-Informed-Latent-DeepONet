{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08a75ad",
   "metadata": {
    "papermill": {
     "duration": 0.002966,
     "end_time": "2024-07-29T11:45:35.301387",
     "exception": false,
     "start_time": "2024-07-29T11:45:35.298421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1395ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:45:35.307583Z",
     "iopub.status.busy": "2024-07-29T11:45:35.307339Z",
     "iopub.status.idle": "2024-07-29T11:45:37.436067Z",
     "shell.execute_reply": "2024-07-29T11:45:37.435650Z"
    },
    "papermill": {
     "duration": 2.13292,
     "end_time": "2024-07-29T11:45:37.437193",
     "exception": false,
     "start_time": "2024-07-29T11:45:35.304273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from utils.networks import *\n",
    "from utils.deeponet_networks_1d import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.visualizer_1d import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46ed1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:45:37.443379Z",
     "iopub.status.busy": "2024-07-29T11:45:37.443097Z",
     "iopub.status.idle": "2024-07-29T11:45:37.445342Z",
     "shell.execute_reply": "2024-07-29T11:45:37.445028Z"
    },
    "papermill": {
     "duration": 0.006134,
     "end_time": "2024-07-29T11:45:37.446107",
     "exception": false,
     "start_time": "2024-07-29T11:45:37.439973",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Tag this cell with 'parameters'\n",
    "# parameters\n",
    "seed = 0 # Seed number.\n",
    "n_used = 200 # Number of full training fields used for estimating the data-driven loss term in the PI-Latent-NO\n",
    "save = True # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:45:37.459481Z",
     "iopub.status.busy": "2024-07-29T11:45:37.459269Z",
     "iopub.status.idle": "2024-07-29T11:45:37.461668Z",
     "shell.execute_reply": "2024-07-29T11:45:37.461364Z"
    },
    "papermill": {
     "duration": 0.005977,
     "end_time": "2024-07-29T11:45:37.462403",
     "exception": false,
     "start_time": "2024-07-29T11:45:37.456426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','c_PI-Latent-NO_with-AE','seed='+str(seed)+'_n_used='+str(n_used)) \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7126e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:45:37.467925Z",
     "iopub.status.busy": "2024-07-29T11:45:37.467719Z",
     "iopub.status.idle": "2024-07-29T11:45:37.470421Z",
     "shell.execute_reply": "2024-07-29T11:45:37.470105Z"
    },
    "papermill": {
     "duration": 0.006298,
     "end_time": "2024-07-29T11:45:37.471192",
     "exception": false,
     "start_time": "2024-07-29T11:45:37.464894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d11f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:45:37.476884Z",
     "iopub.status.busy": "2024-07-29T11:45:37.476671Z",
     "iopub.status.idle": "2024-07-29T11:45:37.516198Z",
     "shell.execute_reply": "2024-07-29T11:45:37.515847Z"
    },
    "papermill": {
     "duration": 0.043276,
     "end_time": "2024-07-29T11:45:37.516992",
     "exception": false,
     "start_time": "2024-07-29T11:45:37.473716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346e7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:45:37.523167Z",
     "iopub.status.busy": "2024-07-29T11:45:37.522880Z",
     "iopub.status.idle": "2024-07-29T11:45:37.679754Z",
     "shell.execute_reply": "2024-07-29T11:45:37.679389Z"
    },
    "papermill": {
     "duration": 0.160771,
     "end_time": "2024-07-29T11:45:37.680564",
     "exception": false,
     "start_time": "2024-07-29T11:45:37.519793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = np.load(os.path.join('..','..','data/Diffusion-reaction_dynamics_t=0to1/Diffusion-reaction_dynamics.npz')) # Load the .npz file\n",
    "print(data)\n",
    "print(data['t_span'].shape)\n",
    "print(data['x_span'].shape)\n",
    "print(data['input_s_samples'].shape) # Random Source fields: Gaussian random fields, Nsamples x 100, each sample is (1 x 100)\n",
    "print(data['output_u_samples'].shape) # Time evolution of the solution field: Nsamples x 101 x 100.\n",
    "                               # Each field is 101 x 100, rows correspond to time and columns respond to location.\n",
    "                               # First row corresponds to solution at t=0 (1st time step)\n",
    "                               # and next  row corresponds to solution at t=0.01 (2nd time step) and so on.\n",
    "                               # last row correspond to solution at t=1 (101th time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18da80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:45:37.687247Z",
     "iopub.status.busy": "2024-07-29T11:45:37.686944Z",
     "iopub.status.idle": "2024-07-29T11:45:39.302839Z",
     "shell.execute_reply": "2024-07-29T11:45:39.302507Z"
    },
    "papermill": {
     "duration": 1.620165,
     "end_time": "2024-07-29T11:45:39.303806",
     "exception": false,
     "start_time": "2024-07-29T11:45:37.683641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "inputs = torch.from_numpy(data['input_s_samples']).float().to(device)\n",
    "outputs = torch.from_numpy(data['output_u_samples']).float().to(device)\n",
    "\n",
    "t_span = torch.from_numpy(data['t_span']).float().to(device)\n",
    "x_span = torch.from_numpy(data['x_span']).float().to(device)\n",
    "nt, nx = len(t_span), len(x_span) # number of discretizations in time and location.\n",
    "print(\"nt =\",nt, \", nx =\",nx)\n",
    "print(\"Shape of t-span and x-span:\",t_span.shape, x_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "\n",
    "# Estimating grid points\n",
    "T, X = torch.meshgrid(t_span, x_span)\n",
    "# print(T)\n",
    "# print(X)\n",
    "\n",
    "# Split the data into training and testing samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=500, random_state=seed)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca370b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:45:39.310731Z",
     "iopub.status.busy": "2024-07-29T11:45:39.310525Z",
     "iopub.status.idle": "2024-07-29T11:45:39.314020Z",
     "shell.execute_reply": "2024-07-29T11:45:39.313686Z"
    },
    "papermill": {
     "duration": 0.007884,
     "end_time": "2024-07-29T11:45:39.314889",
     "exception": false,
     "start_time": "2024-07-29T11:45:39.307005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term in the PI-Latent-NO\n",
    "inputs_train_used = inputs_train[:n_used, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0496130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:45:39.321499Z",
     "iopub.status.busy": "2024-07-29T11:45:39.321338Z",
     "iopub.status.idle": "2024-07-29T11:53:35.094597Z",
     "shell.execute_reply": "2024-07-29T11:53:35.094232Z"
    },
    "papermill": {
     "duration": 475.777601,
     "end_time": "2024-07-29T11:53:35.095410",
     "exception": false,
     "start_time": "2024-07-29T11:45:39.317809",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_dim = 9 # d_z  \n",
    "\n",
    "if n_used > 0:\n",
    "    # Learning latent fields using Autoencoder\n",
    "\n",
    "    class Autoencoder_MLP(nn.Module):\n",
    "\n",
    "        def __init__(self, encoder_net, decoder_net):\n",
    "            super().__init__()\n",
    "\n",
    "            self.encoder_net = encoder_net\n",
    "            self.decoder_net = decoder_net\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            encoded = self.encoder_net(x)\n",
    "            decoded = self.decoder_net(encoded)\n",
    "\n",
    "            return decoded\n",
    "\n",
    "\n",
    "    input_dim = nx\n",
    "\n",
    "    encoder_net = DenseNet(layersizes=[input_dim] + [80, 60, 40] + [latent_dim], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "    encoder_net.to(device)\n",
    "    # print(encoder_net)\n",
    "    # print('ENCODER-NET SUMMARY:')\n",
    "    # summary(encoder_net, input_size=(input_dim,))  \n",
    "    # print('#'*100)\n",
    "\n",
    "    decoder_net = DenseNet(layersizes=[latent_dim] + [40, 60, 80] + [input_dim], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "    decoder_net.to(device)\n",
    "    # print(decoder_net)\n",
    "    # print('DECODER-NET SUMMARY:')\n",
    "    # summary(decoder_net, input_size=(latent_dim,))\n",
    "    # print('#'*100)\n",
    "\n",
    "    model_AE = Autoencoder_MLP(encoder_net, decoder_net)\n",
    "    model_AE.to(device);\n",
    "\n",
    "    if save == True:\n",
    "        resultdir_ = os.path.join(resultdir, 'pretrained-AE_latent_dim='+str(latent_dim)) \n",
    "        if not os.path.exists(resultdir_):\n",
    "            os.makedirs(resultdir_)\n",
    "    else:\n",
    "        resultdir_ = None\n",
    "\n",
    "    data_used_for_AE = outputs_train_used.reshape(-1, nx)\n",
    "    print(\"Shape of data_used_for_AE:\", data_used_for_AE.shape)\n",
    "\n",
    "    n_iterations_AE = 60000\n",
    "    data_train = data_used_for_AE\n",
    "\n",
    "    print(colored('LATENT DIMENSION = '+str(latent_dim), 'red'))\n",
    "    print (\"------STARTED-TRAINING------\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    bs = 512 # Batch size\n",
    "\n",
    "    # Training\n",
    "    optimizer = torch.optim.Adam(model_AE.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "    iteration_list, loss_list, learningrates_list = [], [], []\n",
    "\n",
    "    for iteration in range(n_iterations_AE):\n",
    "\n",
    "        num_samples = len(data_train)\n",
    "        indices = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "        data_batch = data_train[indices[0:bs]]\n",
    "        #print(f\"Shape of data_train_batch[{i}]:\", data_batch.shape) # (bs, nx)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_AE(data_batch)\n",
    "        loss = nn.MSELoss()(outputs, data_batch)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_value_(model_AE.parameters(), clip_value=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "                  'learning rate = %f' % optimizer.state_dict()['param_groups'][0]['lr']) \n",
    "\n",
    "        iteration_list.append(iteration)\n",
    "        loss_list.append(loss.item())\n",
    "        learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    print (\"------ENDED-TRAINING------\")\n",
    "\n",
    "    if save == True:\n",
    "        np.save(os.path.join(resultdir_,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "        np.save(os.path.join(resultdir_,'loss_list.npy'), np.asarray(loss_list))\n",
    "        np.save(os.path.join(resultdir_,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "\n",
    "    plot_training_loss(resultdir_, iteration_list, loss_list, save)\n",
    "\n",
    "    plot_learningrates(resultdir_, iteration_list, learningrates_list, save)\n",
    "\n",
    "    # end timer\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time # time for AE network to train\n",
    "    print(\"Time (sec) to complete:\\n\" +str(training_time)) # time for AE network to train\n",
    "    print('*'*10)\n",
    "\n",
    "    if save == True:\n",
    "        torch.save(model_AE.state_dict(), os.path.join(resultdir_,'model_state_dict.pt'))\n",
    "\n",
    "    # Evaluate on test data\n",
    "    data_test = outputs_test.reshape(-1, nx) #(test_size*nt, nx)\n",
    "    outputs = model_AE(data_test) #(test_size*nt, nx)\n",
    "    reconstruction_loss_test = nn.MSELoss()(outputs, data_test)\n",
    "    print(f\"TEST DATA RECONSTRUCTION ERROR FOR LATENT DIMENSION {latent_dim}: {reconstruction_loss_test.item():.2e}\")\n",
    "    print('*'*10)\n",
    "\n",
    "    for i in range(outputs_test.shape[0]):\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            data_i = outputs_test[i] #(nt, nx)\n",
    "            reconstructed_i = model_AE(data_i) #(nt, nx)\n",
    "\n",
    "            plot_AE_reconstructions(i, resultdir_, data_i, reconstructed_i, X, T, 'seismic', save)\n",
    "\n",
    "    print(colored('*'*115, 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7fd61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:53:35.118072Z",
     "iopub.status.busy": "2024-07-29T11:53:35.117777Z",
     "iopub.status.idle": "2024-07-29T11:53:35.574270Z",
     "shell.execute_reply": "2024-07-29T11:53:35.573915Z"
    },
    "papermill": {
     "duration": 0.468253,
     "end_time": "2024-07-29T11:53:35.575046",
     "exception": false,
     "start_time": "2024-07-29T11:53:35.106793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if n_used > 0:\n",
    "    # Load the pretrained autoencoder\n",
    "    pretrained_autoencoder_model = Autoencoder_MLP(encoder_net, decoder_net)\n",
    "    pretrained_autoencoder_model.to(device) \n",
    "    if save == True:\n",
    "        pretrained_autoencoder_model.load_state_dict(torch.load(os.path.join(resultdir,\n",
    "                                                                         'pretrained-AE_latent_dim='+str(latent_dim),\n",
    "                                                                         'model_state_dict.pt'), map_location=device))\n",
    "    if save == False:\n",
    "        pretrained_autoencoder_model.load_state_dict(model_AE.state_dict())\n",
    "\n",
    "    # Evaluating latent data and detach autoencoder (detaching is important, otherwise graph will be retained and takes lot of time to train)\n",
    "    latent_outputs_train = torch.zeros((outputs_train.shape[0], outputs_train.shape[1], latent_dim)).to(device)\n",
    "    latent_outputs_train_used = torch.zeros((outputs_train_used.shape[0], outputs_train_used.shape[1], latent_dim)).to(device)\n",
    "    latent_outputs_test = torch.zeros((outputs_test.shape[0], outputs_test.shape[1], latent_dim)).to(device)\n",
    "    for i in range(latent_outputs_train.shape[0]):\n",
    "        latent_outputs_train[i] = pretrained_autoencoder_model.encoder_net(outputs_train[i]).detach()\n",
    "    for i in range(latent_outputs_train_used.shape[0]):\n",
    "        latent_outputs_train_used[i] = pretrained_autoencoder_model.encoder_net(outputs_train_used[i]).detach()\n",
    "    for i in range(latent_outputs_test.shape[0]):\n",
    "        latent_outputs_test[i] = pretrained_autoencoder_model.encoder_net(outputs_test[i]).detach()\n",
    "\n",
    "    # Check the shapes of the subsets\n",
    "    print(\"Shape of latent_outputs_train:\", latent_outputs_train.shape)\n",
    "    print(\"Shape of latent_outputs_train_used:\", latent_outputs_train_used.shape)\n",
    "    print(\"Shape of latent_outputs_test:\", latent_outputs_test.shape)\n",
    "    print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5719c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:53:35.597526Z",
     "iopub.status.busy": "2024-07-29T11:53:35.597241Z",
     "iopub.status.idle": "2024-07-29T11:53:35.610319Z",
     "shell.execute_reply": "2024-07-29T11:53:35.609985Z"
    },
    "papermill": {
     "duration": 0.024917,
     "end_time": "2024-07-29T11:53:35.611072",
     "exception": false,
     "start_time": "2024-07-29T11:53:35.586155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_neurons_latent_branch: Number of input neurons in the latent_branch net.\n",
    "input_neurons_latent_trunk: Number of input neurons in the latent_trunk net.\n",
    "latent_p: Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\"\"\"\n",
    "latent_p = latent_dim*16 # Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\n",
    "input_neurons_latent_branch = nx # m\n",
    "latent_branch_net = DenseNet(layersizes=[input_neurons_latent_branch] + [64]*3 + [latent_p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_branch_net.to(device)\n",
    "# print(latent_branch_net)\n",
    "print('LATENT BRANCH-NET SUMMARY:')\n",
    "summary(latent_branch_net, input_size=(input_neurons_latent_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_latent_trunk = 1 # 1 corresponds to t\n",
    "latent_trunk_net = DenseNet(layersizes=[input_neurons_latent_trunk] + [64]*3 + [latent_p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_trunk_net.to(device)\n",
    "# print(latent_trunk_net)\n",
    "print('LATENT TRUNK-NET SUMMARY:')\n",
    "summary(latent_trunk_net, input_size=(input_neurons_latent_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "\"\"\"\n",
    "input_neurons_reconstruction_branch: Number of input neurons in the reconstruction_branch net.\n",
    "input_neurons_reconstruction_trunk: Number of input neurons in the reconstruction_trunk net.\n",
    "reconstruction_q: Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\"\"\"\n",
    "reconstruction_q = 128 # Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\n",
    "input_neurons_reconstruction_branch = latent_dim # d_z\n",
    "reconstruction_branch_net = DenseNet(layersizes=[input_neurons_reconstruction_branch] + [64]*3 + [reconstruction_q], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_branch_net.to(device)\n",
    "# print(reconstruction_branch_net)\n",
    "print('RECONSTRUCTION BRANCH-NET SUMMARY:')\n",
    "summary(reconstruction_branch_net, input_size=(input_neurons_reconstruction_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_reconstruction_trunk = 1 # 1 corresponds to x\n",
    "reconstruction_trunk_net = DenseNet(layersizes=[input_neurons_reconstruction_trunk] + [64]*3 + [reconstruction_q], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_trunk_net.to(device)\n",
    "# print(reconstruction_trunk_net)\n",
    "print('RECONSTRUCTION TRUNK-NET SUMMARY:')\n",
    "summary(reconstruction_trunk_net, input_size=(input_neurons_reconstruction_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Latent_NO_model(latent_branch_net, latent_trunk_net, latent_dim, reconstruction_branch_net, reconstruction_trunk_net)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86a175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:53:35.632693Z",
     "iopub.status.busy": "2024-07-29T11:53:35.632470Z",
     "iopub.status.idle": "2024-07-29T11:53:35.634916Z",
     "shell.execute_reply": "2024-07-29T11:53:35.634603Z"
    },
    "papermill": {
     "duration": 0.014108,
     "end_time": "2024-07-29T11:53:35.635653",
     "exception": false,
     "start_time": "2024-07-29T11:53:35.621545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_learnable_parameters = (count_learnable_parameters(latent_branch_net)\n",
    "                            + count_learnable_parameters(latent_trunk_net)\n",
    "                            + count_learnable_parameters(reconstruction_branch_net)\n",
    "                            + count_learnable_parameters(reconstruction_trunk_net))\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7510cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:53:35.657257Z",
     "iopub.status.busy": "2024-07-29T11:53:35.657042Z",
     "iopub.status.idle": "2024-07-29T11:53:35.660875Z",
     "shell.execute_reply": "2024-07-29T11:53:35.660570Z"
    },
    "papermill": {
     "duration": 0.015463,
     "end_time": "2024-07-29T11:53:35.661599",
     "exception": false,
     "start_time": "2024-07-29T11:53:35.646136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, source_fields, t, x):\n",
    "    \n",
    "    _, u = net(source_fields, t, x) # u is (bs, neval_t, neval_x)\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device)\n",
    "    ut  = FWDAD_first_order_derivative(lambda t: net(source_fields, t, x)[1], t, tangent_t) # (bs, neval_t, neval_x)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: net(source_fields, t, x)[1], x, tangent_x) # (bs, neval_t, neval_x)\n",
    "    \n",
    "    bs_ = source_fields.shape[0]\n",
    "    sf_values_ = torch.zeros((bs_, x.shape[0], 1)).to(device)\n",
    "    for j in range(bs_):\n",
    "        sf_values_[j] = linear_interpolation(x, x_span, source_fields[j]) # source function: s(x) values\n",
    "    sf_values__ = sf_values_.reshape(-1, 1, x.shape[0]) # (bs, 1, neval_x)\n",
    "    # Repeat elements along neval_x for neval_t times and reshape\n",
    "    sf_values = sf_values__.repeat(1, neval_t, 1) # (bs, neval_t, neval_x) # s(x) values are same for all times\n",
    "    \n",
    "    pde_residual = (ut - (0.01*uxx) - (0.01*(u**2)) - sf_values)**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0afa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:53:35.683199Z",
     "iopub.status.busy": "2024-07-29T11:53:35.682991Z",
     "iopub.status.idle": "2024-07-29T11:53:35.685718Z",
     "shell.execute_reply": "2024-07-29T11:53:35.685416Z"
    },
    "papermill": {
     "duration": 0.014396,
     "end_time": "2024-07-29T11:53:35.686438",
     "exception": false,
     "start_time": "2024-07-29T11:53:35.672042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_bcs(net, source_fields, t, x):\n",
    "    \n",
    "    t_b1, x_b1 = t[0], x[0]\n",
    "    t_b2, x_b2 = t[1], x[1]\n",
    "\n",
    "    _, u_b1 = net(source_fields, t_b1, x_b1) # u is (bs, neval_t, 1)\n",
    "    _, u_b2 = net(source_fields, t_b2, x_b2) # u is (bs, neval_t, 1)\n",
    "\n",
    "    bc1_value, bc2_value = 0., 0.\n",
    "    pde_bc1 = (u_b1 - bc1_value)**2\n",
    "    pde_bc2 = (u_b2 - bc2_value)**2\n",
    "    \n",
    "    return torch.mean(pde_bc1) + torch.mean(pde_bc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48197e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:53:35.708027Z",
     "iopub.status.busy": "2024-07-29T11:53:35.707823Z",
     "iopub.status.idle": "2024-07-29T11:53:35.710010Z",
     "shell.execute_reply": "2024-07-29T11:53:35.709710Z"
    },
    "papermill": {
     "duration": 0.013734,
     "end_time": "2024-07-29T11:53:35.710738",
     "exception": false,
     "start_time": "2024-07-29T11:53:35.697004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_ic(net, source_fields, t, x):\n",
    "\n",
    "    _, u_ic = net(source_fields, t, x) # u is (bs, 1, neval_x)\n",
    "    \n",
    "    ic_value = 0.\n",
    "    pde_ic = (u_ic - ic_value)**2\n",
    "    \n",
    "    return torch.mean(pde_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e40852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:53:35.732564Z",
     "iopub.status.busy": "2024-07-29T11:53:35.732398Z",
     "iopub.status.idle": "2024-07-29T14:35:32.570012Z",
     "shell.execute_reply": "2024-07-29T14:35:32.569679Z"
    },
    "papermill": {
     "duration": 9716.849559,
     "end_time": "2024-07-29T14:35:32.570898",
     "exception": false,
     "start_time": "2024-07-29T11:53:35.721339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 64 # Batch size\n",
    "neval_t = 16  # Number of time points at which latent output field is evaluated for a given input source field sample\n",
    "neval_x = 16 \n",
    "# neval_loc = neval_x  # Number of locations at which output field is evaluated at each time point.\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x}  # Number of collocation points within the domain.\n",
    "neval_b = {'t': neval_t, 'loc': 1}        # Number of collocation points on each boundary.\n",
    "neval_i = {'t': 1, 'loc': neval_x}        # Number of collocation points at t=0.\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "\n",
    "n_iterations = 50000\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used > 0:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        inputs_train_used_batch = inputs_train_used[indices_datadriven[0:bs]]\n",
    "        latent_outputs_train_used_batch = latent_outputs_train_used[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used[indices_datadriven[0:bs]]\n",
    "        #print(\"Shape of inputs_train_used_batch:\", inputs_train_used_batch.shape) # (bs, nx)\n",
    "        #print(\"Shape of latent_outputs_train_used_batch:\", latent_outputs_train_used_batch.shape) # (bs, nt, latent_dim)\n",
    "        #print(\"Shape of outputs_train_used_batch:\", outputs_train_used_batch.shape) # (bs, nt, nx)\n",
    "\n",
    "        latent_predicted_values, reconstruction_predicted_values = model(inputs_train_used_batch, t_span.reshape(-1, 1), x_span.reshape(-1, 1)) # (bs, nt, latent_dim), (bs, nt, nx)\n",
    "        latent_target_values = latent_outputs_train_used_batch # (bs, nt, latent_dim)\n",
    "        reconstruction_target_values = outputs_train_used_batch # (bs, nt, nx)\n",
    "        datadriven_loss = nn.MSELoss()(latent_predicted_values, latent_target_values) + nn.MSELoss()(reconstruction_predicted_values, reconstruction_target_values)\n",
    "        # print('*********')\n",
    "    elif n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "    \n",
    "    indices_pinn = torch.randperm(len(inputs_train)).to(device) # Generate random permutation of indices\n",
    "    inputs_batch = inputs_train[indices_pinn[0:bs]]\n",
    "    #print(f\"Shape of inputs_train_batch:\", inputs_batch.shape) # (bs, nx)\n",
    "    \n",
    "    # points within the domain\n",
    "    tc = td.uniform.Uniform(0., 1.).sample((neval_c['t'], 1)).to(device)\n",
    "    xc = td.uniform.Uniform(0., 1.).sample((neval_c['loc'], 1)).to(device)\n",
    "\n",
    "    # boundary points on the 2 boundaries (hard-coded)\n",
    "    tb = [td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device),\n",
    "          td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device)]\n",
    "    xb = [torch.tensor([[0.]]).to(device), \n",
    "          torch.tensor([[1.]]).to(device)]\n",
    "\n",
    "    # initial points\n",
    "    ti = torch.zeros((1, 1)).to(device)\n",
    "    xi = td.uniform.Uniform(0., 1.).sample((neval_i['loc'], 1)).to(device)\n",
    "\n",
    "    pinn_loss = (loss_pde_residual(model, inputs_batch, tc, xc) \n",
    "               + loss_pde_bcs(model, inputs_batch, tb, xb) \n",
    "               + loss_pde_ic(model, inputs_batch, ti, xi))\n",
    "    # print('*********')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if iteration % 500 == 0:\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f' % optimizer.state_dict()['param_groups'][0]['lr']) \n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    \n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "    \n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save)  \n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)  \n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d2881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T14:35:32.600487Z",
     "iopub.status.busy": "2024-07-29T14:35:32.600282Z",
     "iopub.status.idle": "2024-07-29T14:35:32.605743Z",
     "shell.execute_reply": "2024-07-29T14:35:32.605431Z"
    },
    "papermill": {
     "duration": 0.020927,
     "end_time": "2024-07-29T14:35:32.606476",
     "exception": false,
     "start_time": "2024-07-29T14:35:32.585549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir,'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c742e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T14:35:32.634610Z",
     "iopub.status.busy": "2024-07-29T14:35:32.634388Z",
     "iopub.status.idle": "2024-07-29T14:36:26.230458Z",
     "shell.execute_reply": "2024-07-29T14:36:26.230094Z"
    },
    "papermill": {
     "duration": 53.611685,
     "end_time": "2024-07-29T14:36:26.231848",
     "exception": false,
     "start_time": "2024-07-29T14:35:32.620163",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "latent_branch_inputs = inputs_test # (bs, m) = (bs, nx) \n",
    "latent_trunk_inputs = t_span.reshape(-1, 1) # (nt, 1)\n",
    "reconstruction_trunk_inputs = x_span.reshape(-1, 1) # (nx, 1)\n",
    "latent_predictions_test, reconstruction_predictions_test = model(latent_branch_inputs, latent_trunk_inputs, reconstruction_trunk_inputs)# (bs, nt, latent_dim), (bs, nt, nx)\n",
    "# print(latent_predictions_test.shape, reconstruction_predictions_test.shape)\n",
    "\n",
    "mse_list, latent_mse_list, reconstruction_mse_list, r2score_list, relerror_list = [], [], [], [], []\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "\n",
    "    latent_prediction_i, reconstruction_prediction_i = latent_predictions_test[i].unsqueeze(0), reconstruction_predictions_test[i].unsqueeze(0)# (1, nt, latent_dim), (1, nt, nx)\n",
    "    \n",
    "    if n_used > 0:\n",
    "        latent_target_i = latent_outputs_test[i].unsqueeze(0) # (1, nt, latent_dim)\n",
    "    reconstruction_target_i = outputs_test[i].unsqueeze(0) # (1, nt, nx)\n",
    "    \n",
    "    if n_used > 0:\n",
    "        latent_mse_i = F.mse_loss(latent_prediction_i.cpu(), latent_target_i.cpu())\n",
    "    elif n_used == 0:\n",
    "        latent_mse_i = torch.tensor([0.])\n",
    "    reconstruction_mse_i = F.mse_loss(reconstruction_prediction_i.cpu(), reconstruction_target_i.cpu())\n",
    "    mse_i = latent_mse_i + reconstruction_mse_i\n",
    "    \n",
    "    r2score_i = metrics.r2_score(reconstruction_target_i.flatten().cpu().detach().numpy(), reconstruction_prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy() - reconstruction_prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy())\n",
    "        \n",
    "    latent_mse_list.append(latent_mse_i.item())\n",
    "    reconstruction_mse_list.append(reconstruction_mse_i.item())\n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item())\n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        plot_predictions(i, resultdir, reconstruction_target_i, reconstruction_prediction_i, x_span, inputs_test, X, T, nt, nx, r'$s(x)$', 'Source field', 'seismic', save)\n",
    "        \n",
    "latent_mse = sum(latent_mse_list) / len(latent_mse_list)\n",
    "print(\"Latent Mean Squared Error Test:\\n\", latent_mse)\n",
    "reconstruction_mse = sum(reconstruction_mse_list) / len(reconstruction_mse_list)\n",
    "print(\"Reconstruction Mean Squared Error Test:\\n\", reconstruction_mse)\n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d3413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T14:36:26.369155Z",
     "iopub.status.busy": "2024-07-29T14:36:26.368822Z",
     "iopub.status.idle": "2024-07-29T14:36:35.423441Z",
     "shell.execute_reply": "2024-07-29T14:36:35.423093Z"
    },
    "papermill": {
     "duration": 9.122918,
     "end_time": "2024-07-29T14:36:35.424346",
     "exception": false,
     "start_time": "2024-07-29T14:36:26.301428",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if n_used > 0:\n",
    "    # Plotting learned latent fields\n",
    "\n",
    "    z_span = torch.arange(1, latent_dim+1, 1).float().to(device)\n",
    "    # Estimating grid points\n",
    "    T_z, Z = torch.meshgrid(t_span, z_span)\n",
    "    # print(Z.shape, T_z.shape)\n",
    "    # print(T_z)\n",
    "    # print(Z)\n",
    "\n",
    "    for i in range(inputs_test.shape[0]):\n",
    "\n",
    "        latent_prediction_i = latent_predictions_test[i].unsqueeze(0) # (1, nt, latent_dim)\n",
    "        latent_target_i = latent_outputs_test[i].unsqueeze(0) # (1, nt, latent_dim)\n",
    "        reconstruction_target_i = outputs_test[i].unsqueeze(0) # (1, nt, nx)\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            plot_latentfields(i, resultdir, reconstruction_target_i, latent_target_i, latent_prediction_i, x_span, inputs_test, z_span, X, T, Z, T_z, nt, nx, latent_dim, r'$s(x)$', 'Source field', 'True latent field from AE', 'seismic', save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": reconstruction_predictions_test.cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6d8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T14:36:35.567502Z",
     "iopub.status.busy": "2024-07-29T14:36:35.567268Z",
     "iopub.status.idle": "2024-07-29T14:36:35.570086Z",
     "shell.execute_reply": "2024-07-29T14:36:35.569760Z"
    },
    "papermill": {
     "duration": 0.073383,
     "end_time": "2024-07-29T14:36:35.570839",
     "exception": false,
     "start_time": "2024-07-29T14:36:35.497456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(reconstruction_mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a366414",
   "metadata": {
    "papermill": {
     "duration": 0.070904,
     "end_time": "2024-07-29T14:36:35.710752",
     "exception": false,
     "start_time": "2024-07-29T14:36:35.639848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10262.444657,
   "end_time": "2024-07-29T14:36:36.697100",
   "environment_variables": {},
   "exception": null,
   "input_path": "c_PI-Latent-NO_with-AE.ipynb",
   "output_path": "results/c_PI-Latent-NO_with-AE/seed=0_n_used=200/output_seed=0_n_used=200.ipynb",
   "parameters": {
    "n_used": 200,
    "seed": 0
   },
   "start_time": "2024-07-29T11:45:34.252443",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
