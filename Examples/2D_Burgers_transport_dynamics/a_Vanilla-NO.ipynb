{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f24266",
   "metadata": {
    "papermill": {
     "duration": 0.003816,
     "end_time": "2025-01-24T06:12:45.901358",
     "exception": false,
     "start_time": "2025-01-24T06:12:45.897542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Comparisons\n",
    "\n",
    "This code can execute the following variants of losses:  \n",
    "\n",
    "1. **Variant 1:**  Purely Physics  \n",
    "   $L_{\\theta} = L_{\\text{PDE}}$  \n",
    "   Use $n_{\\text{used}} = 0$  \n",
    "\n",
    "2. **Variant 2:**  Physics + Data  \n",
    "   $L_{\\theta} = L_{\\text{PDE}} + \\Sigma_{i=1}^{n_{\\text{used}}}\\| u_i - \\hat{u}_i \\|_2^2$  \n",
    "   Use $n_{\\text{used}} \\in (0, 300]$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224272da",
   "metadata": {
    "papermill": {
     "duration": 0.002691,
     "end_time": "2025-01-24T06:12:45.907009",
     "exception": false,
     "start_time": "2025-01-24T06:12:45.904318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1395ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:45.913201Z",
     "iopub.status.busy": "2025-01-24T06:12:45.912910Z",
     "iopub.status.idle": "2025-01-24T06:12:48.625457Z",
     "shell.execute_reply": "2025-01-24T06:12:48.624860Z"
    },
    "papermill": {
     "duration": 2.716881,
     "end_time": "2025-01-24T06:12:48.626643",
     "exception": false,
     "start_time": "2025-01-24T06:12:45.909762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn import metrics\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from utils.networks import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "from utils.deeponet_networks_2d import *\n",
    "from utils.visualizer_2d import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e21cdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:48.633366Z",
     "iopub.status.busy": "2025-01-24T06:12:48.633099Z",
     "iopub.status.idle": "2025-01-24T06:12:48.635810Z",
     "shell.execute_reply": "2025-01-24T06:12:48.635388Z"
    },
    "papermill": {
     "duration": 0.006979,
     "end_time": "2025-01-24T06:12:48.636648",
     "exception": false,
     "start_time": "2025-01-24T06:12:48.629669",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Tag this cell with 'parameters'\n",
    "# parameters\n",
    "seed = 0 # Seed number.\n",
    "n_used = 150 # Number of full training fields used for estimating the data-driven loss term\n",
    "n_iterations = 80000 # Number of iterations.\n",
    "use_fourier_features = True\n",
    "n_fourier = 10 # Number of fourier frequencies considered.\n",
    "save = True # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f9ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:48.651640Z",
     "iopub.status.busy": "2025-01-24T06:12:48.651308Z",
     "iopub.status.idle": "2025-01-24T06:12:48.654079Z",
     "shell.execute_reply": "2025-01-24T06:12:48.653683Z"
    },
    "papermill": {
     "duration": 0.006648,
     "end_time": "2025-01-24T06:12:48.654850",
     "exception": false,
     "start_time": "2025-01-24T06:12:48.648202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','a_Vanilla-NO','seed='+str(seed)+'_n_used='+str(n_used)) \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb4cab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:48.660878Z",
     "iopub.status.busy": "2025-01-24T06:12:48.660700Z",
     "iopub.status.idle": "2025-01-24T06:12:48.674308Z",
     "shell.execute_reply": "2025-01-24T06:12:48.673877Z"
    },
    "papermill": {
     "duration": 0.01757,
     "end_time": "2025-01-24T06:12:48.675126",
     "exception": false,
     "start_time": "2025-01-24T06:12:48.657556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d11f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:48.681456Z",
     "iopub.status.busy": "2025-01-24T06:12:48.681136Z",
     "iopub.status.idle": "2025-01-24T06:12:48.811653Z",
     "shell.execute_reply": "2025-01-24T06:12:48.811180Z"
    },
    "papermill": {
     "duration": 0.134527,
     "end_time": "2025-01-24T06:12:48.812420",
     "exception": false,
     "start_time": "2025-01-24T06:12:48.677893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e8654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:48.819014Z",
     "iopub.status.busy": "2025-01-24T06:12:48.818662Z",
     "iopub.status.idle": "2025-01-24T06:12:48.986123Z",
     "shell.execute_reply": "2025-01-24T06:12:48.985610Z"
    },
    "papermill": {
     "duration": 0.171624,
     "end_time": "2025-01-24T06:12:48.987002",
     "exception": false,
     "start_time": "2025-01-24T06:12:48.815378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = torch.load(os.path.join('..','..','data/2D_Burgers_equation_scalar/Burgers_equation_2D_scalar.pt'))\n",
    "\n",
    "for key, tensor in data.items():\n",
    "    print(f\"{key}: {tensor.shape}\")\n",
    "    \n",
    "# Random Initial conditions: Nsamples x 32 x 32, each IC sample is (32 x 32)\n",
    "# Time evolution of the solution field: Nsamples x 21 x 32 x 32.\n",
    "# Each field is  21 x 32 x 32, rows correspond to time and other dimensions correspond to the field.\n",
    "# First row corresponds to solution at t=0 (1st time step)\n",
    "# and next  row corresponds to solution at t=0.05 (2nd time step) and so on.\n",
    "# last row correspond to solution at t=1 (21st time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0cbee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:48.993973Z",
     "iopub.status.busy": "2025-01-24T06:12:48.993660Z",
     "iopub.status.idle": "2025-01-24T06:12:49.379234Z",
     "shell.execute_reply": "2025-01-24T06:12:49.378723Z"
    },
    "papermill": {
     "duration": 0.389861,
     "end_time": "2025-01-24T06:12:49.380017",
     "exception": false,
     "start_time": "2025-01-24T06:12:48.990156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = data['input_samples'].float().to(device)\n",
    "outputs = data['output_samples'].float().to(device)\n",
    "t_span = data['t_span'].float().to(device)\n",
    "x_span = data['x_span'].float().to(device)\n",
    "y_span = data['y_span'].float().to(device)\n",
    "\n",
    "L = 1.         # Simulation domain [0, L]^2\n",
    "T = 1.         # Simulation time [0, T]\n",
    "\n",
    "nt, nx, ny = len(t_span), len(x_span), len(y_span) # number of discretizations in time, location_x and location_y.\n",
    "\n",
    "grid = torch.vstack((t_span.repeat_interleave(ny*nx), \n",
    "              x_span.flatten().repeat(nt),\n",
    "              y_span.flatten().repeat(nt))).T\n",
    "print(\"Shape of grid:\", grid.shape) # (nt*nx*ny, 3)\n",
    "print(\"grid:\", grid) # time, location_x, location_y\n",
    "\n",
    "# Split the data into training and testing samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=50, random_state=seed)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c383e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:49.386800Z",
     "iopub.status.busy": "2025-01-24T06:12:49.386478Z",
     "iopub.status.idle": "2025-01-24T06:12:49.389548Z",
     "shell.execute_reply": "2025-01-24T06:12:49.389116Z"
    },
    "papermill": {
     "duration": 0.007327,
     "end_time": "2025-01-24T06:12:49.390351",
     "exception": false,
     "start_time": "2025-01-24T06:12:49.383024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term \n",
    "inputs_train_used = inputs_train[:n_used, :, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e7d04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:49.396988Z",
     "iopub.status.busy": "2025-01-24T06:12:49.396760Z",
     "iopub.status.idle": "2025-01-24T06:12:49.515468Z",
     "shell.execute_reply": "2025-01-24T06:12:49.515020Z"
    },
    "papermill": {
     "duration": 0.122982,
     "end_time": "2025-01-24T06:12:49.516284",
     "exception": false,
     "start_time": "2025-01-24T06:12:49.393302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_initial_fields_only = torch.load(os.path.join('..','..','data/2D_Burgers_equation_scalar/Burgers_equation_2D_scalar_initial_fields_only.pt')).to(device)\n",
    "print(inputs_initial_fields_only.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c848919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_features(t, x, y, n_fourier):\n",
    "    pi = math.pi\n",
    "    result = torch.zeros(t.size(0), 3+(6*n_fourier)).to(device)  # Initialize result tensor\n",
    "\n",
    "    t = t.squeeze()\n",
    "    x = x.squeeze()\n",
    "    y = y.squeeze()\n",
    "\n",
    "    # Compute the transformation\n",
    "    result[:, 0] = t\n",
    "    result[:, 1] = x\n",
    "    result[:, 2] = y\n",
    "    for i in range(n_fourier):\n",
    "        result[:, 6*i + 0 + 3] = torch.cos((i + 1) * pi * t)\n",
    "        result[:, 6*i + 1 + 3] = torch.sin((i + 1) * pi * t)\n",
    "        result[:, 6*i + 2 + 3] = torch.cos((i + 1) * pi * x)\n",
    "        result[:, 6*i + 3 + 3] = torch.sin((i + 1) * pi * x)\n",
    "        result[:, 6*i + 4 + 3] = torch.cos((i + 1) * pi * y)\n",
    "        result[:, 6*i + 5 + 3] = torch.sin((i + 1) * pi * y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23987407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_features_spatial(x, y, n_fourier):\n",
    "    pi = math.pi\n",
    "    result = torch.zeros(x.size(0), 2+(4*n_fourier)).to(device)  # Initialize result tensor\n",
    "    \n",
    "    x = x.squeeze()\n",
    "    y = y.squeeze()\n",
    "    \n",
    "    # Compute the transformation\n",
    "    result[:, 0] = x\n",
    "    result[:, 1] = y\n",
    "    for i in range(n_fourier):\n",
    "        result[:, 4*i + 0 + 2] = torch.cos((i + 1) * pi * x)\n",
    "        result[:, 4*i + 1 + 2] = torch.sin((i + 1) * pi * x)\n",
    "        result[:, 4*i + 2 + 2] = torch.cos((i + 1) * pi * y)\n",
    "        result[:, 4*i + 3 + 2] = torch.sin((i + 1) * pi * y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae139a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:49.534591Z",
     "iopub.status.busy": "2025-01-24T06:12:49.534291Z",
     "iopub.status.idle": "2025-01-24T06:12:49.732355Z",
     "shell.execute_reply": "2025-01-24T06:12:49.731903Z"
    },
    "papermill": {
     "duration": 0.202357,
     "end_time": "2025-01-24T06:12:49.733156",
     "exception": false,
     "start_time": "2025-01-24T06:12:49.530799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_neurons_branch: Number of input neurons in the branch net.\n",
    "input_neurons_trunk: Number of input neurons in the trunk net.\n",
    "p: Number of output neurons in both the branch and trunk net.\n",
    "\"\"\"\n",
    "\n",
    "p = 128 # Number of output neurons in both the branch and trunk net.\n",
    "\n",
    "input_neurons_branch = (ny, nx) # Specify input size of image as a tuple (height, width)\n",
    "n_channels = 1\n",
    "num_filters = [20, 30, 40]\n",
    "filter_sizes = [3, 3, 3]\n",
    "strides = [1]*len(num_filters)\n",
    "paddings = [0]*len(num_filters)\n",
    "poolings = [('avg', 2, 2), ('avg', 2, 2), ('avg', 2, 2)]  # Pooling layer specification (type, kernel_size, stride)\n",
    "end_MLP_layersizes = [150, 150, p]\n",
    "activation = nn.SiLU() # nn.ReLU() nn.SiLU() #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "branch_net = ConvNet(input_neurons_branch, n_channels, num_filters, filter_sizes, strides, paddings, poolings, end_MLP_layersizes, activation)\n",
    "branch_net.to(device)\n",
    "# print(branch_net)\n",
    "print('BRANCH-NET SUMMARY:')\n",
    "summary(branch_net, input_size=(n_channels, ny, nx))  # input shape is (channels, height, width)\n",
    "print('#'*100)\n",
    "\n",
    "# 3 corresponds to t, x and y\n",
    "if use_fourier_features == False:\n",
    "    input_neurons_trunk = 3 # Number of input neurons in the trunk net.\n",
    "else:\n",
    "    input_neurons_trunk = 3 + (4*n_fourier) # Number of input neurons in the trunk net.\n",
    "trunk_net = DenseNet(layersizes=[input_neurons_trunk] + [128]*4 + [p], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "trunk_net.to(device)\n",
    "# print(trunk_net)\n",
    "print('TRUNK-NET SUMMARY:')\n",
    "summary(trunk_net, input_size=(input_neurons_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Vanilla_NO_model(branch_net, trunk_net)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e3c67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:49.740433Z",
     "iopub.status.busy": "2025-01-24T06:12:49.740142Z",
     "iopub.status.idle": "2025-01-24T06:12:49.742901Z",
     "shell.execute_reply": "2025-01-24T06:12:49.742478Z"
    },
    "papermill": {
     "duration": 0.007249,
     "end_time": "2025-01-24T06:12:49.743700",
     "exception": false,
     "start_time": "2025-01-24T06:12:49.736451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_learnable_parameters = count_learnable_parameters(branch_net) + count_learnable_parameters(trunk_net)\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33118b66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:49.750893Z",
     "iopub.status.busy": "2025-01-24T06:12:49.750663Z",
     "iopub.status.idle": "2025-01-24T06:12:49.753277Z",
     "shell.execute_reply": "2025-01-24T06:12:49.752880Z"
    },
    "papermill": {
     "duration": 0.007115,
     "end_time": "2025-01-24T06:12:49.754058",
     "exception": false,
     "start_time": "2025-01-24T06:12:49.746943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def u_pred(net, inputs, t, x, y):\n",
    "    if use_fourier_features == False:\n",
    "        u = net(inputs, torch.hstack([t, x, y])) # (bs, neval)\n",
    "    elif use_fourier_features == True:\n",
    "        u = net(inputs,  torch.hstack([t, fourier_features_spatial(x, y, n_fourier)])) # (bs, neval)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f588b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:49.761230Z",
     "iopub.status.busy": "2025-01-24T06:12:49.760933Z",
     "iopub.status.idle": "2025-01-24T06:12:49.764980Z",
     "shell.execute_reply": "2025-01-24T06:12:49.764576Z"
    },
    "papermill": {
     "duration": 0.008454,
     "end_time": "2025-01-24T06:12:49.765749",
     "exception": false,
     "start_time": "2025-01-24T06:12:49.757295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, initial_fields, t, x, y):\n",
    "    \n",
    "    u = u_pred(net, initial_fields, t, x, y)\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x, tangent_y = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device), torch.ones(y.shape).to(device)\n",
    "    ut  = FWDAD_first_order_derivative(lambda t: u_pred(net, initial_fields, t, x, y), t, tangent_t)  # (bs, neval_c) \n",
    "    ux = FWDAD_first_order_derivative(lambda x: u_pred(net, initial_fields, t, x, y), x, tangent_x) # (bs, neval_c)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: u_pred(net, initial_fields, t, x, y), x, tangent_x) # (bs, neval_c)\n",
    "    uy = FWDAD_first_order_derivative(lambda y: u_pred(net, initial_fields, t, x, y), y, tangent_y) # (bs, neval_c)\n",
    "    uyy = FWDAD_second_order_derivative(lambda y: u_pred(net, initial_fields, t, x, y), y, tangent_y) # (bs, neval_c)\n",
    "    \n",
    "    pde_residual = (ut + (u*ux) + (u*uy) - (0.01*uxx) - (0.01*uyy))**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6bb15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:49.773006Z",
     "iopub.status.busy": "2025-01-24T06:12:49.772787Z",
     "iopub.status.idle": "2025-01-24T06:12:49.778068Z",
     "shell.execute_reply": "2025-01-24T06:12:49.777671Z"
    },
    "papermill": {
     "duration": 0.009855,
     "end_time": "2025-01-24T06:12:49.778854",
     "exception": false,
     "start_time": "2025-01-24T06:12:49.768999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_bcs(net, initial_fields, t, x, y):\n",
    "    \n",
    "    t_left, x_left, y_left = t[0], x[0], y[0]\n",
    "    t_right, x_right, y_right = t[1], x[1], y[1]\n",
    "    t_bottom, x_bottom, y_bottom = t[2], x[2], y[2]\n",
    "    t_top, x_top, y_top = t[3], x[3], y[3]\n",
    "\n",
    "    u_left = u_pred(net, initial_fields, t_left, x_left, y_left) # u is (bs, neval_b)\n",
    "    u_right = u_pred(net, initial_fields, t_right, x_right, y_right) # u is (bs, neval_b)\n",
    "    u_bottom = u_pred(net, initial_fields, t_bottom, x_bottom, y_bottom)  # u is (bs, neval_b)\n",
    "    u_top = u_pred(net, initial_fields, t_top, x_top, y_top) # u is (bs, neval_b)\n",
    "    \n",
    "    # bc1a and bc1b\n",
    "    pde_bc1a = (u_left - u_right)**2\n",
    "    tangent_x_left, tangent_x_right = torch.ones(x_left.shape).to(device), torch.ones(x_right.shape).to(device)\n",
    "    ux_left = FWDAD_first_order_derivative(lambda x_left: u_pred(net, initial_fields, t_left, x_left, y_left), x_left, tangent_x_left) # (bs, neval_b)\n",
    "    ux_right = FWDAD_first_order_derivative(lambda x_right: u_pred(net, initial_fields, t_right, x_right, y_right), x_right, tangent_x_right) # (bs, neval_b)\n",
    "    pde_bc1b = (ux_left - ux_right)**2\n",
    "    \n",
    "    # bc2a and bc2b\n",
    "    pde_bc2a = (u_bottom - u_top)**2\n",
    "    tangent_y_bottom, tangent_y_top = torch.ones(y_bottom.shape).to(device), torch.ones(y_top.shape).to(device)\n",
    "    uy_bottom = FWDAD_first_order_derivative(lambda y_bottom: u_pred(net, initial_fields, t_bottom, x_bottom, y_bottom), y_bottom, tangent_y_bottom) # (bs, neval_b)\n",
    "    uy_top = FWDAD_first_order_derivative(lambda y_top: u_pred(net, initial_fields, t_top, x_top, y_top), y_top, tangent_y_top) # (bs, neval_b)\n",
    "    pde_bc2b = (uy_bottom - uy_top)**2\n",
    "    \n",
    "    return torch.mean(pde_bc1a) + torch.mean(pde_bc1b) + torch.mean(pde_bc2a) + torch.mean(pde_bc2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf5af2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:49.786120Z",
     "iopub.status.busy": "2025-01-24T06:12:49.785815Z",
     "iopub.status.idle": "2025-01-24T06:12:49.788908Z",
     "shell.execute_reply": "2025-01-24T06:12:49.788519Z"
    },
    "papermill": {
     "duration": 0.007562,
     "end_time": "2025-01-24T06:12:49.789699",
     "exception": false,
     "start_time": "2025-01-24T06:12:49.782137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_ic(net, initial_fields, t, x, y):\n",
    "    \n",
    "    u_ic = u_pred(net, initial_fields, t, x, y) # u is (bs, neval_i)\n",
    "    \n",
    "    bs_ = initial_fields.shape[0]\n",
    "    ic_values_ = torch.zeros((bs_, x.shape[0], 1)).to(device)\n",
    "    for j in range(bs_):\n",
    "        ic_values_[j] = linear_interpolation_2D(x, y, x_span, y_span, initial_fields[j]) # initial condition: u_0(x) values\n",
    "    ic_values = ic_values_.reshape(-1, x.shape[0]) # (bs, neval_i)\n",
    "    \n",
    "    pde_ic = (u_ic - ic_values)**2\n",
    "    \n",
    "    return torch.mean(pde_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation_points(tc_span, xc_span, yc_span, neval_dict):\n",
    "    tc = tc_span.repeat_interleave(neval_dict['loc']).unsqueeze(-1)\n",
    "    xc = xc_span.flatten().repeat(neval_dict['t']).unsqueeze(-1)\n",
    "    yc = yc_span.flatten().repeat(neval_dict['t']).unsqueeze(-1)\n",
    "    return tc, xc, yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430a632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:12:49.807656Z",
     "iopub.status.busy": "2025-01-24T06:12:49.807369Z",
     "iopub.status.idle": "2025-01-24T19:09:54.676803Z",
     "shell.execute_reply": "2025-01-24T19:09:54.676443Z"
    },
    "papermill": {
     "duration": 46624.874185,
     "end_time": "2025-01-24T19:09:54.677712",
     "exception": false,
     "start_time": "2025-01-24T06:12:49.803527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 32 # Batch size\n",
    "\n",
    "neval_t = 21  # Number of randomly chosen time points at which output field is evaluated.\n",
    "neval_x = neval_y = 64\n",
    "# neval_loc = neval_x*neval_y  # Number of locations at which output field is evaluated at each time point.\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x*neval_y}  # Number of collocation points within the domain.\n",
    "neval_b = {'t': neval_t, 'loc': neval_x*1}        # Number of collocation points on each boundary.\n",
    "neval_i = {'t': 1, 'loc': neval_x*neval_y}        # Number of collocation points at t=0.\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20000, gamma=1.0) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "test_iteration_list, test_loss_list = [], []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "    else:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        inputs_train_used_batch = inputs_train_used.reshape(-1, 1, ny, nx)[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used.reshape(-1, nt*nx*ny)[indices_datadriven[0:bs]]\n",
    "        # print(f\"Shape of inputs_train_used_batch:\", inputs_train_used_batch.shape) # (bs, no. of channels, height, width)\n",
    "        # print(f\"Shape of outputs_train_used_batch:\", outputs_train_used_batch.shape) # (bs, nt*nx*ny)\n",
    "\n",
    "        predicted_values = u_pred(model, inputs_train_used_batch, \n",
    "                              grid[:, 0].reshape(-1,1), \n",
    "                              grid[:, 1].reshape(-1,1), \n",
    "                              grid[:, 2].reshape(-1,1))  # (bs, neval) = (bs, nt*nx*ny)\n",
    "        target_values = outputs_train_used_batch # (bs, nt*nx*ny)\n",
    "        datadriven_loss = nn.MSELoss()(predicted_values, target_values)\n",
    "        # print('*********')\n",
    "    \n",
    "    num_samples = len(inputs_initial_fields_only)\n",
    "    indices_pinn = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "    inputs_batch = inputs_initial_fields_only.reshape(-1, 1, ny, nx)[indices_pinn[0:bs]]\n",
    "    # print(f\"Shape of inputs_batch:\", inputs_batch.shape) # (bs, no. of channels, height, width)\n",
    "\n",
    "    # points within the domain\n",
    "    tc_span = td.uniform.Uniform(0., T).sample((neval_c['t'], 1)).to(device)\n",
    "    xc_span = td.uniform.Uniform(0., L).sample((neval_c['loc'], 1)).to(device)\n",
    "    yc_span = td.uniform.Uniform(0., L).sample((neval_c['loc'], 1)).to(device)\n",
    "\n",
    "    tc, xc, yc = collocation_points(tc_span, xc_span, yc_span, neval_c)\n",
    "    \n",
    "    # boundary points\n",
    "    \n",
    "    # for bc1a and bc1b\n",
    "    t_bc1_span = td.uniform.Uniform(0., T).sample((neval_b['t'], 1)).to(device)\n",
    "    x_left_span = torch.full((neval_b['loc'], 1), 0.).to(device)\n",
    "    x_right_span = torch.full((neval_b['loc'], 1), L).to(device)\n",
    "    y_bc1_span = td.uniform.Uniform(0, L).sample((neval_b['loc'], 1)).to(device)\n",
    "    t_left, x_left, y_left = collocation_points(t_bc1_span, x_left_span, y_bc1_span, neval_b)\n",
    "    t_right, x_right, y_right = collocation_points(t_bc1_span, x_right_span, y_bc1_span, neval_b)\n",
    "    \n",
    "    # for bc2a and bc2b\n",
    "    t_bc2_span = td.uniform.Uniform(0., T).sample((neval_b['t'], 1)).to(device)\n",
    "    y_bottom_span = torch.full((neval_b['loc'], 1), 0.).to(device)\n",
    "    y_top_span = torch.full((neval_b['loc'], 1), L).to(device)\n",
    "    x_bc2_span = td.uniform.Uniform(0, L).sample((neval_b['loc'], 1)).to(device)\n",
    "    t_bottom, x_bottom, y_bottom = collocation_points(t_bc2_span, x_bc2_span, y_bottom_span, neval_b)\n",
    "    t_top, x_top, y_top = collocation_points(t_bc2_span, x_bc2_span, y_top_span, neval_b)\n",
    "    \n",
    "    tb = [t_left, t_right, t_bottom, t_top]\n",
    "    xb = [x_left, x_right, x_bottom, x_top]\n",
    "    yb = [y_left, y_right, y_bottom, y_top]\n",
    "    \n",
    "    # initial points\n",
    "    ti = torch.full((neval_i['loc'], 1), 0.).to(device)\n",
    "    xi = td.uniform.Uniform(0., L).sample((neval_i['loc'], 1)).to(device)\n",
    "    yi = td.uniform.Uniform(0., L).sample((neval_i['loc'], 1)).to(device)\n",
    "    \n",
    "    pinn_loss = (loss_pde_residual(model, inputs_batch, tc, xc, yc)\n",
    "                 + loss_pde_bcs(model, inputs_batch, tb, xb, yb) \n",
    "                 + loss_pde_ic(model, inputs_batch, ti, xi, yi))\n",
    "    # print('*********')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if iteration % 500 == 0:\n",
    "        # Test loss calculation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_predicted_values = u_pred(model, inputs_test.reshape(-1, 1, ny, nx), \n",
    "                              grid[:, 0].reshape(-1,1), \n",
    "                              grid[:, 1].reshape(-1,1), \n",
    "                              grid[:, 2].reshape(-1,1))  # (bs, neval) = (bs, nt*nx*ny)\n",
    "            test_loss = nn.MSELoss()(test_predicted_values, outputs_test.reshape(-1, nt*nx*ny))\n",
    "            test_iteration_list.append(iteration)\n",
    "            test_loss_list.append(test_loss.item())  \n",
    "        model.train()  # Switch back to training mode\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f,' % optimizer.state_dict()['param_groups'][0]['lr'], \n",
    "              'test loss = %f' % test_loss)\n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    \n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "    np.save(os.path.join(resultdir,'test_iteration_list.npy'), np.asarray(test_iteration_list))\n",
    "    np.save(os.path.join(resultdir, 'test_loss_list.npy'), np.asarray(test_loss_list)) \n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "    \n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save) \n",
    "\n",
    "plot_testing_loss(resultdir, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_training_testing_loss(resultdir, iteration_list, loss_list, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)  \n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67807e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:09:54.691243Z",
     "iopub.status.busy": "2025-01-24T19:09:54.690828Z",
     "iopub.status.idle": "2025-01-24T19:09:54.698010Z",
     "shell.execute_reply": "2025-01-24T19:09:54.697683Z"
    },
    "papermill": {
     "duration": 0.014591,
     "end_time": "2025-01-24T19:09:54.698760",
     "exception": false,
     "start_time": "2025-01-24T19:09:54.684169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir, 'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521aff73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:09:54.711371Z",
     "iopub.status.busy": "2025-01-24T19:09:54.711128Z",
     "iopub.status.idle": "2025-01-24T19:13:14.280448Z",
     "shell.execute_reply": "2025-01-24T19:13:14.280069Z"
    },
    "papermill": {
     "duration": 199.583881,
     "end_time": "2025-01-24T19:13:14.288539",
     "exception": false,
     "start_time": "2025-01-24T19:09:54.704658",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions_test = u_pred(model, inputs_test.reshape(-1, 1, ny, nx), \n",
    "                          grid[:, 0].reshape(-1,1), \n",
    "                          grid[:, 1].reshape(-1,1), \n",
    "                          grid[:, 2].reshape(-1,1))  # (bs, neval) = (bs, nt*nx*ny)\n",
    "# print(predictions_test.shape)\n",
    "\n",
    "mse_list, r2score_list, relerror_list = [], [], []\n",
    "    \n",
    "for i in range(inputs_test.shape[0]):\n",
    "    \n",
    "    prediction_i = predictions_test[i].reshape(1, -1) # (1, nt*nx*ny)\n",
    "    target_i = outputs_test[i].reshape(1, -1) # (1, nt*nx*ny)\n",
    "\n",
    "    mse_i = F.mse_loss(prediction_i.cpu(), target_i.cpu())\n",
    "    r2score_i = metrics.r2_score(target_i.flatten().cpu().detach().numpy(), prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(target_i.flatten().cpu().detach().numpy() - prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(target_i.flatten().cpu().detach().numpy())\n",
    "        \n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item())\n",
    "    \n",
    "    # Plot the full solution-field for few cases:\n",
    "    if (i+1) % 5 == 0:\n",
    "        print(colored('TEST SAMPLE '+str(i+1), 'red'))\n",
    "\n",
    "        r2score_i = float('%.4f'%r2score_i)\n",
    "        relerror_i = float('%.4f'%relerror_i)\n",
    "        print('Rel. L2 Error = '+str(relerror_i)+', R2 score = '+str(r2score_i))\n",
    "        \n",
    "        cmap = 'jet'  # Color map\n",
    "        fontsize = 14  # Font size for labels and titles\n",
    "        levels = 100\n",
    "        # Plotting \n",
    "        plot_input_field(i, x_span.cpu().detach().numpy(), y_span.cpu().detach().numpy(), inputs_test[i].cpu().detach().numpy(), f\"Initial field\", cmap, fontsize, levels, resultdir, save)\n",
    "        plot_solution_field(i, x_span.cpu().detach().numpy(), y_span.cpu().detach().numpy(), target_i.reshape(nt,ny,nx).cpu().detach().numpy(), t_span.cpu().detach().numpy(), f\"True Solution\", cmap, fontsize, levels, resultdir, save, 'True-Solution')\n",
    "        plot_solution_field(i, x_span.cpu().detach().numpy(), y_span.cpu().detach().numpy(), prediction_i.reshape(nt,ny,nx).cpu().detach().numpy(), t_span.cpu().detach().numpy(), f\"Predicted Solution\", cmap, fontsize, levels, resultdir, save, 'Predicted-Solution')\n",
    "        plot_solution_field(i, x_span.cpu().detach().numpy(), y_span.cpu().detach().numpy(), torch.abs(target_i.reshape(nt,ny,nx) - prediction_i.reshape(nt,ny,nx)).cpu().detach().numpy(), t_span.cpu().detach().numpy(), f\"Absolute error\", cmap, fontsize, levels, resultdir, save, 'Absolute error')\n",
    "        print(colored('#'*230, 'green'))\n",
    "\n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac31ad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:13:14.823807Z",
     "iopub.status.busy": "2025-01-24T19:13:14.823549Z",
     "iopub.status.idle": "2025-01-24T19:13:14.859261Z",
     "shell.execute_reply": "2025-01-24T19:13:14.858870Z"
    },
    "papermill": {
     "duration": 0.30363,
     "end_time": "2025-01-24T19:13:14.860097",
     "exception": false,
     "start_time": "2025-01-24T19:13:14.556467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": predictions_test.reshape(-1, nt, ny, nx).cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965287e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:13:15.405020Z",
     "iopub.status.busy": "2025-01-24T19:13:15.404773Z",
     "iopub.status.idle": "2025-01-24T19:13:15.407711Z",
     "shell.execute_reply": "2025-01-24T19:13:15.407356Z"
    },
    "papermill": {
     "duration": 0.270984,
     "end_time": "2025-01-24T19:13:15.408541",
     "exception": false,
     "start_time": "2025-01-24T19:13:15.137557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9542a7",
   "metadata": {
    "papermill": {
     "duration": 0.266304,
     "end_time": "2025-01-24T19:13:15.940938",
     "exception": false,
     "start_time": "2025-01-24T19:13:15.674634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdbede",
   "metadata": {
    "papermill": {
     "duration": 0.266479,
     "end_time": "2025-01-24T19:13:16.473376",
     "exception": false,
     "start_time": "2025-01-24T19:13:16.206897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 46833.182582,
   "end_time": "2025-01-24T19:13:17.956155",
   "environment_variables": {},
   "exception": null,
   "input_path": "a_Vanilla-NO.ipynb",
   "output_path": "results/a_Vanilla-NO/seed=0_n_used=0/output_seed=0_n_used=0.ipynb",
   "parameters": {
    "n_used": 0,
    "save": true,
    "seed": 0
   },
   "start_time": "2025-01-24T06:12:44.773573",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
