{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccc73df",
   "metadata": {
    "papermill": {
     "duration": 0.004208,
     "end_time": "2025-01-24T06:13:12.883067",
     "exception": false,
     "start_time": "2025-01-24T06:13:12.878859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Comparisons\n",
    "\n",
    "This code can execute the following variants of losses:  \n",
    "\n",
    "1. **Variant 1:**  Purely Physics  \n",
    "   $L_{\\theta} = L_{\\text{PDE}}$  \n",
    "   Use $n_{\\text{used}} = 0$  \n",
    "\n",
    "2. **Variant 2:**  Physics + Data  \n",
    "   $L_{\\theta} = L_{\\text{PDE}} + \\Sigma_{i=1}^{n_{\\text{used}}}\\| u_i - \\hat{u}_i \\|_2^2$  \n",
    "   Use $n_{\\text{used}} \\in (0, 300]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a75ad",
   "metadata": {
    "papermill": {
     "duration": 0.002822,
     "end_time": "2025-01-24T06:13:12.889099",
     "exception": false,
     "start_time": "2025-01-24T06:13:12.886277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1395ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:12.895523Z",
     "iopub.status.busy": "2025-01-24T06:13:12.895306Z",
     "iopub.status.idle": "2025-01-24T06:13:15.369376Z",
     "shell.execute_reply": "2025-01-24T06:13:15.368759Z"
    },
    "papermill": {
     "duration": 2.478797,
     "end_time": "2025-01-24T06:13:15.370789",
     "exception": false,
     "start_time": "2025-01-24T06:13:12.891992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from termcolor import colored\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from utils.networks import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "from utils.deeponet_networks_2d import *\n",
    "from utils.visualizer_2d import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921aabe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:15.378508Z",
     "iopub.status.busy": "2025-01-24T06:13:15.377924Z",
     "iopub.status.idle": "2025-01-24T06:13:15.380778Z",
     "shell.execute_reply": "2025-01-24T06:13:15.380338Z"
    },
    "papermill": {
     "duration": 0.007437,
     "end_time": "2025-01-24T06:13:15.381609",
     "exception": false,
     "start_time": "2025-01-24T06:13:15.374172",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Tag this cell with 'parameters'\n",
    "# parameters\n",
    "seed = 0 # Seed number.\n",
    "n_used = 150 # Number of full training fields used for estimating the data-driven loss term\n",
    "n_iterations = 80000 # Number of iterations.\n",
    "use_fourier_features = True\n",
    "n_fourier = 10 # Number of fourier frequencies considered.\n",
    "save = True # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:15.397200Z",
     "iopub.status.busy": "2025-01-24T06:13:15.396924Z",
     "iopub.status.idle": "2025-01-24T06:13:15.399624Z",
     "shell.execute_reply": "2025-01-24T06:13:15.399213Z"
    },
    "papermill": {
     "duration": 0.006798,
     "end_time": "2025-01-24T06:13:15.400446",
     "exception": false,
     "start_time": "2025-01-24T06:13:15.393648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','b_Latent-NO','seed='+str(seed)+'_n_used='+str(n_used)) \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7126e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:15.406808Z",
     "iopub.status.busy": "2025-01-24T06:13:15.406532Z",
     "iopub.status.idle": "2025-01-24T06:13:15.410366Z",
     "shell.execute_reply": "2025-01-24T06:13:15.409926Z"
    },
    "papermill": {
     "duration": 0.007938,
     "end_time": "2025-01-24T06:13:15.411181",
     "exception": false,
     "start_time": "2025-01-24T06:13:15.403243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d11f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:15.417795Z",
     "iopub.status.busy": "2025-01-24T06:13:15.417452Z",
     "iopub.status.idle": "2025-01-24T06:13:15.542874Z",
     "shell.execute_reply": "2025-01-24T06:13:15.542391Z"
    },
    "papermill": {
     "duration": 0.129689,
     "end_time": "2025-01-24T06:13:15.543757",
     "exception": false,
     "start_time": "2025-01-24T06:13:15.414068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce4ce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:15.550718Z",
     "iopub.status.busy": "2025-01-24T06:13:15.550401Z",
     "iopub.status.idle": "2025-01-24T06:13:15.588910Z",
     "shell.execute_reply": "2025-01-24T06:13:15.588390Z"
    },
    "papermill": {
     "duration": 0.042928,
     "end_time": "2025-01-24T06:13:15.589883",
     "exception": false,
     "start_time": "2025-01-24T06:13:15.546955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = torch.load(os.path.join('..','..','data/2D_Burgers_equation_scalar/Burgers_equation_2D_scalar.pt'))\n",
    "\n",
    "for key, tensor in data.items():\n",
    "    print(f\"{key}: {tensor.shape}\")\n",
    "    \n",
    "# Random Initial conditions: Nsamples x 32 x 32, each IC sample is (32 x 32)\n",
    "# Time evolution of the solution field: Nsamples x 21 x 32 x 32.\n",
    "# Each field is  21 x 32 x 32, rows correspond to time and other dimensions correspond to the field.\n",
    "# First row corresponds to solution at t=0 (1st time step)\n",
    "# and next  row corresponds to solution at t=0.05 (2nd time step) and so on.\n",
    "# last row correspond to solution at t=1 (21st time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee83656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:15.597553Z",
     "iopub.status.busy": "2025-01-24T06:13:15.597174Z",
     "iopub.status.idle": "2025-01-24T06:13:15.979534Z",
     "shell.execute_reply": "2025-01-24T06:13:15.979009Z"
    },
    "papermill": {
     "duration": 0.387183,
     "end_time": "2025-01-24T06:13:15.980531",
     "exception": false,
     "start_time": "2025-01-24T06:13:15.593348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = data['input_samples'].float().to(device)\n",
    "outputs = data['output_samples'].float().to(device)\n",
    "t_span = data['t_span'].float().to(device)\n",
    "x_span = data['x_span'].float().to(device)\n",
    "y_span = data['y_span'].float().to(device)\n",
    "\n",
    "L = 1.         # Simulation domain [0, L]^2\n",
    "T = 1.         # Simulation time [0, T]\n",
    "\n",
    "nt, nx, ny = len(t_span), len(x_span), len(y_span) # number of discretizations in time, location_x and location_y.\n",
    "\n",
    "grid = torch.vstack((t_span.repeat_interleave(ny*nx), \n",
    "              x_span.flatten().repeat(nt),\n",
    "              y_span.flatten().repeat(nt))).T\n",
    "print(\"Shape of grid:\", grid.shape) # (nt*nx*ny, 3)\n",
    "print(\"grid:\", grid) # time, location_x, location_y\n",
    "\n",
    "# Split the data into training and testing samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=50, random_state=seed)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d843048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:15.988295Z",
     "iopub.status.busy": "2025-01-24T06:13:15.987878Z",
     "iopub.status.idle": "2025-01-24T06:13:15.991042Z",
     "shell.execute_reply": "2025-01-24T06:13:15.990613Z"
    },
    "papermill": {
     "duration": 0.007768,
     "end_time": "2025-01-24T06:13:15.991882",
     "exception": false,
     "start_time": "2025-01-24T06:13:15.984114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term \n",
    "inputs_train_used = inputs_train[:n_used, :, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586e7e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:15.998839Z",
     "iopub.status.busy": "2025-01-24T06:13:15.998566Z",
     "iopub.status.idle": "2025-01-24T06:13:16.019612Z",
     "shell.execute_reply": "2025-01-24T06:13:16.019163Z"
    },
    "papermill": {
     "duration": 0.025447,
     "end_time": "2025-01-24T06:13:16.020428",
     "exception": false,
     "start_time": "2025-01-24T06:13:15.994981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_initial_fields_only = torch.load(os.path.join('..','..','data/2D_Burgers_equation_scalar/Burgers_equation_2D_scalar_initial_fields_only.pt')).to(device)\n",
    "print(inputs_initial_fields_only.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb259c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:16.027633Z",
     "iopub.status.busy": "2025-01-24T06:13:16.027304Z",
     "iopub.status.idle": "2025-01-24T06:13:16.030570Z",
     "shell.execute_reply": "2025-01-24T06:13:16.030160Z"
    },
    "papermill": {
     "duration": 0.007754,
     "end_time": "2025-01-24T06:13:16.031406",
     "exception": false,
     "start_time": "2025-01-24T06:13:16.023652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fourier_features_temporal(t, n_fourier):\n",
    "    pi = math.pi\n",
    "    result = torch.zeros(t.size(0), 1+(2*n_fourier)).to(device)  # Initialize result tensor\n",
    "\n",
    "    t = t.squeeze()\n",
    "\n",
    "    # Compute the transformation\n",
    "    result[:, 0] = t\n",
    "    for i in range(n_fourier):\n",
    "        result[:, 2*i + 0 + 1] = torch.cos((i + 1) * pi * t)\n",
    "        result[:, 2*i + 1 + 1] = torch.sin((i + 1) * pi * t)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c842b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:16.038524Z",
     "iopub.status.busy": "2025-01-24T06:13:16.038194Z",
     "iopub.status.idle": "2025-01-24T06:13:16.041793Z",
     "shell.execute_reply": "2025-01-24T06:13:16.041395Z"
    },
    "papermill": {
     "duration": 0.008052,
     "end_time": "2025-01-24T06:13:16.042593",
     "exception": false,
     "start_time": "2025-01-24T06:13:16.034541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fourier_features_spatial(x, y, n_fourier):\n",
    "    pi = math.pi\n",
    "    result = torch.zeros(x.size(0), 2+(4*n_fourier)).to(device)  # Initialize result tensor\n",
    "    \n",
    "    x = x.squeeze()\n",
    "    y = y.squeeze()\n",
    "    \n",
    "    # Compute the transformation\n",
    "    result[:, 0] = x\n",
    "    result[:, 1] = y\n",
    "    for i in range(n_fourier):\n",
    "        result[:, 4*i + 0 + 2] = torch.cos((i + 1) * pi * x)\n",
    "        result[:, 4*i + 1 + 2] = torch.sin((i + 1) * pi * x)\n",
    "        result[:, 4*i + 2 + 2] = torch.cos((i + 1) * pi * y)\n",
    "        result[:, 4*i + 3 + 2] = torch.sin((i + 1) * pi * y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6e869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:16.049553Z",
     "iopub.status.busy": "2025-01-24T06:13:16.049289Z",
     "iopub.status.idle": "2025-01-24T06:13:16.051365Z",
     "shell.execute_reply": "2025-01-24T06:13:16.050964Z"
    },
    "papermill": {
     "duration": 0.006434,
     "end_time": "2025-01-24T06:13:16.052169",
     "exception": false,
     "start_time": "2025-01-24T06:13:16.045735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_dim = 60 # d_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5719c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:16.059399Z",
     "iopub.status.busy": "2025-01-24T06:13:16.059167Z",
     "iopub.status.idle": "2025-01-24T06:13:16.267692Z",
     "shell.execute_reply": "2025-01-24T06:13:16.266909Z"
    },
    "papermill": {
     "duration": 0.213338,
     "end_time": "2025-01-24T06:13:16.268661",
     "exception": false,
     "start_time": "2025-01-24T06:13:16.055323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_neurons_latent_branch: Number of input neurons in the latent_branch net.\n",
    "input_neurons_latent_trunk: Number of input neurons in the latent_trunk net.\n",
    "latent_p: Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\"\"\"\n",
    "latent_p = latent_dim*10 # Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\n",
    "input_neurons_latent_branch = (ny, nx) # Specify input size of image as a tuple (height, width)\n",
    "n_channels = 1\n",
    "num_filters = [20, 30, 40]\n",
    "filter_sizes = [3, 3, 3]\n",
    "strides = [1]*len(num_filters)\n",
    "paddings = [0]*len(num_filters)\n",
    "poolings = [('avg', 2, 2), ('avg', 2, 2), ('avg', 2, 2)]  # Pooling layer specification (type, kernel_size, stride)\n",
    "end_MLP_layersizes = [150, 150, latent_p]\n",
    "activation = nn.SiLU() # nn.ReLU() #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_branch_net = ConvNet(input_neurons_latent_branch, n_channels, num_filters, filter_sizes, strides, paddings, poolings, end_MLP_layersizes, activation)\n",
    "latent_branch_net.to(device)\n",
    "# print(latent_branch_net)\n",
    "print('LATENT BRANCH-NET SUMMARY:')\n",
    "summary(latent_branch_net, input_size=(n_channels, ny, nx))  # input shape is (channels, height, width)\n",
    "print('#'*100)\n",
    "\n",
    "# 1 corresponds to t\n",
    "input_neurons_latent_trunk = 1 # Number of input neurons in the trunk net.\n",
    "latent_trunk_net = DenseNet(layersizes=[input_neurons_latent_trunk] + [128]*4 + [latent_p], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_trunk_net.to(device)\n",
    "# print(latent_trunk_net)\n",
    "print('LATENT TRUNK-NET SUMMARY:')\n",
    "summary(latent_trunk_net, input_size=(input_neurons_latent_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "\"\"\"\n",
    "input_neurons_reconstruction_branch: Number of input neurons in the reconstruction_branch net.\n",
    "input_neurons_reconstruction_trunk: Number of input neurons in the reconstruction_trunk net.\n",
    "reconstruction_q: Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\"\"\"\n",
    "reconstruction_q = 128 # Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\n",
    "input_neurons_reconstruction_branch = latent_dim # d_z\n",
    "reconstruction_branch_net = DenseNet(layersizes=[input_neurons_reconstruction_branch] + [128]*4 + [reconstruction_q], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_branch_net.to(device)\n",
    "# print(reconstruction_branch_net)\n",
    "print('RECONSTRUCTION BRANCH-NET SUMMARY:')\n",
    "summary(reconstruction_branch_net, input_size=(input_neurons_reconstruction_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "# 2 corresponds to x and y\n",
    "if use_fourier_features == False:\n",
    "    input_neurons_reconstruction_trunk = 2 # Number of input neurons in the trunk net.\n",
    "else:\n",
    "    input_neurons_reconstruction_trunk = 2 + (4*n_fourier) # Number of input neurons in the trunk net.\n",
    "reconstruction_trunk_net = DenseNet(layersizes=[input_neurons_reconstruction_trunk] + [128]*4 + [reconstruction_q], activation=nn.SiLU()) #Sin() #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_trunk_net.to(device)\n",
    "# print(reconstruction_trunk_net)\n",
    "print('RECONSTRUCTION TRUNK-NET SUMMARY:')\n",
    "summary(reconstruction_trunk_net, input_size=(input_neurons_reconstruction_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Latent_NO_model(latent_branch_net, latent_trunk_net, latent_dim, reconstruction_branch_net, reconstruction_trunk_net)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86a175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:16.277112Z",
     "iopub.status.busy": "2025-01-24T06:13:16.276770Z",
     "iopub.status.idle": "2025-01-24T06:13:16.279893Z",
     "shell.execute_reply": "2025-01-24T06:13:16.279454Z"
    },
    "papermill": {
     "duration": 0.00812,
     "end_time": "2025-01-24T06:13:16.280693",
     "exception": false,
     "start_time": "2025-01-24T06:13:16.272573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_learnable_parameters = (count_learnable_parameters(latent_branch_net)\n",
    "                            + count_learnable_parameters(latent_trunk_net)\n",
    "                            + count_learnable_parameters(reconstruction_branch_net)\n",
    "                            + count_learnable_parameters(reconstruction_trunk_net))\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f66a2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:16.288287Z",
     "iopub.status.busy": "2025-01-24T06:13:16.288005Z",
     "iopub.status.idle": "2025-01-24T06:13:16.290881Z",
     "shell.execute_reply": "2025-01-24T06:13:16.290482Z"
    },
    "papermill": {
     "duration": 0.007582,
     "end_time": "2025-01-24T06:13:16.291665",
     "exception": false,
     "start_time": "2025-01-24T06:13:16.284083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def u_pred(net, inputs, t, x, y):\n",
    "    if use_fourier_features == False:\n",
    "        latent_prediction, reconstruction_prediction = net(inputs, t, torch.hstack([x, y])) \n",
    "    elif use_fourier_features == True:\n",
    "        latent_prediction, reconstruction_prediction = net(inputs, t, fourier_features_spatial(x, y, n_fourier)) \n",
    "    u = reconstruction_prediction # (bs, neval_t, neval_loc)\n",
    "    return latent_prediction, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c17aa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:16.299269Z",
     "iopub.status.busy": "2025-01-24T06:13:16.298918Z",
     "iopub.status.idle": "2025-01-24T06:13:16.303166Z",
     "shell.execute_reply": "2025-01-24T06:13:16.302767Z"
    },
    "papermill": {
     "duration": 0.008784,
     "end_time": "2025-01-24T06:13:16.303905",
     "exception": false,
     "start_time": "2025-01-24T06:13:16.295121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, initial_fields, t, x, y):\n",
    "    \n",
    "    u = u_pred(net, initial_fields, t, x, y)[1]\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x, tangent_y = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device), torch.ones(y.shape).to(device)\n",
    "    ut = FWDAD_first_order_derivative(lambda t: u_pred(net, initial_fields, t, x, y)[1], t, tangent_t) # (bs, neval_t, neval_loc)\n",
    "    ux = FWDAD_first_order_derivative(lambda x: u_pred(net, initial_fields, t, x, y)[1], x, tangent_x) # (bs, neval_t, neval_loc)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: u_pred(net, initial_fields, t, x, y)[1], x, tangent_x) # (bs, neval_t, neval_loc)\n",
    "    uy = FWDAD_first_order_derivative(lambda y: u_pred(net, initial_fields, t, x, y)[1], y, tangent_y) # (bs, neval_t, neval_loc)\n",
    "    uyy = FWDAD_second_order_derivative(lambda y: u_pred(net, initial_fields, t, x, y)[1], y, tangent_y) # (bs, neval_t, neval_loc)\n",
    "   \n",
    "    pde_residual = (ut + (u*ux) + (u*uy) - (0.01*uxx) - (0.01*uyy))**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9deff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:16.311633Z",
     "iopub.status.busy": "2025-01-24T06:13:16.311311Z",
     "iopub.status.idle": "2025-01-24T06:13:16.316715Z",
     "shell.execute_reply": "2025-01-24T06:13:16.316305Z"
    },
    "papermill": {
     "duration": 0.010192,
     "end_time": "2025-01-24T06:13:16.317469",
     "exception": false,
     "start_time": "2025-01-24T06:13:16.307277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_bcs(net, initial_fields, t, x, y):\n",
    "    \n",
    "    t_left, x_left, y_left = t[0], x[0], y[0]\n",
    "    t_right, x_right, y_right = t[1], x[1], y[1]\n",
    "    t_bottom, x_bottom, y_bottom = t[2], x[2], y[2]\n",
    "    t_top, x_top, y_top = t[3], x[3], y[3]\n",
    "\n",
    "    u_left = u_pred(net, initial_fields, t_left, x_left, y_left)[1] # u is (bs, neval_t, neval_loc)\n",
    "    u_right = u_pred(net, initial_fields, t_right, x_right, y_right)[1] # u is (bs, neval_t, neval_loc)\n",
    "    u_bottom = u_pred(net, initial_fields, t_bottom, x_bottom, y_bottom)[1]  # u is (bs, neval_t, neval_loc)\n",
    "    u_top = u_pred(net, initial_fields, t_top, x_top, y_top)[1] # u is (bs, neval_t, neval_loc)\n",
    "    \n",
    "    # bc1a and bc1b\n",
    "    pde_bc1a = (u_left - u_right)**2\n",
    "    tangent_x_left, tangent_x_right = torch.ones(x_left.shape).to(device), torch.ones(x_right.shape).to(device)\n",
    "    ux_left = FWDAD_first_order_derivative(lambda x_left: u_pred(net, initial_fields, t_left, x_left, y_left)[1], x_left, tangent_x_left) # (bs, neval_t, neval_loc)\n",
    "    ux_right = FWDAD_first_order_derivative(lambda x_right: u_pred(net, initial_fields, t_right, x_right, y_right)[1], x_right, tangent_x_right) # (bs, neval_t, neval_loc)\n",
    "    pde_bc1b = (ux_left - ux_right)**2\n",
    "    \n",
    "    # bc2a and bc2b\n",
    "    pde_bc2a = (u_bottom - u_top)**2\n",
    "    tangent_y_bottom, tangent_y_top = torch.ones(y_bottom.shape).to(device), torch.ones(y_top.shape).to(device)\n",
    "    uy_bottom = FWDAD_first_order_derivative(lambda y_bottom: u_pred(net, initial_fields, t_bottom, x_bottom, y_bottom)[1], y_bottom, tangent_y_bottom) # (bs, neval_t, neval_loc)\n",
    "    uy_top = FWDAD_first_order_derivative(lambda y_top: u_pred(net, initial_fields, t_top, x_top, y_top)[1], y_top, tangent_y_top) # (bs, neval_t, neval_loc)\n",
    "    pde_bc2b = (uy_bottom - uy_top)**2\n",
    "    \n",
    "    return torch.mean(pde_bc1a) + torch.mean(pde_bc1b) + torch.mean(pde_bc2a) + torch.mean(pde_bc2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd126f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:16.324988Z",
     "iopub.status.busy": "2025-01-24T06:13:16.324663Z",
     "iopub.status.idle": "2025-01-24T06:13:16.327834Z",
     "shell.execute_reply": "2025-01-24T06:13:16.327437Z"
    },
    "papermill": {
     "duration": 0.00778,
     "end_time": "2025-01-24T06:13:16.328619",
     "exception": false,
     "start_time": "2025-01-24T06:13:16.320839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_ic(net, initial_fields, t, x, y):\n",
    "    \n",
    "    u_ic = u_pred(net, initial_fields, t, x, y)[1] # u is (bs, 1, neval_loc)\n",
    "    \n",
    "    bs_ = initial_fields.shape[0]\n",
    "    ic_values_ = torch.zeros((bs_, x.shape[0], 1)).to(device)\n",
    "    for j in range(bs_):\n",
    "        ic_values_[j] = linear_interpolation_2D(x, y, x_span, y_span, initial_fields[j]) # initial condition: u_0(x) values\n",
    "    ic_values = ic_values_.reshape(-1, 1, x.shape[0]) # (bs, 1, neval_loc)\n",
    "    \n",
    "    pde_ic = (u_ic - ic_values)**2\n",
    "    \n",
    "    return torch.mean(pde_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e40852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:13:16.336355Z",
     "iopub.status.busy": "2025-01-24T06:13:16.336200Z",
     "iopub.status.idle": "2025-01-24T20:25:30.294882Z",
     "shell.execute_reply": "2025-01-24T20:25:30.294537Z"
    },
    "papermill": {
     "duration": 51133.963887,
     "end_time": "2025-01-24T20:25:30.295914",
     "exception": false,
     "start_time": "2025-01-24T06:13:16.332027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 32 # Batch size\n",
    "\n",
    "neval_t = 21  # Number of time points at which latent output field is evaluated for a given input sample.\n",
    "neval_x = neval_y = 64\n",
    "# neval_loc = neval_x*neval_y  # Number of locations at which output field is evaluated at each time point.\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x*neval_y}  # Number of collocation points within the domain.\n",
    "neval_b = {'t': neval_t, 'loc': neval_x*1}        # Number of collocation points on each boundary.\n",
    "neval_i = {'t': 1, 'loc': neval_x*neval_y}        # Number of collocation points at t=0.\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20000, gamma=1.0) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "test_iteration_list, test_loss_list = [], []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "    else:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        inputs_train_used_batch = inputs_train_used.reshape(-1, 1, ny, nx)[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used.reshape(-1, nt, nx*ny)[indices_datadriven[0:bs]]\n",
    "        # print(f\"Shape of inputs_train_used_batch:\", inputs_train_used_batch.shape) # (bs, no. of channels, height, width)\n",
    "        # print(f\"Shape of outputs_train_used_batch:\", outputs_train_used_batch.shape) # (bs, nt, nx*ny)\n",
    "\n",
    "        _, reconstruction_predicted_values = u_pred(model, inputs_train_used_batch, \n",
    "                                                          t_span.reshape(-1, 1), \n",
    "                                                          x_span.flatten().reshape(-1,1), \n",
    "                                                          y_span.flatten().reshape(-1,1)) # (bs, nt, latent_dim), (bs, nt, nx*ny)\n",
    "        reconstruction_target_values = outputs_train_used_batch # (bs, nt, nx*ny)\n",
    "        datadriven_loss = nn.MSELoss()(reconstruction_predicted_values, reconstruction_target_values)\n",
    "        # print('*********')\n",
    "    \n",
    "    num_samples = len(inputs_initial_fields_only)\n",
    "    indices_pinn = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "    inputs_batch = inputs_initial_fields_only.reshape(-1, 1, ny, nx)[indices_pinn[0:bs]]\n",
    "    #print(f\"Shape of inputs_batch:\", inputs_batch.shape) # (bs, no. of channels, height, width)\n",
    "\n",
    "    # points within the domain\n",
    "    tc = td.uniform.Uniform(0., T).sample((neval_c['t'], 1)).to(device)\n",
    "    xc = td.uniform.Uniform(0., L).sample((neval_c['loc'], 1)).to(device)\n",
    "    yc = td.uniform.Uniform(0., L).sample((neval_c['loc'], 1)).to(device)\n",
    "    \n",
    "    # boundary points\n",
    "    \n",
    "    # for bc1a and bc1b\n",
    "    t_bc1 = td.uniform.Uniform(0., T).sample((neval_b['t'], 1)).to(device)\n",
    "    x_left = torch.full((neval_b['loc'], 1), 0.).to(device)\n",
    "    x_right = torch.full((neval_b['loc'], 1), L).to(device)\n",
    "    y_bc1 = td.uniform.Uniform(0, L).sample((neval_b['loc'], 1)).to(device)\n",
    "    t_left, x_left, y_left = t_bc1, x_left, y_bc1\n",
    "    t_right, x_right, y_right = t_bc1, x_right, y_bc1\n",
    "    \n",
    "    # for bc2a and bc2b\n",
    "    t_bc2 = td.uniform.Uniform(0., T).sample((neval_b['t'], 1)).to(device)\n",
    "    y_bottom = torch.full((neval_b['loc'], 1), 0.).to(device)\n",
    "    y_top = torch.full((neval_b['loc'], 1), L).to(device)\n",
    "    x_bc2 = td.uniform.Uniform(0, L).sample((neval_b['loc'], 1)).to(device)\n",
    "    t_bottom, x_bottom, y_bottom = t_bc2, x_bc2, y_bottom\n",
    "    t_top, x_top, y_top = t_bc2, x_bc2, y_top\n",
    "    \n",
    "    tb = [t_left, t_right, t_bottom, t_top]\n",
    "    xb = [x_left, x_right, x_bottom, x_top]\n",
    "    yb = [y_left, y_right, y_bottom, y_top]\n",
    "    \n",
    "    # initial points\n",
    "    ti = torch.zeros((1, 1)).to(device)\n",
    "    xi = td.uniform.Uniform(0., L).sample((neval_i['loc'], 1)).to(device)\n",
    "    yi = td.uniform.Uniform(0., L).sample((neval_i['loc'], 1)).to(device)\n",
    "    \n",
    "\n",
    "    pinn_loss = (loss_pde_residual(model, inputs_batch, tc, xc, yc) \n",
    "                 + loss_pde_bcs(model, inputs_batch, tb, xb, yb) \n",
    "                 + loss_pde_ic(model, inputs_batch, ti, xi, yi))\n",
    "    # print('*********')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if iteration % 500 == 0:\n",
    "        # Test loss calculation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_predicted_values = u_pred(model, inputs_test.reshape(-1, 1, ny, nx), \n",
    "                              t_span.reshape(-1, 1), \n",
    "                              x_span.flatten().reshape(-1,1), \n",
    "                              y_span.flatten().reshape(-1,1))[1]  # (bs, neval) = (bs, nt, nx*ny)\n",
    "            test_loss = nn.MSELoss()(test_predicted_values, outputs_test.reshape(-1, nt, nx*ny))\n",
    "            test_iteration_list.append(iteration)\n",
    "            test_loss_list.append(test_loss.item())  \n",
    "        model.train()  # Switch back to training mode\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f,' % optimizer.state_dict()['param_groups'][0]['lr'], \n",
    "              'test loss = %f' % test_loss)\n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    \n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "    np.save(os.path.join(resultdir,'test_iteration_list.npy'), np.asarray(test_iteration_list))\n",
    "    np.save(os.path.join(resultdir, 'test_loss_list.npy'), np.asarray(test_loss_list)) \n",
    "\n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "    \n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save)\n",
    "\n",
    "plot_testing_loss(resultdir, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_training_testing_loss(resultdir, iteration_list, loss_list, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)  \n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d2881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T20:25:30.310613Z",
     "iopub.status.busy": "2025-01-24T20:25:30.310287Z",
     "iopub.status.idle": "2025-01-24T20:25:30.319471Z",
     "shell.execute_reply": "2025-01-24T20:25:30.319083Z"
    },
    "papermill": {
     "duration": 0.017743,
     "end_time": "2025-01-24T20:25:30.320335",
     "exception": false,
     "start_time": "2025-01-24T20:25:30.302592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir, 'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c742e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T20:25:30.333741Z",
     "iopub.status.busy": "2025-01-24T20:25:30.333447Z",
     "iopub.status.idle": "2025-01-24T20:28:57.051733Z",
     "shell.execute_reply": "2025-01-24T20:28:57.051234Z"
    },
    "papermill": {
     "duration": 207.017037,
     "end_time": "2025-01-24T20:28:57.343488",
     "exception": false,
     "start_time": "2025-01-24T20:25:30.326451",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "_, reconstruction_predictions_test = u_pred(model, inputs_test.reshape(-1, 1, ny, nx), \n",
    "                                                                  t_span.reshape(-1, 1), \n",
    "                                                                  x_span.flatten().reshape(-1,1), \n",
    "                                                                  y_span.flatten().reshape(-1,1)) # (bs, nt, latent_dim), (bs, nt, nx*ny)\n",
    "# print(reconstruction_predictions_test.shape)\n",
    "\n",
    "mse_list, r2score_list, relerror_list = [], [], []\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "    \n",
    "    reconstruction_prediction_i = reconstruction_predictions_test[i].unsqueeze(0)# (1, nt, nx*ny)\n",
    "    reconstruction_target_i = outputs_test[i].reshape(nt, nx*ny).unsqueeze(0) # (1, nt, nx*ny)\n",
    "\n",
    "    reconstruction_mse_i = F.mse_loss(reconstruction_prediction_i.cpu(), reconstruction_target_i.cpu())\n",
    "    mse_i = reconstruction_mse_i\n",
    "    \n",
    "    r2score_i = metrics.r2_score(reconstruction_target_i.flatten().cpu().detach().numpy(), reconstruction_prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy() - reconstruction_prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy())\n",
    "        \n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item())\n",
    "    \n",
    "    # Plot the full solution-field for few cases:\n",
    "    if (i+1) % 5 == 0:\n",
    "        print(colored('TEST SAMPLE '+str(i+1), 'red'))\n",
    "\n",
    "        r2score_i = float('%.4f'%r2score_i)\n",
    "        relerror_i = float('%.4f'%relerror_i)\n",
    "        print('Rel. L2 Error = '+str(relerror_i)+', R2 score = '+str(r2score_i))\n",
    "        \n",
    "        cmap = 'jet'  # Color map\n",
    "        fontsize = 14  # Font size for labels and titles\n",
    "        levels = 100\n",
    "        # Plotting \n",
    "        plot_input_field(i, x_span.cpu().detach().numpy(), y_span.cpu().detach().numpy(), inputs_test[i].cpu().detach().numpy(), f\"Initial field\", cmap, fontsize, levels, resultdir, save)\n",
    "        plot_solution_field(i, x_span.cpu().detach().numpy(), y_span.cpu().detach().numpy(), reconstruction_target_i.reshape(nt,ny,nx).cpu().detach().numpy(), t_span.cpu().detach().numpy(), f\"True Solution\", cmap, fontsize, levels, resultdir, save, 'True-Solution')\n",
    "        plot_solution_field(i, x_span.cpu().detach().numpy(), y_span.cpu().detach().numpy(), reconstruction_prediction_i.reshape(nt,ny,nx).cpu().detach().numpy(), t_span.cpu().detach().numpy(), f\"Predicted Solution\", cmap, fontsize, levels, resultdir, save, 'Predicted-Solution')\n",
    "        plot_solution_field(i, x_span.cpu().detach().numpy(), y_span.cpu().detach().numpy(), torch.abs(reconstruction_target_i.reshape(nt,ny,nx) - reconstruction_prediction_i.reshape(nt,ny,nx)).cpu().detach().numpy(), t_span.cpu().detach().numpy(), f\"Absolute error\", cmap, fontsize, levels, resultdir, save, 'Absolute error')\n",
    "        print(colored('#'*230, 'green'))\n",
    "        \n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0b218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T20:28:57.897266Z",
     "iopub.status.busy": "2025-01-24T20:28:57.896875Z",
     "iopub.status.idle": "2025-01-24T20:28:57.940444Z",
     "shell.execute_reply": "2025-01-24T20:28:57.940029Z"
    },
    "papermill": {
     "duration": 0.320645,
     "end_time": "2025-01-24T20:28:57.941291",
     "exception": false,
     "start_time": "2025-01-24T20:28:57.620646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": reconstruction_predictions_test.reshape(-1, nt, ny, nx).cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6d8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T20:28:58.478130Z",
     "iopub.status.busy": "2025-01-24T20:28:58.477800Z",
     "iopub.status.idle": "2025-01-24T20:28:58.481172Z",
     "shell.execute_reply": "2025-01-24T20:28:58.480816Z"
    },
    "papermill": {
     "duration": 0.273704,
     "end_time": "2025-01-24T20:28:58.481971",
     "exception": false,
     "start_time": "2025-01-24T20:28:58.208267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a366414",
   "metadata": {
    "papermill": {
     "duration": 0.264932,
     "end_time": "2025-01-24T20:28:59.010959",
     "exception": false,
     "start_time": "2025-01-24T20:28:58.746027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e4665",
   "metadata": {
    "papermill": {
     "duration": 0.265803,
     "end_time": "2025-01-24T20:28:59.541270",
     "exception": false,
     "start_time": "2025-01-24T20:28:59.275467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 51349.519695,
   "end_time": "2025-01-24T20:29:01.342364",
   "environment_variables": {},
   "exception": null,
   "input_path": "b_Latent-NO.ipynb",
   "output_path": "results/b_Latent-NO/seed=0_n_used=0/output_seed=0_n_used=0.ipynb",
   "parameters": {
    "n_used": 0,
    "save": true,
    "seed": 0
   },
   "start_time": "2025-01-24T06:13:11.822669",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
