{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8191f46",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "This code can execute the following three variants of losses:  \n",
    "\n",
    "1. **Variant 1:**  Purely Physics  \n",
    "   $L_{\\theta} = L_{\\text{PDE}}$  \n",
    "   Use $n_{\\text{used}} = 0$  \n",
    "\n",
    "2. **Variant 2:**  Physics + Data  \n",
    "   $L_{\\theta} = L_{\\text{PDE}} + \\Sigma_{i=1}^{n_{\\text{used}}}\\| u_i - \\hat{u}_i \\|_2^2$  \n",
    "   Use $n_{\\text{used}} \\in (0, 1000]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224272da",
   "metadata": {
    "papermill": {
     "duration": 0.003707,
     "end_time": "2024-07-31T06:19:13.524446",
     "exception": false,
     "start_time": "2024-07-31T06:19:13.520739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1395ed4",
   "metadata": {
    "papermill": {
     "duration": 2.578015,
     "end_time": "2024-07-31T06:19:16.105403",
     "exception": false,
     "start_time": "2024-07-31T06:19:13.527388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "from scipy.io import loadmat\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from utils.networks import *\n",
    "from utils.deeponet_networks_1d import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.visualizer_1d import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e21cdf",
   "metadata": {
    "papermill": {
     "duration": 0.007011,
     "end_time": "2024-07-31T06:19:16.115384",
     "exception": false,
     "start_time": "2024-07-31T06:19:16.108373",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Tag this cell with 'parameters'\n",
    "# parameters\n",
    "seed = 0 # Seed number.\n",
    "n_used = 0 #100 # Number of full training fields used for estimating the data-driven loss term\n",
    "save = True # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f9ecb",
   "metadata": {
    "papermill": {
     "duration": 0.006692,
     "end_time": "2024-07-31T06:19:16.133549",
     "exception": false,
     "start_time": "2024-07-31T06:19:16.126857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','a_Vanilla-NO','seed='+str(seed)+'_n_used='+str(n_used)) \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb4cab",
   "metadata": {
    "papermill": {
     "duration": 0.007478,
     "end_time": "2024-07-31T06:19:16.143386",
     "exception": false,
     "start_time": "2024-07-31T06:19:16.135908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d11f0e",
   "metadata": {
    "papermill": {
     "duration": 0.04621,
     "end_time": "2024-07-31T06:19:16.192125",
     "exception": false,
     "start_time": "2024-07-31T06:19:16.145915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37c44a",
   "metadata": {
    "papermill": {
     "duration": 1.159964,
     "end_time": "2024-07-31T06:19:17.355131",
     "exception": false,
     "start_time": "2024-07-31T06:19:16.195167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = loadmat(os.path.join('..','..','data/1D_Burgers_transport_dynamics_t=0to1/Burger.mat')) # Load the .mat file\n",
    "#print(data)\n",
    "print(data['tspan'].shape)\n",
    "print(data['input'].shape)  # Random Initial conditions: Gaussian random fields, Nsamples x 101, each IC sample is (1 x 101)\n",
    "print(data['output'].shape) # Time evolution of the solution field: Nsamples x 101 x 101.\n",
    "                             # Each field is 101 x 101, rows correspond to time and columns respond to location.\n",
    "                             # First row corresponds to solution at t=0 (1st time step)\n",
    "                             # and next  row corresponds to solution at t=0.01 (2nd time step) and so on.\n",
    "                             # last row correspond to solution at t=1 (101th time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6609428",
   "metadata": {
    "papermill": {
     "duration": 1.54079,
     "end_time": "2024-07-31T06:19:18.898921",
     "exception": false,
     "start_time": "2024-07-31T06:19:17.358131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "inputs = torch.from_numpy(data['input'][0:1500]).float().to(device)\n",
    "outputs = torch.from_numpy(data['output'][0:1500]).float().to(device)\n",
    "\n",
    "t_span = torch.from_numpy(data['tspan'].flatten()).float().to(device)\n",
    "x_span = torch.linspace(0, 1, data['output'].shape[2]).float().to(device)\n",
    "nt, nx = len(t_span), len(x_span) # number of discretizations in time and location.\n",
    "print(\"nt =\",nt, \", nx =\",nx)\n",
    "print(\"Shape of t-span and x-span:\",t_span.shape, x_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "\n",
    "# Estimating grid points\n",
    "T, X = torch.meshgrid(t_span, x_span)\n",
    "# print(T)\n",
    "# print(X)\n",
    "grid = torch.vstack((T.flatten(), X.flatten())).T\n",
    "print(\"Shape of grid:\", grid.shape) # (nt*nx, 2)\n",
    "print(\"grid:\", grid) # time, location\n",
    "\n",
    "# Split the data into training and testing samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=500, random_state=seed)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc805cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term in the PI-Latent-NO\n",
    "inputs_train_used = inputs_train[:n_used, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae139a",
   "metadata": {
    "papermill": {
     "duration": 0.975893,
     "end_time": "2024-07-31T06:19:19.877980",
     "exception": false,
     "start_time": "2024-07-31T06:19:18.902087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_neurons_branch: Number of input neurons in the branch net.\n",
    "input_neurons_trunk: Number of input neurons in the trunk net.\n",
    "p: Number of output neurons in both the branch and trunk net.\n",
    "\"\"\"\n",
    "p = 128 # Number of output neurons in both the branch and trunk net.\n",
    "\n",
    "input_neurons_branch = nx # m\n",
    "branch_net = DenseNet(layersizes=[input_neurons_branch] + [64]*3 + [p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "branch_net.to(device)\n",
    "# print(branch_net)\n",
    "print('BRANCH-NET SUMMARY:')\n",
    "summary(branch_net, input_size=(input_neurons_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_trunk = 2 # 2 corresponds to t and x\n",
    "trunk_net = DenseNet(layersizes=[input_neurons_trunk] + [64]*3 + [p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "trunk_net.to(device)\n",
    "# print(trunk_net)\n",
    "print('TRUNK-NET SUMMARY:')\n",
    "summary(trunk_net, input_size=(input_neurons_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Vanilla_NO_model(branch_net, trunk_net)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e3c67",
   "metadata": {
    "papermill": {
     "duration": 0.007608,
     "end_time": "2024-07-31T06:19:19.888877",
     "exception": false,
     "start_time": "2024-07-31T06:19:19.881269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_learnable_parameters = count_learnable_parameters(branch_net) + count_learnable_parameters(trunk_net)\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b037bc",
   "metadata": {
    "papermill": {
     "duration": 0.008106,
     "end_time": "2024-07-31T06:19:19.899873",
     "exception": false,
     "start_time": "2024-07-31T06:19:19.891767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, initial_fields, t, x):\n",
    "    \n",
    "    u = net(initial_fields, torch.hstack([t, x])) # u is (bs, neval_c)\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device)\n",
    "    ut  = FWDAD_first_order_derivative(lambda t: net(initial_fields, torch.hstack([t, x])), t, tangent_t) # (bs, neval_c)\n",
    "    ux  = FWDAD_first_order_derivative(lambda x: net(initial_fields, torch.hstack([t, x])), x, tangent_x) # (bs, neval_c)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: net(initial_fields, torch.hstack([t, x])), x, tangent_x) # (bs, neval_c)\n",
    "    \n",
    "    pde_residual = (ut + (u*ux) - (0.01*uxx))**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48082483",
   "metadata": {
    "papermill": {
     "duration": 0.008056,
     "end_time": "2024-07-31T06:19:19.910934",
     "exception": false,
     "start_time": "2024-07-31T06:19:19.902878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_bcs(net, initial_fields, t, x):\n",
    "    \n",
    "    t_b1, x_b1 = t[0], x[0]\n",
    "    t_b2, x_b2 = t[1], x[1]\n",
    "\n",
    "    u_b1 = net(initial_fields, torch.hstack([t_b1, x_b1])) # u is (bs, neval_b)\n",
    "    u_b2 = net(initial_fields, torch.hstack([t_b2, x_b2])) # u is (bs, neval_b)\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_x_b1, tangent_x_b2 = torch.ones(x_b1.shape).to(device), torch.ones(x_b2.shape).to(device)\n",
    "    ux_b1 = FWDAD_first_order_derivative(lambda x_b1: net(initial_fields, torch.hstack([t_b1, x_b1])), x_b1, tangent_x_b1) # (bs, neval_b)\n",
    "    ux_b2 = FWDAD_first_order_derivative(lambda x_b2: net(initial_fields, torch.hstack([t_b2, x_b2])), x_b2, tangent_x_b2) # (bs, neval_b)\n",
    "\n",
    "    pde_bc1 = (u_b1 - u_b2)**2\n",
    "    pde_bc2 = (ux_b1 - ux_b2)**2\n",
    "    \n",
    "    return torch.mean(pde_bc1) + torch.mean(pde_bc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338689a3",
   "metadata": {
    "papermill": {
     "duration": 0.007493,
     "end_time": "2024-07-31T06:19:19.921323",
     "exception": false,
     "start_time": "2024-07-31T06:19:19.913830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_ic(net, initial_fields, t, x):\n",
    "    \n",
    "    u_ic = net(initial_fields, torch.hstack([t, x])) # u is (bs, neval_i)\n",
    "    \n",
    "    bs_ = initial_fields.shape[0]\n",
    "    ic_values_ = torch.zeros((bs_, x.shape[0], 1)).to(device)\n",
    "    for j in range(bs_):\n",
    "        ic_values_[j] = linear_interpolation_1D(x, x_span, initial_fields[j]) # initial condition: u_0(x) values\n",
    "    ic_values = ic_values_.reshape(-1, x.shape[0]) # (bs, neval_i)\n",
    "    \n",
    "    pde_ic = (u_ic - ic_values)**2\n",
    "    \n",
    "    return torch.mean(pde_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation_points(tc_span, xc_span, neval_dict):\n",
    "    tc = tc_span.repeat_interleave(neval_dict['loc']).unsqueeze(-1)\n",
    "    xc = xc_span.flatten().repeat(neval_dict['t']).unsqueeze(-1)\n",
    "    return tc, xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 64 # Batch size\n",
    "neval_t = 512 # Number of time points at which output field is evaluated.\n",
    "neval_x = 512\n",
    "# neval_loc = neval_x  # Number of locations at which output field is evaluated at each time point.\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x}  # Number of collocation points within the domain.\n",
    "neval_b = {'t': neval_t, 'loc': 1}        # Number of collocation points on each boundary.\n",
    "neval_i = {'t': 1, 'loc': neval_x}        # Number of collocation points at t=0.\n",
    "        \n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.5*1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "test_iteration_list, test_loss_list = [], []\n",
    "\n",
    "n_iterations = 50000\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "    else:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        inputs_train_used_batch = inputs_train_used[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used.reshape(-1, nt*nx)[indices_datadriven[0:bs]]\n",
    "        # print(f\"Shape of inputs_train_used_batch:\", inputs_train_used_batch.shape) # (bs, nx)\n",
    "        # print(f\"Shape of outputs_train_used_batch:\", outputs_train_used_batch.shape) # (bs, nt*nx)\n",
    "\n",
    "        predicted_values = model(inputs_train_used_batch, grid)  # (bs, neval) = (bs, nt*nx)\n",
    "        target_values = outputs_train_used_batch # (bs, nt*nx)\n",
    "        datadriven_loss = nn.MSELoss()(predicted_values, target_values)\n",
    "        # print('*********')\n",
    "    \n",
    "    # Shuffle the train data using the generated indices\n",
    "    num_samples = len(inputs_train)\n",
    "    indices = torch.randperm(num_samples).to(device) # Generate random permutation of indices\n",
    "    inputs_batch = inputs_train[indices[0:bs]]\n",
    "    #print(f\"Shape of inputs_train_batch:\", inputs_batch.shape) # (bs, nx)\n",
    "        \n",
    "    # points within the domain\n",
    "    tc_span = td.uniform.Uniform(0., 1.).sample((neval_c['t'], 1)).to(device)\n",
    "    xc_span = td.uniform.Uniform(0., 1.).sample((neval_c['loc'], 1)).to(device)\n",
    "    tc, xc= collocation_points(tc_span, xc_span, neval_c)\n",
    "\n",
    "    # boundary points on the 2 boundaries (hard-coded)\n",
    "    tb = [td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device),\n",
    "          td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device)]\n",
    "\n",
    "    xb = [torch.full((neval_b['t'], 1), 0.).to(device),\n",
    "          torch.full((neval_b['t'], 1), 1.).to(device)]\n",
    "\n",
    "    # initial points\n",
    "    ti = torch.full((neval_i['loc'], 1), 0.).to(device)\n",
    "    xi = td.uniform.Uniform(0., 1.).sample((neval_i['loc'], 1)).to(device)\n",
    "    \n",
    "    pinn_loss = (loss_pde_residual(model, inputs_batch, tc, xc) \n",
    "           + loss_pde_bcs(model, inputs_batch, tb, xb) \n",
    "           + loss_pde_ic(model, inputs_batch, ti, xi))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if iteration % 500 == 0:\n",
    "        # Test loss calculation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_predicted_values = model(inputs_test, grid)  # (bs, neval) = (bs, nt*nx)\n",
    "            test_loss = nn.MSELoss()(test_predicted_values, outputs_test.reshape(-1, nt*nx))\n",
    "            test_iteration_list.append(iteration)\n",
    "            test_loss_list.append(test_loss.item())  \n",
    "        model.train()  # Switch back to training mode\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f,' % optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "              'test loss = %f' % test_loss)\n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "\n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "    np.save(os.path.join(resultdir,'test_iteration_list.npy'), np.asarray(test_iteration_list))\n",
    "    np.save(os.path.join(resultdir, 'test_loss_list.npy'), np.asarray(test_loss_list)) \n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "\n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save)\n",
    "\n",
    "plot_testing_loss(resultdir, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_training_testing_loss(resultdir, iteration_list, loss_list, test_iteration_list, test_loss_list, save)\n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)\n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time # in sec\n",
    " \n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67807e7",
   "metadata": {
    "papermill": {
     "duration": 0.011998,
     "end_time": "2024-07-31T08:53:01.041169",
     "exception": false,
     "start_time": "2024-07-31T08:53:01.029171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir,'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47234a6a",
   "metadata": {
    "papermill": {
     "duration": 56.15264,
     "end_time": "2024-07-31T08:53:57.198814",
     "exception": false,
     "start_time": "2024-07-31T08:53:01.046174",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "branch_inputs = inputs_test # (bs, m) = (bs, nx) \n",
    "trunk_inputs = grid # (neval, 2) = (nt*nx, 2)\n",
    "predictions_test = model(branch_inputs, trunk_inputs) # (bs, neval) = (bs, nt*nx)\n",
    "\n",
    "mse_list, r2score_list, relerror_list = [], [], []\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "\n",
    "    prediction_i = predictions_test[i].reshape(1, -1) # (1, nt*nx)\n",
    "    target_i = outputs_test[i].reshape(1, -1) # (1, nt*nx)\n",
    "    \n",
    "    mse_i = F.mse_loss(prediction_i.cpu(), target_i.cpu())\n",
    "    r2score_i = metrics.r2_score(target_i.flatten().cpu().detach().numpy(), prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(target_i.flatten().cpu().detach().numpy() - prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(target_i.flatten().cpu().detach().numpy())\n",
    "    \n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item())\n",
    "    \n",
    "    if (i+1) % 10 == 0:\n",
    "        plot_predictions(i, resultdir, target_i, prediction_i, x_span, inputs_test, X, T, nt, nx, r'$u_0(x)$', 'Initial field', 'jet', save)\n",
    "\n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ca675",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": predictions_test.reshape(-1, nt, nx).cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965287e4",
   "metadata": {
    "papermill": {
     "duration": 0.066074,
     "end_time": "2024-07-31T08:53:57.320620",
     "exception": false,
     "start_time": "2024-07-31T08:53:57.254546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0834f",
   "metadata": {
    "papermill": {
     "duration": 0.056255,
     "end_time": "2024-07-31T08:53:57.437040",
     "exception": false,
     "start_time": "2024-07-31T08:53:57.380785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c7e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9286.126422,
   "end_time": "2024-07-31T08:53:58.709644",
   "environment_variables": {},
   "exception": null,
   "input_path": "a_PI-Vanilla-NO.ipynb",
   "output_path": "results/a_PI-Vanilla-NO/seed=0/output_seed=0.ipynb",
   "parameters": {
    "seed": 0
   },
   "start_time": "2024-07-31T06:19:12.583222",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
