{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08a75ad",
   "metadata": {
    "papermill": {
     "duration": 0.003438,
     "end_time": "2024-07-31T06:19:27.862504",
     "exception": false,
     "start_time": "2024-07-31T06:19:27.859066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All the results presented were obtained as follows:\n",
    "1. By estimating the gradients in the physics-informed loss terms using forward mode automatic differentiation (AD).\n",
    "2. The output field values at given grid points were computed in one forward pass of the network using the einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1395ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:27.869437Z",
     "iopub.status.busy": "2024-07-31T06:19:27.869137Z",
     "iopub.status.idle": "2024-07-31T06:19:30.862739Z",
     "shell.execute_reply": "2024-07-31T06:19:30.862148Z"
    },
    "papermill": {
     "duration": 2.998327,
     "end_time": "2024-07-31T06:19:30.864011",
     "exception": false,
     "start_time": "2024-07-31T06:19:27.865684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.distributions as td\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import time \n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from utils.networks import *\n",
    "from utils.deeponet_networks_1d import *\n",
    "from utils.visualizer_misc import *\n",
    "from utils.visualizer_1d import *\n",
    "from utils.forward_autodiff import *\n",
    "from utils.misc import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921aabe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:30.871093Z",
     "iopub.status.busy": "2024-07-31T06:19:30.870812Z",
     "iopub.status.idle": "2024-07-31T06:19:30.873294Z",
     "shell.execute_reply": "2024-07-31T06:19:30.872891Z"
    },
    "papermill": {
     "duration": 0.006868,
     "end_time": "2024-07-31T06:19:30.874111",
     "exception": false,
     "start_time": "2024-07-31T06:19:30.867243",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Tag this cell with 'parameters'\n",
    "# parameters\n",
    "seed = 0 # Seed number.\n",
    "n_used = 200 # Number of full training fields used for estimating the data-driven loss term in the PI-Latent-NO\n",
    "save = True # Save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:30.889618Z",
     "iopub.status.busy": "2024-07-31T06:19:30.889382Z",
     "iopub.status.idle": "2024-07-31T06:19:30.891989Z",
     "shell.execute_reply": "2024-07-31T06:19:30.891598Z"
    },
    "papermill": {
     "duration": 0.006717,
     "end_time": "2024-07-31T06:19:30.892767",
     "exception": false,
     "start_time": "2024-07-31T06:19:30.886050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    resultdir = os.path.join(os.getcwd(),'results','b_PI-Latent-NO_with-PCA','seed='+str(seed)+'_n_used='+str(n_used)) \n",
    "    if not os.path.exists(resultdir):\n",
    "        os.makedirs(resultdir)\n",
    "else:\n",
    "    resultdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7126e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:30.899226Z",
     "iopub.status.busy": "2024-07-31T06:19:30.898992Z",
     "iopub.status.idle": "2024-07-31T06:19:30.902283Z",
     "shell.execute_reply": "2024-07-31T06:19:30.901860Z"
    },
    "papermill": {
     "duration": 0.007407,
     "end_time": "2024-07-31T06:19:30.903088",
     "exception": false,
     "start_time": "2024-07-31T06:19:30.895681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d11f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:30.909684Z",
     "iopub.status.busy": "2024-07-31T06:19:30.909445Z",
     "iopub.status.idle": "2024-07-31T06:19:30.948119Z",
     "shell.execute_reply": "2024-07-31T06:19:30.947683Z"
    },
    "papermill": {
     "duration": 0.042944,
     "end_time": "2024-07-31T06:19:30.948954",
     "exception": false,
     "start_time": "2024-07-31T06:19:30.906010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346e7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:30.955754Z",
     "iopub.status.busy": "2024-07-31T06:19:30.955487Z",
     "iopub.status.idle": "2024-07-31T06:19:32.101210Z",
     "shell.execute_reply": "2024-07-31T06:19:32.100667Z"
    },
    "papermill": {
     "duration": 1.150092,
     "end_time": "2024-07-31T06:19:32.102088",
     "exception": false,
     "start_time": "2024-07-31T06:19:30.951996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = loadmat(os.path.join('..','..','data/Burgers_transport_dynamics_t=0to1/Burger.mat')) # Load the .mat file\n",
    "#print(data)\n",
    "print(data['tspan'].shape)\n",
    "print(data['input'].shape)  # Random Initial conditions: Gaussian random fields, Nsamples x 101, each IC sample is (1 x 101)\n",
    "print(data['output'].shape) # Time evolution of the solution field: Nsamples x 101 x 101.\n",
    "                             # Each field is 101 x 101, rows correspond to time and columns respond to location.\n",
    "                             # First row corresponds to solution at t=0 (1st time step)\n",
    "                             # and next  row corresponds to solution at t=0.01 (2nd time step) and so on.\n",
    "                             # last row correspond to solution at t=1 (101th time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18da80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:32.109446Z",
     "iopub.status.busy": "2024-07-31T06:19:32.109138Z",
     "iopub.status.idle": "2024-07-31T06:19:34.831605Z",
     "shell.execute_reply": "2024-07-31T06:19:34.827252Z"
    },
    "papermill": {
     "duration": 2.727233,
     "end_time": "2024-07-31T06:19:34.832632",
     "exception": false,
     "start_time": "2024-07-31T06:19:32.105399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "inputs = torch.from_numpy(data['input']).float().to(device)\n",
    "outputs = torch.from_numpy(data['output']).float().to(device)\n",
    "\n",
    "t_span = torch.from_numpy(data['tspan'].flatten()).float().to(device)\n",
    "x_span = torch.linspace(0, 1, data['output'].shape[2]).float().to(device)\n",
    "nt, nx = len(t_span), len(x_span) # number of discretizations in time and location.\n",
    "print(\"nt =\",nt, \", nx =\",nx)\n",
    "print(\"Shape of t-span and x-span:\",t_span.shape, x_span.shape)\n",
    "print(\"t-span:\", t_span)\n",
    "print(\"x-span:\", x_span)\n",
    "\n",
    "# Estimating grid points\n",
    "T, X = torch.meshgrid(t_span, x_span)\n",
    "# print(T)\n",
    "# print(X)\n",
    "\n",
    "# Split the data into training and testing samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=500, random_state=seed)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)\n",
    "print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca370b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:34.840609Z",
     "iopub.status.busy": "2024-07-31T06:19:34.840373Z",
     "iopub.status.idle": "2024-07-31T06:19:34.849969Z",
     "shell.execute_reply": "2024-07-31T06:19:34.849628Z"
    },
    "papermill": {
     "duration": 0.014497,
     "end_time": "2024-07-31T06:19:34.850730",
     "exception": false,
     "start_time": "2024-07-31T06:19:34.836233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Of these full training fields available I am using only n_used fields for estimating the data-driven loss term in the PI-Latent-NO\n",
    "inputs_train_used = inputs_train[:n_used, :]\n",
    "print(\"Shape of inputs_train_used:\", inputs_train_used.shape)\n",
    "outputs_train_used = outputs_train[:n_used, :, :]\n",
    "print(\"Shape of outputs_train_used:\", outputs_train_used.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cfaf0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:34.857660Z",
     "iopub.status.busy": "2024-07-31T06:19:34.857516Z",
     "iopub.status.idle": "2024-07-31T06:19:42.701217Z",
     "shell.execute_reply": "2024-07-31T06:19:42.700733Z"
    },
    "papermill": {
     "duration": 7.848367,
     "end_time": "2024-07-31T06:19:42.702192",
     "exception": false,
     "start_time": "2024-07-31T06:19:34.853825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if n_used > 0:\n",
    "    # Learning latent fields using PCA\n",
    "\n",
    "    outputs_train_, outputs_test_ = outputs_train.reshape(-1, nx), outputs_test.reshape(-1, nx) \n",
    "    print(\"Shape of outputs_train_ and outputs_test_:\", outputs_train_.shape, outputs_test_.shape)\n",
    "    outputs_train_used_ = outputs_train_used.reshape(-1, nx) \n",
    "    print(\"Shape of outputs_train_used_:\", outputs_train_used_.shape)\n",
    "\n",
    "    data_used_for_PCA = torch.vstack((inputs_train.cpu(), outputs_train_used_.cpu())) # Move to CPU for PCA\n",
    "    print(\"Shape of data_used_for_PCA:\", data_used_for_PCA.shape)\n",
    "\n",
    "    # Preprocess the dataset for doing PCA\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data_used_for_PCA.numpy())\n",
    "    outputs_s_train_ = torch.from_numpy(scaler.transform(outputs_train_.cpu().numpy())).float().to(device)\n",
    "    outputs_s_train_used_ = torch.from_numpy(scaler.transform(outputs_train_used_.cpu().numpy())).float().to(device)\n",
    "    outputs_s_test_ = torch.from_numpy(scaler.transform(outputs_test_.cpu().numpy())).float().to(device)\n",
    "    inputs_s_train = torch.from_numpy(scaler.transform(inputs_train.cpu().numpy())).float().to(device)\n",
    "    inputs_s_train_used = torch.from_numpy(scaler.transform(inputs_train_used.cpu().numpy())).float().to(device)\n",
    "    inputs_s_test = torch.from_numpy(scaler.transform(inputs_test.cpu().numpy())).float().to(device)\n",
    "    if save == True:\n",
    "        pickle.dump(scaler, open(os.path.join(resultdir,'scaler.pkl'), 'wb')) # saving scaler\n",
    "\n",
    "    # Perform PCA on the Preprocessed dataset\n",
    "    nums = np.arange(nx+1)\n",
    "    var_ratio = []\n",
    "    for num in nums:\n",
    "        pca = PCA(n_components=num)\n",
    "        pca.fit(data_used_for_PCA.numpy())\n",
    "        var_ratio.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "    threshold=0.99\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(nums, var_ratio,marker='o')\n",
    "    plt.axhline(threshold, color='r', linestyle='--')\n",
    "    plt.xticks(nums, rotation = 270)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('n-components')\n",
    "    plt.ylabel('explained variance ratio')\n",
    "    plt.title('n-components vs. explained variance ratio')\n",
    "    plt.tight_layout()\n",
    "    if save == True:\n",
    "        plt.savefig(os.path.join(resultdir,'explained_variance_plot.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1226e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:42.711272Z",
     "iopub.status.busy": "2024-07-31T06:19:42.711050Z",
     "iopub.status.idle": "2024-07-31T06:19:42.868970Z",
     "shell.execute_reply": "2024-07-31T06:19:42.868618Z"
    },
    "papermill": {
     "duration": 0.163262,
     "end_time": "2024-07-31T06:19:42.869800",
     "exception": false,
     "start_time": "2024-07-31T06:19:42.706538",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluating latent data\n",
    "latent_dim = 9 # d_z\n",
    "\n",
    "if n_used > 0:\n",
    "    pca = PCA(n_components=latent_dim)\n",
    "    pca.fit(data_used_for_PCA.numpy())\n",
    "    latent_outputs_train = torch.from_numpy(pca.transform(outputs_s_train_.cpu().numpy())).float().to(device).reshape(-1, nt, latent_dim)\n",
    "    latent_outputs_train_used = torch.from_numpy(pca.transform(outputs_s_train_used_.cpu().numpy())).float().to(device).reshape(-1, nt, latent_dim)\n",
    "    latent_outputs_test = torch.from_numpy(pca.transform(outputs_s_test_.cpu().numpy())).float().to(device).reshape(-1, nt, latent_dim)\n",
    "    latent_inputs_train = torch.from_numpy(pca.transform(inputs_s_train.cpu().numpy())).float().to(device)\n",
    "    latent_inputs_train_used = torch.from_numpy(pca.transform(inputs_s_train_used.cpu().numpy())).float().to(device)\n",
    "    latent_inputs_test = torch.from_numpy(pca.transform(inputs_s_test.cpu().numpy())).float().to(device)\n",
    "    print(\"Principal component vectors (eigenvectors):\", pca.components_.shape, \"\\n\", pca.components_)\n",
    "    print(\"Fraction of total variance captured by each principal component:\", pca.explained_variance_ratio_) \n",
    "    print(f\"Total variance captured by {pca.n_components} principal components:\", sum(pca.explained_variance_ratio_)) \n",
    "    print(\"Number of components selected:\", pca.n_components)\n",
    "    if save == True:\n",
    "        pickle.dump(pca, open(os.path.join(resultdir,'pca_model.pkl'), 'wb')) # saving pca model\n",
    "\n",
    "    # Check the shapes of the subsets\n",
    "    print(\"Shape of latent_inputs_train:\", latent_inputs_train.shape)\n",
    "    print(\"Shape of latent_inputs_train_used:\", latent_inputs_train_used.shape)\n",
    "    print(\"Shape of latent_inputs_test:\", latent_inputs_test.shape)\n",
    "    print(\"Shape of latent_outputs_train:\", latent_outputs_train.shape)\n",
    "    print(\"Shape of latent_outputs_train_used:\", latent_outputs_train_used.shape)\n",
    "    print(\"Shape of latent_outputs_test:\", latent_outputs_test.shape)\n",
    "    print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5719c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:42.878655Z",
     "iopub.status.busy": "2024-07-31T06:19:42.878494Z",
     "iopub.status.idle": "2024-07-31T06:19:44.100979Z",
     "shell.execute_reply": "2024-07-31T06:19:44.100457Z"
    },
    "papermill": {
     "duration": 1.22795,
     "end_time": "2024-07-31T06:19:44.101897",
     "exception": false,
     "start_time": "2024-07-31T06:19:42.873947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_neurons_latent_branch: Number of input neurons in the latent_branch net.\n",
    "input_neurons_latent_trunk: Number of input neurons in the latent_trunk net.\n",
    "latent_p: Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\"\"\"\n",
    "latent_p = latent_dim*16 # Number of output neurons in both the latent_branch and latent_trunk net.\n",
    "\n",
    "input_neurons_latent_branch = latent_dim # m\n",
    "latent_branch_net = DenseNet(layersizes=[input_neurons_latent_branch] + [64]*3 + [latent_p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_branch_net.to(device)\n",
    "# print(latent_branch_net)\n",
    "print('LATENT BRANCH-NET SUMMARY:')\n",
    "summary(latent_branch_net, input_size=(input_neurons_latent_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_latent_trunk = 1 # 1 corresponds to t\n",
    "latent_trunk_net = DenseNet(layersizes=[input_neurons_latent_trunk] + [64]*3 + [latent_p], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "latent_trunk_net.to(device)\n",
    "# print(latent_trunk_net)\n",
    "print('LATENT TRUNK-NET SUMMARY:')\n",
    "summary(latent_trunk_net, input_size=(input_neurons_latent_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "\"\"\"\n",
    "input_neurons_reconstruction_branch: Number of input neurons in the reconstruction_branch net.\n",
    "input_neurons_reconstruction_trunk: Number of input neurons in the reconstruction_trunk net.\n",
    "reconstruction_q: Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\"\"\"\n",
    "reconstruction_q = 128 # Number of output neurons in both the reconstruction_branch and reconstruction_trunk net.\n",
    "\n",
    "input_neurons_reconstruction_branch = latent_dim # d_z\n",
    "reconstruction_branch_net = DenseNet(layersizes=[input_neurons_reconstruction_branch] + [64]*3 + [reconstruction_q], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_branch_net.to(device)\n",
    "# print(reconstruction_branch_net)\n",
    "print('RECONSTRUCTION BRANCH-NET SUMMARY:')\n",
    "summary(reconstruction_branch_net, input_size=(input_neurons_reconstruction_branch,))  \n",
    "print('#'*100)\n",
    "\n",
    "input_neurons_reconstruction_trunk = 1 # 1 corresponds to x\n",
    "reconstruction_trunk_net = DenseNet(layersizes=[input_neurons_reconstruction_trunk] + [64]*3 + [reconstruction_q], activation=nn.SiLU()) #nn.LeakyReLU() #nn.Tanh()\n",
    "reconstruction_trunk_net.to(device)\n",
    "# print(reconstruction_trunk_net)\n",
    "print('RECONSTRUCTION TRUNK-NET SUMMARY:')\n",
    "summary(reconstruction_trunk_net, input_size=(input_neurons_reconstruction_trunk,))\n",
    "print('#'*100)\n",
    "\n",
    "model = Latent_NO_model(latent_branch_net, latent_trunk_net, latent_dim, reconstruction_branch_net, reconstruction_trunk_net)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86a175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:44.111529Z",
     "iopub.status.busy": "2024-07-31T06:19:44.110989Z",
     "iopub.status.idle": "2024-07-31T06:19:44.114253Z",
     "shell.execute_reply": "2024-07-31T06:19:44.113830Z"
    },
    "papermill": {
     "duration": 0.008676,
     "end_time": "2024-07-31T06:19:44.115094",
     "exception": false,
     "start_time": "2024-07-31T06:19:44.106418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_learnable_parameters = (count_learnable_parameters(latent_branch_net)\n",
    "                            + count_learnable_parameters(latent_trunk_net)\n",
    "                            + count_learnable_parameters(reconstruction_branch_net)\n",
    "                            + count_learnable_parameters(reconstruction_trunk_net))\n",
    "print(\"Total number of learnable parameters:\", num_learnable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7510cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:44.123711Z",
     "iopub.status.busy": "2024-07-31T06:19:44.123357Z",
     "iopub.status.idle": "2024-07-31T06:19:44.127043Z",
     "shell.execute_reply": "2024-07-31T06:19:44.126651Z"
    },
    "papermill": {
     "duration": 0.00884,
     "end_time": "2024-07-31T06:19:44.127831",
     "exception": false,
     "start_time": "2024-07-31T06:19:44.118991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_residual(net, latent_initial_fields, t, x):\n",
    "    \n",
    "    _, u = net(latent_initial_fields, t, x) # u is (bs, neval_t, neval_x)\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_t, tangent_x = torch.ones(t.shape).to(device), torch.ones(x.shape).to(device)\n",
    "    ut  = FWDAD_first_order_derivative(lambda t: net(latent_initial_fields, t, x)[1], t, tangent_t) # (bs, neval_t, neval_x)\n",
    "    ux  = FWDAD_first_order_derivative(lambda x: net(latent_initial_fields, t, x)[1], x, tangent_x) # (bs, neval_t, neval_x)\n",
    "    uxx = FWDAD_second_order_derivative(lambda x: net(latent_initial_fields, t, x)[1], x, tangent_x) # (bs, neval_t, neval_x)\n",
    "    \n",
    "    pde_residual = (ut + (u*ux) - (0.01*uxx))**2\n",
    "    \n",
    "    return torch.mean(pde_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0afa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:44.136299Z",
     "iopub.status.busy": "2024-07-31T06:19:44.135976Z",
     "iopub.status.idle": "2024-07-31T06:19:44.139673Z",
     "shell.execute_reply": "2024-07-31T06:19:44.139289Z"
    },
    "papermill": {
     "duration": 0.008753,
     "end_time": "2024-07-31T06:19:44.140419",
     "exception": false,
     "start_time": "2024-07-31T06:19:44.131666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_bcs(net, latent_initial_fields, t, x):\n",
    "    \n",
    "    t_b1, x_b1 = t[0], x[0]\n",
    "    t_b2, x_b2 = t[1], x[1]\n",
    "\n",
    "    _, u_b1 = net(latent_initial_fields, t_b1, x_b1) # u is (bs, neval_t, 1)\n",
    "    _, u_b2 = net(latent_initial_fields, t_b2, x_b2) # u is (bs, neval_t, 1)\n",
    "    \n",
    "    # Using forward automatic differention to estimate derivatives in the physics informed loss\n",
    "    tangent_x_b1, tangent_x_b2 = torch.ones(x_b1.shape).to(device), torch.ones(x_b2.shape).to(device)\n",
    "    ux_b1 = FWDAD_first_order_derivative(lambda x_b1: net(latent_initial_fields, t_b1, x_b1)[1], x_b1, tangent_x_b1) # (bs, neval_t, 1)\n",
    "    ux_b2 = FWDAD_first_order_derivative(lambda x_b2: net(latent_initial_fields, t_b2, x_b2)[1], x_b2, tangent_x_b2) # (bs, neval_t, 1)\n",
    "\n",
    "    pde_bc1 = (u_b1 - u_b2)**2\n",
    "    pde_bc2 = (ux_b1 - ux_b2)**2\n",
    "    \n",
    "    return torch.mean(pde_bc1) + torch.mean(pde_bc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48197e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T06:19:44.148923Z",
     "iopub.status.busy": "2024-07-31T06:19:44.148590Z",
     "iopub.status.idle": "2024-07-31T06:19:44.151780Z",
     "shell.execute_reply": "2024-07-31T06:19:44.151395Z"
    },
    "papermill": {
     "duration": 0.008295,
     "end_time": "2024-07-31T06:19:44.152558",
     "exception": false,
     "start_time": "2024-07-31T06:19:44.144263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_pde_ic(net, latent_initial_fields, t, x, initial_fields):\n",
    "\n",
    "    _, u_ic = net(latent_initial_fields, t, x) # u is (bs, 1, neval_x)\n",
    "    \n",
    "    bs_ = latent_initial_fields.shape[0]\n",
    "    ic_values_ = torch.zeros((bs_, x.shape[0], 1)).to(device)\n",
    "    for j in range(bs_):\n",
    "        ic_values_[j] = linear_interpolation(x, x_span, initial_fields[j]) # initial condition: u_0(x) values\n",
    "    ic_values = ic_values_.reshape(-1, 1, x.shape[0]) # (bs, 1, neval_x)\n",
    "    \n",
    "    pde_ic = (u_ic - ic_values)**2\n",
    "    \n",
    "    return torch.mean(pde_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bs = 64 # Batch size\n",
    "neval_t = 16   # Number of time points at which latent output field is evaluated for a given input latent initial field sample\n",
    "neval_x = 16 \n",
    "# neval_loc = neval_x  # Number of locations at which output field is evaluated at each time point.\n",
    "\n",
    "neval_c = {'t': neval_t, 'loc': neval_x}  # Number of collocation points within the domain.\n",
    "neval_b = {'t': neval_t, 'loc': 1}        # Number of collocation points on each boundary.\n",
    "neval_i = {'t': 1, 'loc': neval_x}        # Number of collocation points at t=0.\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40000, gamma=0.1) # gamma=0.8\n",
    "\n",
    "iteration_list, loss_list, learningrates_list = [], [], []\n",
    "datadriven_loss_list, pinn_loss_list = [], []\n",
    "\n",
    "n_iterations = 80000\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    if n_used > 0:\n",
    "        indices_datadriven = torch.randperm(n_used).to(device) # Generate random permutation of indices\n",
    "        latent_inputs_train_used_batch = latent_inputs_train_used[indices_datadriven[0:bs]]\n",
    "        latent_outputs_train_used_batch = latent_outputs_train_used[indices_datadriven[0:bs]]\n",
    "        outputs_train_used_batch = outputs_train_used[indices_datadriven[0:bs]]\n",
    "        #print(\"Shape of latent_inputs_train_used_batch:\", latent_inputs_train_used_batch.shape) # (bs, latent_dim)\n",
    "        #print(\"Shape of latent_outputs_train_used_batch:\", latent_outputs_train_used_batch.shape) # (bs, nt, latent_dim)\n",
    "        #print(\"Shape of outputs_train_used_batch:\", outputs_train_used_batch.shape) # (bs, nt, nx)\n",
    "        \n",
    "        latent_predicted_values, reconstruction_predicted_values = model(latent_inputs_train_used_batch, t_span.reshape(-1, 1), x_span.reshape(-1, 1)) # (bs, nt, latent_dim), (bs, nt, nx)\n",
    "        latent_target_values = latent_outputs_train_used_batch # (bs nt, latent_dim)\n",
    "        reconstruction_target_values = outputs_train_used_batch # (bs, nt, nx)\n",
    "        datadriven_loss = nn.MSELoss()(latent_predicted_values, latent_target_values) + nn.MSELoss()(reconstruction_predicted_values, reconstruction_target_values)\n",
    "        # print('*********')\n",
    "    elif n_used == 0:\n",
    "        datadriven_loss = torch.tensor([0.]).to(device)\n",
    "        # print('*********')\n",
    "    \n",
    "    indices_pinn = torch.randperm(len(inputs_train)).to(device) # Generate random permutation of indices\n",
    "    latent_inputs_batch = latent_inputs_train[indices_pinn[0:bs]]\n",
    "    inputs_batch = inputs_train[indices_pinn[0:bs]]\n",
    "    #print(f\"Shape of latent_inputs_train_batch:\", latent_inputs_batch.shape) # (bs, latent_dim)\n",
    "    #print(f\"Shape of inputs_train_batch:\", inputs_batch.shape) # (bs, nx)\n",
    "    \n",
    "    # points within the domain\n",
    "    tc = td.uniform.Uniform(0., 1.).sample((neval_c['t'], 1)).to(device)\n",
    "    xc = td.uniform.Uniform(0., 1.).sample((neval_c['loc'], 1)).to(device)\n",
    "\n",
    "    # boundary points on the 2 boundaries (hard-coded)\n",
    "    tb = [td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device),\n",
    "          td.uniform.Uniform(0., 1.).sample((neval_b['t'], 1)).to(device)]\n",
    "    xb = [torch.tensor([[0.]]).to(device), \n",
    "          torch.tensor([[1.]]).to(device)]\n",
    "\n",
    "    # initial points\n",
    "    ti = torch.zeros((1, 1)).to(device)\n",
    "    xi = td.uniform.Uniform(0., 1.).sample((neval_i['loc'], 1)).to(device)\n",
    "\n",
    "    pinn_loss = (loss_pde_residual(model, latent_inputs_batch, tc, xc) \n",
    "               + loss_pde_bcs(model, latent_inputs_batch, tb, xb) \n",
    "               + loss_pde_ic(model, latent_inputs_batch, ti, xi, inputs_batch))\n",
    "    # print('*********')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = datadriven_loss + pinn_loss\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if iteration % 500 == 0:\n",
    "        print('Iteration %s -' % iteration, 'loss = %f,' % loss,\n",
    "              'data-driven loss = %f,' % datadriven_loss,'pinn loss = %f,' % pinn_loss,\n",
    "              'learning rate = %f' % optimizer.state_dict()['param_groups'][0]['lr']) \n",
    "\n",
    "    iteration_list.append(iteration)\n",
    "    loss_list.append(loss.item())\n",
    "    datadriven_loss_list.append(datadriven_loss.item())\n",
    "    pinn_loss_list.append(pinn_loss.item())\n",
    "    learningrates_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    \n",
    "if save == True:\n",
    "    np.save(os.path.join(resultdir,'iteration_list.npy'), np.asarray(iteration_list))\n",
    "    np.save(os.path.join(resultdir,'loss_list.npy'), np.asarray(loss_list))\n",
    "    np.save(os.path.join(resultdir, 'datadriven_loss_list.npy'), np.asarray(datadriven_loss_list))\n",
    "    np.save(os.path.join(resultdir, 'pinn_loss_list.npy'), np.asarray(pinn_loss_list))\n",
    "    np.save(os.path.join(resultdir,'learningrates_list.npy'), np.asarray(learningrates_list))\n",
    "\n",
    "plot_loss_terms(resultdir, iteration_list, loss_list, datadriven_loss_list, pinn_loss_list, save)  \n",
    "    \n",
    "plot_training_loss(resultdir, iteration_list, loss_list, save)  \n",
    "\n",
    "plot_learningrates(resultdir, iteration_list, learningrates_list, save)  \n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "runtime_per_iter = training_time/n_iterations # in sec/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d2881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T10:16:35.881880Z",
     "iopub.status.busy": "2024-07-31T10:16:35.881555Z",
     "iopub.status.idle": "2024-07-31T10:16:35.887855Z",
     "shell.execute_reply": "2024-07-31T10:16:35.887523Z"
    },
    "papermill": {
     "duration": 0.015278,
     "end_time": "2024-07-31T10:16:35.888653",
     "exception": false,
     "start_time": "2024-07-31T10:16:35.873375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save == True:\n",
    "    torch.save(model.state_dict(), os.path.join(resultdir,'model_state_dict.pt'))\n",
    "# model.load_state_dict(torch.load(os.path.join(resultdir,'model_state_dict.pt'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c742e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T10:16:35.903218Z",
     "iopub.status.busy": "2024-07-31T10:16:35.902942Z",
     "iopub.status.idle": "2024-07-31T10:17:31.390797Z",
     "shell.execute_reply": "2024-07-31T10:17:31.390437Z"
    },
    "papermill": {
     "duration": 55.49624,
     "end_time": "2024-07-31T10:17:31.391867",
     "exception": false,
     "start_time": "2024-07-31T10:16:35.895627",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "latent_branch_inputs = latent_inputs_test # (bs, m) = (bs, latent_dim)\n",
    "latent_trunk_inputs = t_span.reshape(-1, 1) # (nt, 1)\n",
    "reconstruction_trunk_inputs = x_span.reshape(-1, 1) # (nx, 1)\n",
    "latent_predictions_test, reconstruction_predictions_test = model(latent_branch_inputs, latent_trunk_inputs, reconstruction_trunk_inputs)# (bs, nt, latent_dim), (bs, nt, nx)\n",
    "# print(latent_predictions_test.shape, reconstruction_predictions_test.shape)\n",
    "\n",
    "mse_list, latent_mse_list, reconstruction_mse_list, r2score_list, relerror_list = [], [], [], [], []\n",
    "\n",
    "for i in range(inputs_test.shape[0]):\n",
    "\n",
    "    latent_prediction_i, reconstruction_prediction_i = latent_predictions_test[i].unsqueeze(0), reconstruction_predictions_test[i].unsqueeze(0)# (1, nt, latent_dim), (1, nt, nx)\n",
    "    \n",
    "    if n_used > 0:\n",
    "        latent_target_i = latent_outputs_test[i].unsqueeze(0) # (1, nt, latent_dim)\n",
    "    reconstruction_target_i = outputs_test[i].unsqueeze(0) # (1, nt, nx)\n",
    "    \n",
    "    if n_used > 0:\n",
    "        latent_mse_i = F.mse_loss(latent_prediction_i.cpu(), latent_target_i.cpu())\n",
    "    elif n_used == 0:\n",
    "        latent_mse_i = torch.tensor([0.])\n",
    "    reconstruction_mse_i = F.mse_loss(reconstruction_prediction_i.cpu(), reconstruction_target_i.cpu())\n",
    "    mse_i = latent_mse_i + reconstruction_mse_i\n",
    "    \n",
    "    r2score_i = metrics.r2_score(reconstruction_target_i.flatten().cpu().detach().numpy(), reconstruction_prediction_i.flatten().cpu().detach().numpy()) \n",
    "    relerror_i = np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy() - reconstruction_prediction_i.flatten().cpu().detach().numpy()) / np.linalg.norm(reconstruction_target_i.flatten().cpu().detach().numpy())\n",
    "        \n",
    "    latent_mse_list.append(latent_mse_i.item())\n",
    "    reconstruction_mse_list.append(reconstruction_mse_i.item())\n",
    "    mse_list.append(mse_i.item())\n",
    "    r2score_list.append(r2score_i.item())\n",
    "    relerror_list.append(relerror_i.item()) \n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        plot_predictions(i, resultdir, reconstruction_target_i, reconstruction_prediction_i, x_span, inputs_test, X, T, nt, nx, r'$u_0(x)$', 'Initial field', 'jet', save)\n",
    "        \n",
    "latent_mse = sum(latent_mse_list) / len(latent_mse_list)\n",
    "print(\"Latent Mean Squared Error Test:\\n\", latent_mse)\n",
    "reconstruction_mse = sum(reconstruction_mse_list) / len(reconstruction_mse_list)\n",
    "print(\"Reconstruction Mean Squared Error Test:\\n\", reconstruction_mse)\n",
    "mse = sum(mse_list) / len(mse_list)\n",
    "print(\"Mean Squared Error Test:\\n\", mse)\n",
    "r2score = sum(r2score_list) / len(r2score_list)\n",
    "print(\"R2 score Test:\\n\", r2score)\n",
    "relerror = sum(relerror_list) / len(relerror_list)\n",
    "print(\"Rel. L2 Error Test:\\n\", relerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d3413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T10:17:31.504626Z",
     "iopub.status.busy": "2024-07-31T10:17:31.504415Z",
     "iopub.status.idle": "2024-07-31T10:17:40.804046Z",
     "shell.execute_reply": "2024-07-31T10:17:40.803678Z"
    },
    "papermill": {
     "duration": 9.357226,
     "end_time": "2024-07-31T10:17:40.804868",
     "exception": false,
     "start_time": "2024-07-31T10:17:31.447642",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if n_used > 0:\n",
    "    # Plotting learned latent fields\n",
    "\n",
    "    z_span = torch.arange(1, latent_dim+1, 1).float().to(device)\n",
    "    # Estimating grid points\n",
    "    T_z, Z = torch.meshgrid(t_span, z_span)\n",
    "    # print(Z.shape, T_z.shape)\n",
    "    # print(T_z)\n",
    "    # print(Z)\n",
    "\n",
    "    for i in range(inputs_test.shape[0]):\n",
    "\n",
    "        latent_prediction_i = latent_predictions_test[i].unsqueeze(0) # (1, nt, latent_dim)\n",
    "        latent_target_i = latent_outputs_test[i].unsqueeze(0) # (1, nt, latent_dim)\n",
    "        reconstruction_target_i = outputs_test[i].unsqueeze(0) # (1, nt, nx)\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            plot_latentfields(i, resultdir, reconstruction_target_i, latent_target_i, latent_prediction_i, x_span, inputs_test, z_span, X, T, Z, T_z, nt, nx, latent_dim, r'$u_0(x)$', 'Initial field', 'True latent field from PCA', 'jet', save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    \"inputs_test\": inputs_test.cpu(),\n",
    "    \"latent_inputs_test\": latent_inputs_test.cpu(),\n",
    "    \"outputs_test\": outputs_test.cpu(),\n",
    "    \"predictions_test\": reconstruction_predictions_test.cpu()\n",
    "}\n",
    "for key, value in test_dict.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "print(colored('#'*230, 'green'))\n",
    "\n",
    "if save == True:\n",
    "    torch.save(test_dict, os.path.join(resultdir,'test_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6d8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T10:17:40.930040Z",
     "iopub.status.busy": "2024-07-31T10:17:40.929808Z",
     "iopub.status.idle": "2024-07-31T10:17:40.932672Z",
     "shell.execute_reply": "2024-07-31T10:17:40.932335Z"
    },
    "papermill": {
     "duration": 0.064557,
     "end_time": "2024-07-31T10:17:40.933507",
     "exception": false,
     "start_time": "2024-07-31T10:17:40.868950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(reconstruction_mse, r2score, relerror, training_time, runtime_per_iter, resultdir, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a366414",
   "metadata": {
    "papermill": {
     "duration": 0.061621,
     "end_time": "2024-07-31T10:17:41.057289",
     "exception": false,
     "start_time": "2024-07-31T10:17:40.995668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14295.406978,
   "end_time": "2024-07-31T10:17:42.335635",
   "environment_variables": {},
   "exception": null,
   "input_path": "b_PI-Latent-NO_with-PCA.ipynb",
   "output_path": "results/b_PI-Latent-NO_with-PCA/seed=0_n_used=200/output_seed=0_n_used=200.ipynb",
   "parameters": {
    "n_used": 200,
    "seed": 0
   },
   "start_time": "2024-07-31T06:19:26.928657",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
